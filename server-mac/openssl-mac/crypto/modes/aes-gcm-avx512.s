
.globl	_ossl_vaes_vpclmulqdq_capable

.p2align	5
_ossl_vaes_vpclmulqdq_capable:
	movq	_OPENSSL_ia32cap_P+8(%rip),%rcx

	movq	$6600291188736,%rdx
	xorl	%eax,%eax
	andq	%rdx,%rcx
	cmpq	%rdx,%rcx
	cmoveq	%rcx,%rax
	.byte	0xf3,0xc3

.text	
.globl	_ossl_aes_gcm_init_avx512

.p2align	5
_ossl_aes_gcm_init_avx512:

.byte	243,15,30,250
	vpxorq	%xmm16,%xmm16,%xmm16


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	L$aes_128_fouggxrggmzkhlD
	cmpl	$11,%eax
	je	L$aes_192_fouggxrggmzkhlD
	cmpl	$13,%eax
	je	L$aes_256_fouggxrggmzkhlD
	jmp	L$exit_aes_fouggxrggmzkhlD
.p2align	5
L$aes_128_fouggxrggmzkhlD:
	vpxorq	0(%rdi),%xmm16,%xmm16

	vaesenc	16(%rdi),%xmm16,%xmm16

	vaesenc	32(%rdi),%xmm16,%xmm16

	vaesenc	48(%rdi),%xmm16,%xmm16

	vaesenc	64(%rdi),%xmm16,%xmm16

	vaesenc	80(%rdi),%xmm16,%xmm16

	vaesenc	96(%rdi),%xmm16,%xmm16

	vaesenc	112(%rdi),%xmm16,%xmm16

	vaesenc	128(%rdi),%xmm16,%xmm16

	vaesenc	144(%rdi),%xmm16,%xmm16

	vaesenclast	160(%rdi),%xmm16,%xmm16
	jmp	L$exit_aes_fouggxrggmzkhlD
.p2align	5
L$aes_192_fouggxrggmzkhlD:
	vpxorq	0(%rdi),%xmm16,%xmm16

	vaesenc	16(%rdi),%xmm16,%xmm16

	vaesenc	32(%rdi),%xmm16,%xmm16

	vaesenc	48(%rdi),%xmm16,%xmm16

	vaesenc	64(%rdi),%xmm16,%xmm16

	vaesenc	80(%rdi),%xmm16,%xmm16

	vaesenc	96(%rdi),%xmm16,%xmm16

	vaesenc	112(%rdi),%xmm16,%xmm16

	vaesenc	128(%rdi),%xmm16,%xmm16

	vaesenc	144(%rdi),%xmm16,%xmm16

	vaesenc	160(%rdi),%xmm16,%xmm16

	vaesenc	176(%rdi),%xmm16,%xmm16

	vaesenclast	192(%rdi),%xmm16,%xmm16
	jmp	L$exit_aes_fouggxrggmzkhlD
.p2align	5
L$aes_256_fouggxrggmzkhlD:
	vpxorq	0(%rdi),%xmm16,%xmm16

	vaesenc	16(%rdi),%xmm16,%xmm16

	vaesenc	32(%rdi),%xmm16,%xmm16

	vaesenc	48(%rdi),%xmm16,%xmm16

	vaesenc	64(%rdi),%xmm16,%xmm16

	vaesenc	80(%rdi),%xmm16,%xmm16

	vaesenc	96(%rdi),%xmm16,%xmm16

	vaesenc	112(%rdi),%xmm16,%xmm16

	vaesenc	128(%rdi),%xmm16,%xmm16

	vaesenc	144(%rdi),%xmm16,%xmm16

	vaesenc	160(%rdi),%xmm16,%xmm16

	vaesenc	176(%rdi),%xmm16,%xmm16

	vaesenc	192(%rdi),%xmm16,%xmm16

	vaesenc	208(%rdi),%xmm16,%xmm16

	vaesenclast	224(%rdi),%xmm16,%xmm16
	jmp	L$exit_aes_fouggxrggmzkhlD
L$exit_aes_fouggxrggmzkhlD:

	vpshufb	SHUF_MASK(%rip),%xmm16,%xmm16

	vmovdqa64	%xmm16,%xmm2
	vpsllq	$1,%xmm16,%xmm16
	vpsrlq	$63,%xmm2,%xmm2
	vmovdqa	%xmm2,%xmm1
	vpslldq	$8,%xmm2,%xmm2
	vpsrldq	$8,%xmm1,%xmm1
	vporq	%xmm2,%xmm16,%xmm16

	vpshufd	$36,%xmm1,%xmm2
	vpcmpeqd	TWOONE(%rip),%xmm2,%xmm2
	vpand	POLY(%rip),%xmm2,%xmm2
	vpxorq	%xmm2,%xmm16,%xmm16

	vmovdqu64	%xmm16,336(%rsi)
	vshufi32x4	$0x00,%ymm16,%ymm16,%ymm4
	vmovdqa	%ymm4,%ymm3

	vpclmulqdq	$0x11,%ymm4,%ymm3,%ymm0
	vpclmulqdq	$0x00,%ymm4,%ymm3,%ymm1
	vpclmulqdq	$0x01,%ymm4,%ymm3,%ymm2
	vpclmulqdq	$0x10,%ymm4,%ymm3,%ymm3
	vpxorq	%ymm2,%ymm3,%ymm3

	vpsrldq	$8,%ymm3,%ymm2
	vpslldq	$8,%ymm3,%ymm3
	vpxorq	%ymm2,%ymm0,%ymm0
	vpxorq	%ymm1,%ymm3,%ymm3



	vmovdqu64	POLY2(%rip),%ymm2

	vpclmulqdq	$0x01,%ymm3,%ymm2,%ymm1
	vpslldq	$8,%ymm1,%ymm1
	vpxorq	%ymm1,%ymm3,%ymm3



	vpclmulqdq	$0x00,%ymm3,%ymm2,%ymm1
	vpsrldq	$4,%ymm1,%ymm1
	vpclmulqdq	$0x10,%ymm3,%ymm2,%ymm3
	vpslldq	$4,%ymm3,%ymm3

	vpternlogq	$0x96,%ymm1,%ymm0,%ymm3

	vmovdqu64	%xmm3,320(%rsi)
	vinserti64x2	$1,%xmm16,%ymm3,%ymm4
	vmovdqa64	%ymm4,%ymm5

	vpclmulqdq	$0x11,%ymm3,%ymm4,%ymm0
	vpclmulqdq	$0x00,%ymm3,%ymm4,%ymm1
	vpclmulqdq	$0x01,%ymm3,%ymm4,%ymm2
	vpclmulqdq	$0x10,%ymm3,%ymm4,%ymm4
	vpxorq	%ymm2,%ymm4,%ymm4

	vpsrldq	$8,%ymm4,%ymm2
	vpslldq	$8,%ymm4,%ymm4
	vpxorq	%ymm2,%ymm0,%ymm0
	vpxorq	%ymm1,%ymm4,%ymm4



	vmovdqu64	POLY2(%rip),%ymm2

	vpclmulqdq	$0x01,%ymm4,%ymm2,%ymm1
	vpslldq	$8,%ymm1,%ymm1
	vpxorq	%ymm1,%ymm4,%ymm4



	vpclmulqdq	$0x00,%ymm4,%ymm2,%ymm1
	vpsrldq	$4,%ymm1,%ymm1
	vpclmulqdq	$0x10,%ymm4,%ymm2,%ymm4
	vpslldq	$4,%ymm4,%ymm4

	vpternlogq	$0x96,%ymm1,%ymm0,%ymm4

	vmovdqu64	%ymm4,288(%rsi)

	vinserti64x4	$1,%ymm5,%zmm4,%zmm4


	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3
	vmovdqa64	%zmm4,%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm2
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4

	vmovdqu64	%zmm4,224(%rsi)
	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm2
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm5,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm5,%zmm2,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm5

	vmovdqu64	%zmm5,160(%rsi)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm2
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4

	vmovdqu64	%zmm4,96(%rsi)
	vzeroupper
L$abort_init:
	.byte	0xf3,0xc3


.globl	_ossl_aes_gcm_setiv_avx512

.p2align	5
_ossl_aes_gcm_setiv_avx512:

L$setiv_seh_begin:
.byte	243,15,30,250
	pushq	%rbx

L$setiv_seh_push_rbx:
	pushq	%rbp

L$setiv_seh_push_rbp:
	pushq	%r12

L$setiv_seh_push_r12:
	pushq	%r13

L$setiv_seh_push_r13:
	pushq	%r14

L$setiv_seh_push_r14:
	pushq	%r15

L$setiv_seh_push_r15:










	leaq	0(%rsp),%rbp

L$setiv_seh_setfp:

L$setiv_seh_prolog_end:
	subq	$820,%rsp
	andq	$(-64),%rsp
	cmpq	$12,%rcx
	je	iv_len_12_init_IV
	vpxor	%xmm2,%xmm2,%xmm2
	movq	%rdx,%r10
	movq	%rcx,%r11
	orq	%r11,%r11
	jz	L$_CALC_AAD_done_hAnCDfBuoxEgFDy

	xorq	%rbx,%rbx
	vmovdqa64	SHUF_MASK(%rip),%zmm16

L$_get_AAD_loop48x16_hAnCDfBuoxEgFDy:
	cmpq	$768,%r11
	jl	L$_exit_AAD_loop48x16_hAnCDfBuoxEgFDy
	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	L$_skip_hkeys_precomputation_BltklAuqBAtztan

	vmovdqu64	288(%rsi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rsi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rsi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rsi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,192(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,128(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,64(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,0(%rsp)
L$_skip_hkeys_precomputation_BltklAuqBAtztan:
	movq	$1,%rbx
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	0(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	512(%r10),%zmm11
	vmovdqu64	576(%r10),%zmm3
	vmovdqu64	640(%r10),%zmm4
	vmovdqu64	704(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$768,%r11
	je	L$_CALC_AAD_done_hAnCDfBuoxEgFDy

	addq	$768,%r10
	jmp	L$_get_AAD_loop48x16_hAnCDfBuoxEgFDy

L$_exit_AAD_loop48x16_hAnCDfBuoxEgFDy:

	cmpq	$512,%r11
	jl	L$_less_than_32x16_hAnCDfBuoxEgFDy

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	L$_skip_hkeys_precomputation_wjCfxbpqaxoEEdq

	vmovdqu64	288(%rsi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rsi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rsi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rsi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)
L$_skip_hkeys_precomputation_wjCfxbpqaxoEEdq:
	movq	$1,%rbx
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$512,%r11
	je	L$_CALC_AAD_done_hAnCDfBuoxEgFDy

	addq	$512,%r10
	jmp	L$_less_than_16x16_hAnCDfBuoxEgFDy

L$_less_than_32x16_hAnCDfBuoxEgFDy:
	cmpq	$256,%r11
	jl	L$_less_than_16x16_hAnCDfBuoxEgFDy

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	96(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	160(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	224(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	288(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$256,%r11
	je	L$_CALC_AAD_done_hAnCDfBuoxEgFDy

	addq	$256,%r10

L$_less_than_16x16_hAnCDfBuoxEgFDy:

	leaq	byte64_len_to_mask_table(%rip),%r12
	leaq	(%r12,%r11,8),%r12


	addl	$15,%r11d
	shrl	$4,%r11d
	cmpl	$2,%r11d
	jb	L$_AAD_blocks_1_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_2_hAnCDfBuoxEgFDy
	cmpl	$4,%r11d
	jb	L$_AAD_blocks_3_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_4_hAnCDfBuoxEgFDy
	cmpl	$6,%r11d
	jb	L$_AAD_blocks_5_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_6_hAnCDfBuoxEgFDy
	cmpl	$8,%r11d
	jb	L$_AAD_blocks_7_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_8_hAnCDfBuoxEgFDy
	cmpl	$10,%r11d
	jb	L$_AAD_blocks_9_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_10_hAnCDfBuoxEgFDy
	cmpl	$12,%r11d
	jb	L$_AAD_blocks_11_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_12_hAnCDfBuoxEgFDy
	cmpl	$14,%r11d
	jb	L$_AAD_blocks_13_hAnCDfBuoxEgFDy
	je	L$_AAD_blocks_14_hAnCDfBuoxEgFDy
	cmpl	$15,%r11d
	je	L$_AAD_blocks_15_hAnCDfBuoxEgFDy
L$_AAD_blocks_16_hAnCDfBuoxEgFDy:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	96(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	224(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_15_hAnCDfBuoxEgFDy:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	112(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	240(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_14_hAnCDfBuoxEgFDy:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%ymm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%ymm16,%ymm5,%ymm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	128(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	256(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_13_hAnCDfBuoxEgFDy:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%xmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%xmm16,%xmm5,%xmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	144(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	208(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	272(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_12_hAnCDfBuoxEgFDy:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	224(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_11_hAnCDfBuoxEgFDy:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	240(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_10_hAnCDfBuoxEgFDy:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%ymm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%ymm16,%ymm4,%ymm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	256(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_9_hAnCDfBuoxEgFDy:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%xmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%xmm16,%xmm4,%xmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	208(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	272(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_8_hAnCDfBuoxEgFDy:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	224(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_7_hAnCDfBuoxEgFDy:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	240(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_6_hAnCDfBuoxEgFDy:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%ymm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%ymm16,%ymm3,%ymm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	256(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_5_hAnCDfBuoxEgFDy:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%xmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%xmm16,%xmm3,%xmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	272(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_4_hAnCDfBuoxEgFDy:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_3_hAnCDfBuoxEgFDy:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_2_hAnCDfBuoxEgFDy:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%ymm11{%k1}{z}
	vpshufb	%ymm16,%ymm11,%ymm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	L$_CALC_AAD_done_hAnCDfBuoxEgFDy
L$_AAD_blocks_1_hAnCDfBuoxEgFDy:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%xmm11{%k1}{z}
	vpshufb	%xmm16,%xmm11,%xmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

L$_CALC_AAD_done_hAnCDfBuoxEgFDy:
	movq	%rcx,%r10
	shlq	$3,%r10
	vmovq	%r10,%xmm3


	vpxorq	%xmm2,%xmm3,%xmm2

	vmovdqu64	336(%rsi),%xmm1

	vpclmulqdq	$0x11,%xmm1,%xmm2,%xmm11
	vpclmulqdq	$0x00,%xmm1,%xmm2,%xmm3
	vpclmulqdq	$0x01,%xmm1,%xmm2,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm2,%xmm2
	vpxorq	%xmm4,%xmm2,%xmm2

	vpsrldq	$8,%xmm2,%xmm4
	vpslldq	$8,%xmm2,%xmm2
	vpxorq	%xmm4,%xmm11,%xmm11
	vpxorq	%xmm3,%xmm2,%xmm2



	vmovdqu64	POLY2(%rip),%xmm4

	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm2,%xmm2



	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm2
	vpslldq	$4,%xmm2,%xmm2

	vpternlogq	$0x96,%xmm3,%xmm11,%xmm2

	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
	jmp	skip_iv_len_12_init_IV
iv_len_12_init_IV:

	vmovdqu8	ONEf(%rip),%xmm2
	movq	%rdx,%r11
	movl	$0x0000000000000fff,%r10d
	kmovq	%r10,%k1
	vmovdqu8	(%r11),%xmm2{%k1}
skip_iv_len_12_init_IV:
	vmovdqu	%xmm2,%xmm1


	movl	240(%rdi),%r10d
	cmpl	$9,%r10d
	je	L$aes_128_kdjtxrcCAnvsztj
	cmpl	$11,%r10d
	je	L$aes_192_kdjtxrcCAnvsztj
	cmpl	$13,%r10d
	je	L$aes_256_kdjtxrcCAnvsztj
	jmp	L$exit_aes_kdjtxrcCAnvsztj
.p2align	5
L$aes_128_kdjtxrcCAnvsztj:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenclast	160(%rdi),%xmm1,%xmm1
	jmp	L$exit_aes_kdjtxrcCAnvsztj
.p2align	5
L$aes_192_kdjtxrcCAnvsztj:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenc	160(%rdi),%xmm1,%xmm1

	vaesenc	176(%rdi),%xmm1,%xmm1

	vaesenclast	192(%rdi),%xmm1,%xmm1
	jmp	L$exit_aes_kdjtxrcCAnvsztj
.p2align	5
L$aes_256_kdjtxrcCAnvsztj:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenc	160(%rdi),%xmm1,%xmm1

	vaesenc	176(%rdi),%xmm1,%xmm1

	vaesenc	192(%rdi),%xmm1,%xmm1

	vaesenc	208(%rdi),%xmm1,%xmm1

	vaesenclast	224(%rdi),%xmm1,%xmm1
	jmp	L$exit_aes_kdjtxrcCAnvsztj
L$exit_aes_kdjtxrcCAnvsztj:

	vmovdqu	%xmm1,32(%rsi)


	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
	vmovdqu	%xmm2,0(%rsi)
	cmpq	$256,%rcx
	jbe	L$skip_hkeys_cleanup_miEDkzcwChioiaa
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
L$skip_hkeys_cleanup_miEDkzcwChioiaa:
	vzeroupper
	leaq	(%rbp),%rsp

	popq	%r15

	popq	%r14

	popq	%r13

	popq	%r12

	popq	%rbp

	popq	%rbx

L$abort_setiv:
	.byte	0xf3,0xc3
L$setiv_seh_end:


.globl	_ossl_aes_gcm_update_aad_avx512

.p2align	5
_ossl_aes_gcm_update_aad_avx512:

L$ghash_seh_begin:
.byte	243,15,30,250
	pushq	%rbx

L$ghash_seh_push_rbx:
	pushq	%rbp

L$ghash_seh_push_rbp:
	pushq	%r12

L$ghash_seh_push_r12:
	pushq	%r13

L$ghash_seh_push_r13:
	pushq	%r14

L$ghash_seh_push_r14:
	pushq	%r15

L$ghash_seh_push_r15:










	leaq	0(%rsp),%rbp

L$ghash_seh_setfp:

L$ghash_seh_prolog_end:
	subq	$820,%rsp
	andq	$(-64),%rsp
	vmovdqu64	64(%rdi),%xmm14
	movq	%rsi,%r10
	movq	%rdx,%r11
	orq	%r11,%r11
	jz	L$_CALC_AAD_done_FmymeshfeyDGtfh

	xorq	%rbx,%rbx
	vmovdqa64	SHUF_MASK(%rip),%zmm16

L$_get_AAD_loop48x16_FmymeshfeyDGtfh:
	cmpq	$768,%r11
	jl	L$_exit_AAD_loop48x16_FmymeshfeyDGtfh
	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	L$_skip_hkeys_precomputation_BddBtcxjGBjoAFd

	vmovdqu64	288(%rdi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rdi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rdi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rdi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,192(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,128(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,64(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,0(%rsp)
L$_skip_hkeys_precomputation_BddBtcxjGBjoAFd:
	movq	$1,%rbx
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	0(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	512(%r10),%zmm11
	vmovdqu64	576(%r10),%zmm3
	vmovdqu64	640(%r10),%zmm4
	vmovdqu64	704(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$768,%r11
	je	L$_CALC_AAD_done_FmymeshfeyDGtfh

	addq	$768,%r10
	jmp	L$_get_AAD_loop48x16_FmymeshfeyDGtfh

L$_exit_AAD_loop48x16_FmymeshfeyDGtfh:

	cmpq	$512,%r11
	jl	L$_less_than_32x16_FmymeshfeyDGtfh

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	L$_skip_hkeys_precomputation_mdjvyFoBeyvygwj

	vmovdqu64	288(%rdi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rdi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rdi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rdi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)
L$_skip_hkeys_precomputation_mdjvyFoBeyvygwj:
	movq	$1,%rbx
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$512,%r11
	je	L$_CALC_AAD_done_FmymeshfeyDGtfh

	addq	$512,%r10
	jmp	L$_less_than_16x16_FmymeshfeyDGtfh

L$_less_than_32x16_FmymeshfeyDGtfh:
	cmpq	$256,%r11
	jl	L$_less_than_16x16_FmymeshfeyDGtfh

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	96(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	160(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	224(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	288(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$256,%r11
	je	L$_CALC_AAD_done_FmymeshfeyDGtfh

	addq	$256,%r10

L$_less_than_16x16_FmymeshfeyDGtfh:

	leaq	byte64_len_to_mask_table(%rip),%r12
	leaq	(%r12,%r11,8),%r12


	addl	$15,%r11d
	shrl	$4,%r11d
	cmpl	$2,%r11d
	jb	L$_AAD_blocks_1_FmymeshfeyDGtfh
	je	L$_AAD_blocks_2_FmymeshfeyDGtfh
	cmpl	$4,%r11d
	jb	L$_AAD_blocks_3_FmymeshfeyDGtfh
	je	L$_AAD_blocks_4_FmymeshfeyDGtfh
	cmpl	$6,%r11d
	jb	L$_AAD_blocks_5_FmymeshfeyDGtfh
	je	L$_AAD_blocks_6_FmymeshfeyDGtfh
	cmpl	$8,%r11d
	jb	L$_AAD_blocks_7_FmymeshfeyDGtfh
	je	L$_AAD_blocks_8_FmymeshfeyDGtfh
	cmpl	$10,%r11d
	jb	L$_AAD_blocks_9_FmymeshfeyDGtfh
	je	L$_AAD_blocks_10_FmymeshfeyDGtfh
	cmpl	$12,%r11d
	jb	L$_AAD_blocks_11_FmymeshfeyDGtfh
	je	L$_AAD_blocks_12_FmymeshfeyDGtfh
	cmpl	$14,%r11d
	jb	L$_AAD_blocks_13_FmymeshfeyDGtfh
	je	L$_AAD_blocks_14_FmymeshfeyDGtfh
	cmpl	$15,%r11d
	je	L$_AAD_blocks_15_FmymeshfeyDGtfh
L$_AAD_blocks_16_FmymeshfeyDGtfh:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	96(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	160(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	224(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_15_FmymeshfeyDGtfh:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	112(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	176(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	240(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_14_FmymeshfeyDGtfh:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%ymm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%ymm16,%ymm5,%ymm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	128(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	192(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	256(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_13_FmymeshfeyDGtfh:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%xmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%xmm16,%xmm5,%xmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	144(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	208(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	272(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_12_FmymeshfeyDGtfh:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	160(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	224(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_11_FmymeshfeyDGtfh:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	176(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	240(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_10_FmymeshfeyDGtfh:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%ymm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%ymm16,%ymm4,%ymm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	192(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	256(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_9_FmymeshfeyDGtfh:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%xmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%xmm16,%xmm4,%xmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	208(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	272(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_8_FmymeshfeyDGtfh:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	224(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_7_FmymeshfeyDGtfh:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	240(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_6_FmymeshfeyDGtfh:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%ymm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%ymm16,%ymm3,%ymm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	256(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_5_FmymeshfeyDGtfh:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%xmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%xmm16,%xmm3,%xmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	272(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_4_FmymeshfeyDGtfh:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_3_FmymeshfeyDGtfh:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_2_FmymeshfeyDGtfh:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%ymm11{%k1}{z}
	vpshufb	%ymm16,%ymm11,%ymm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	L$_CALC_AAD_done_FmymeshfeyDGtfh
L$_AAD_blocks_1_FmymeshfeyDGtfh:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%xmm11{%k1}{z}
	vpshufb	%xmm16,%xmm11,%xmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

L$_CALC_AAD_done_FmymeshfeyDGtfh:
	vmovdqu64	%xmm14,64(%rdi)
	cmpq	$256,%rdx
	jbe	L$skip_hkeys_cleanup_fDyqFpkvexjdAkh
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
L$skip_hkeys_cleanup_fDyqFpkvexjdAkh:
	vzeroupper
	leaq	(%rbp),%rsp

	popq	%r15

	popq	%r14

	popq	%r13

	popq	%r12

	popq	%rbp

	popq	%rbx

L$exit_update_aad:
	.byte	0xf3,0xc3
L$ghash_seh_end:


.globl	_ossl_aes_gcm_encrypt_avx512

.p2align	5
_ossl_aes_gcm_encrypt_avx512:

L$encrypt_seh_begin:
.byte	243,15,30,250
	pushq	%rbx

L$encrypt_seh_push_rbx:
	pushq	%rbp

L$encrypt_seh_push_rbp:
	pushq	%r12

L$encrypt_seh_push_r12:
	pushq	%r13

L$encrypt_seh_push_r13:
	pushq	%r14

L$encrypt_seh_push_r14:
	pushq	%r15

L$encrypt_seh_push_r15:










	leaq	0(%rsp),%rbp

L$encrypt_seh_setfp:

L$encrypt_seh_prolog_end:
	subq	$1588,%rsp
	andq	$(-64),%rsp


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	L$aes_gcm_encrypt_128_avx512
	cmpl	$11,%eax
	je	L$aes_gcm_encrypt_192_avx512
	cmpl	$13,%eax
	je	L$aes_gcm_encrypt_256_avx512
	xorl	%eax,%eax
	jmp	L$exit_gcm_encrypt
.p2align	5
L$aes_gcm_encrypt_128_avx512:
	orq	%r8,%r8
	je	L$_enc_dec_done_pqhBnagrlByuFgo
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	L$_partial_block_done_iFwueoguEobaCiC
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	L$_no_extra_mask_iFwueoguEobaCiC
	subq	%r13,%r12
L$_no_extra_mask_iFwueoguEobaCiC:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	L$_partial_incomplete_iFwueoguEobaCiC

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	L$_enc_dec_done_iFwueoguEobaCiC

L$_partial_incomplete_iFwueoguEobaCiC:
	addq	%r8,(%rdx)
	movq	%r8,%r11

L$_enc_dec_done_iFwueoguEobaCiC:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
L$_partial_block_done_iFwueoguEobaCiC:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	L$_enc_dec_done_pqhBnagrlByuFgo
	cmpq	$256,%r8
	jbe	L$_message_below_equal_16_blocks_pqhBnagrlByuFgo

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_gsudoCfFCkAvnyb
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_gsudoCfFCkAvnyb
L$_next_16_overflow_gsudoCfFCkAvnyb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_gsudoCfFCkAvnyb:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_uFFxabqcvogFwrf

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
L$_skip_hkeys_precomputation_uFFxabqcvogFwrf:
	cmpq	$512,%r8
	jb	L$_message_below_32_blocks_pqhBnagrlByuFgo



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_csqACedgjwjxxfo
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_csqACedgjwjxxfo
L$_next_16_overflow_csqACedgjwjxxfo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_csqACedgjwjxxfo:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_jkGkluyrBnriuBE
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
L$_skip_hkeys_precomputation_jkGkluyrBnriuBE:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	L$_no_more_big_nblocks_pqhBnagrlByuFgo
L$_encrypt_big_nblocks_pqhBnagrlByuFgo:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_dsoudFmdijEbdBi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_dsoudFmdijEbdBi
L$_16_blocks_overflow_dsoudFmdijEbdBi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_dsoudFmdijEbdBi:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_yBBFElAzljeluex
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_yBBFElAzljeluex
L$_16_blocks_overflow_yBBFElAzljeluex:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_yBBFElAzljeluex:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_phdwDheotCfsCia
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_phdwDheotCfsCia
L$_16_blocks_overflow_phdwDheotCfsCia:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_phdwDheotCfsCia:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	L$_encrypt_big_nblocks_pqhBnagrlByuFgo

L$_no_more_big_nblocks_pqhBnagrlByuFgo:

	cmpq	$512,%r8
	jae	L$_encrypt_32_blocks_pqhBnagrlByuFgo

	cmpq	$256,%r8
	jae	L$_encrypt_16_blocks_pqhBnagrlByuFgo
L$_encrypt_0_blocks_ghash_32_pqhBnagrlByuFgo:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_lwnlbxEsydrnBAu

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_lwnlbxEsydrnBAu
	jb	L$_last_num_blocks_is_7_1_lwnlbxEsydrnBAu


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_lwnlbxEsydrnBAu
	jb	L$_last_num_blocks_is_11_9_lwnlbxEsydrnBAu


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_lwnlbxEsydrnBAu
	ja	L$_last_num_blocks_is_16_lwnlbxEsydrnBAu
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_lwnlbxEsydrnBAu
	jmp	L$_last_num_blocks_is_13_lwnlbxEsydrnBAu

L$_last_num_blocks_is_11_9_lwnlbxEsydrnBAu:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_lwnlbxEsydrnBAu
	ja	L$_last_num_blocks_is_11_lwnlbxEsydrnBAu
	jmp	L$_last_num_blocks_is_9_lwnlbxEsydrnBAu

L$_last_num_blocks_is_7_1_lwnlbxEsydrnBAu:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_lwnlbxEsydrnBAu
	jb	L$_last_num_blocks_is_3_1_lwnlbxEsydrnBAu

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_lwnlbxEsydrnBAu
	je	L$_last_num_blocks_is_6_lwnlbxEsydrnBAu
	jmp	L$_last_num_blocks_is_5_lwnlbxEsydrnBAu

L$_last_num_blocks_is_3_1_lwnlbxEsydrnBAu:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_lwnlbxEsydrnBAu
	je	L$_last_num_blocks_is_2_lwnlbxEsydrnBAu
L$_last_num_blocks_is_1_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_porDargyyFeEBmF
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_porDargyyFeEBmF

L$_16_blocks_overflow_porDargyyFeEBmF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_porDargyyFeEBmF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vsGErihtEqsffqD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vsGErihtEqsffqD
L$_small_initial_partial_block_vsGErihtEqsffqD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_vsGErihtEqsffqD
L$_small_initial_compute_done_vsGErihtEqsffqD:
L$_after_reduction_vsGErihtEqsffqD:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_2_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_FregqjctsGbyAmd
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_FregqjctsGbyAmd

L$_16_blocks_overflow_FregqjctsGbyAmd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_FregqjctsGbyAmd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uhjyvBBnhujbeyx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uhjyvBBnhujbeyx
L$_small_initial_partial_block_uhjyvBBnhujbeyx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uhjyvBBnhujbeyx:

	orq	%r8,%r8
	je	L$_after_reduction_uhjyvBBnhujbeyx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uhjyvBBnhujbeyx:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_3_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_CsxmhzeDixEoGqv
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_CsxmhzeDixEoGqv

L$_16_blocks_overflow_CsxmhzeDixEoGqv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_CsxmhzeDixEoGqv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AFAFoFGnbEhvqqk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AFAFoFGnbEhvqqk
L$_small_initial_partial_block_AFAFoFGnbEhvqqk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AFAFoFGnbEhvqqk:

	orq	%r8,%r8
	je	L$_after_reduction_AFAFoFGnbEhvqqk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AFAFoFGnbEhvqqk:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_4_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_glqamFbdbwtFrjo
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_glqamFbdbwtFrjo

L$_16_blocks_overflow_glqamFbdbwtFrjo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_glqamFbdbwtFrjo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ngBmlAaevpCrGfo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ngBmlAaevpCrGfo
L$_small_initial_partial_block_ngBmlAaevpCrGfo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ngBmlAaevpCrGfo:

	orq	%r8,%r8
	je	L$_after_reduction_ngBmlAaevpCrGfo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ngBmlAaevpCrGfo:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_5_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_ltErsArzyyfwzeG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_ltErsArzyyfwzeG

L$_16_blocks_overflow_ltErsArzyyfwzeG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_ltErsArzyyfwzeG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ciEyEpFAFebksfz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ciEyEpFAFebksfz
L$_small_initial_partial_block_ciEyEpFAFebksfz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ciEyEpFAFebksfz:

	orq	%r8,%r8
	je	L$_after_reduction_ciEyEpFAFebksfz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ciEyEpFAFebksfz:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_6_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_hDfcqpuzGdjkeEy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_hDfcqpuzGdjkeEy

L$_16_blocks_overflow_hDfcqpuzGdjkeEy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_hDfcqpuzGdjkeEy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_umjtuzocDengtED





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_umjtuzocDengtED
L$_small_initial_partial_block_umjtuzocDengtED:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_umjtuzocDengtED:

	orq	%r8,%r8
	je	L$_after_reduction_umjtuzocDengtED
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_umjtuzocDengtED:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_7_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_iarFGwxCqbDoEtG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_iarFGwxCqbDoEtG

L$_16_blocks_overflow_iarFGwxCqbDoEtG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_iarFGwxCqbDoEtG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jGqbxacuiwggbAw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jGqbxacuiwggbAw
L$_small_initial_partial_block_jGqbxacuiwggbAw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jGqbxacuiwggbAw:

	orq	%r8,%r8
	je	L$_after_reduction_jGqbxacuiwggbAw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_jGqbxacuiwggbAw:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_8_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_bvrlhyBjnnuwiuc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_bvrlhyBjnnuwiuc

L$_16_blocks_overflow_bvrlhyBjnnuwiuc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_bvrlhyBjnnuwiuc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tkFyfdpbnCoFgbB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tkFyfdpbnCoFgbB
L$_small_initial_partial_block_tkFyfdpbnCoFgbB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tkFyfdpbnCoFgbB:

	orq	%r8,%r8
	je	L$_after_reduction_tkFyfdpbnCoFgbB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_tkFyfdpbnCoFgbB:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_9_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_qgBskqgyAlpDjDj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_qgBskqgyAlpDjDj

L$_16_blocks_overflow_qgBskqgyAlpDjDj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_qgBskqgyAlpDjDj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yAiaxxjGGpdotfq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yAiaxxjGGpdotfq
L$_small_initial_partial_block_yAiaxxjGGpdotfq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yAiaxxjGGpdotfq:

	orq	%r8,%r8
	je	L$_after_reduction_yAiaxxjGGpdotfq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yAiaxxjGGpdotfq:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_10_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_jmjotxjCDtlpfAo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_jmjotxjCDtlpfAo

L$_16_blocks_overflow_jmjotxjCDtlpfAo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_jmjotxjCDtlpfAo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jjxtiqDBbptarfh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jjxtiqDBbptarfh
L$_small_initial_partial_block_jjxtiqDBbptarfh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jjxtiqDBbptarfh:

	orq	%r8,%r8
	je	L$_after_reduction_jjxtiqDBbptarfh
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_jjxtiqDBbptarfh:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_11_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_AsvryzykBkCrBoG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_AsvryzykBkCrBoG

L$_16_blocks_overflow_AsvryzykBkCrBoG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_AsvryzykBkCrBoG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vExjeuhgbqGAbym





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vExjeuhgbqGAbym
L$_small_initial_partial_block_vExjeuhgbqGAbym:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vExjeuhgbqGAbym:

	orq	%r8,%r8
	je	L$_after_reduction_vExjeuhgbqGAbym
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vExjeuhgbqGAbym:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_12_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_eBvFpkBzakEfujr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_eBvFpkBzakEfujr

L$_16_blocks_overflow_eBvFpkBzakEfujr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_eBvFpkBzakEfujr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_exDxCnqshalaDry





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_exDxCnqshalaDry
L$_small_initial_partial_block_exDxCnqshalaDry:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_exDxCnqshalaDry:

	orq	%r8,%r8
	je	L$_after_reduction_exDxCnqshalaDry
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_exDxCnqshalaDry:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_13_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_GlmCctfvraaEnlf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_GlmCctfvraaEnlf

L$_16_blocks_overflow_GlmCctfvraaEnlf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_GlmCctfvraaEnlf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tGFzhrwAmxusxgr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tGFzhrwAmxusxgr
L$_small_initial_partial_block_tGFzhrwAmxusxgr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tGFzhrwAmxusxgr:

	orq	%r8,%r8
	je	L$_after_reduction_tGFzhrwAmxusxgr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_tGFzhrwAmxusxgr:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_14_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_FBFmqrgdwDGjAfa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_FBFmqrgdwDGjAfa

L$_16_blocks_overflow_FBFmqrgdwDGjAfa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_FBFmqrgdwDGjAfa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qcatbnBgrkzCfBw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qcatbnBgrkzCfBw
L$_small_initial_partial_block_qcatbnBgrkzCfBw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qcatbnBgrkzCfBw:

	orq	%r8,%r8
	je	L$_after_reduction_qcatbnBgrkzCfBw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qcatbnBgrkzCfBw:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_15_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_mfwDxhytvhunrpF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_mfwDxhytvhunrpF

L$_16_blocks_overflow_mfwDxhytvhunrpF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_mfwDxhytvhunrpF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rwldxripBvaAtdt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rwldxripBvaAtdt
L$_small_initial_partial_block_rwldxripBvaAtdt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rwldxripBvaAtdt:

	orq	%r8,%r8
	je	L$_after_reduction_rwldxripBvaAtdt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rwldxripBvaAtdt:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_16_lwnlbxEsydrnBAu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_iDxdyxEosxzbwdi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_iDxdyxEosxzbwdi

L$_16_blocks_overflow_iDxdyxEosxzbwdi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_iDxdyxEosxzbwdi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_buhqhmicvlztvfv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_buhqhmicvlztvfv:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_buhqhmicvlztvfv:
	jmp	L$_last_blocks_done_lwnlbxEsydrnBAu
L$_last_num_blocks_is_0_lwnlbxEsydrnBAu:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_lwnlbxEsydrnBAu:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_pqhBnagrlByuFgo
L$_encrypt_32_blocks_pqhBnagrlByuFgo:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_wejurawqeFeFbky
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_wejurawqeFeFbky
L$_16_blocks_overflow_wejurawqeFeFbky:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_wejurawqeFeFbky:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_ttEdDeveokzwjvj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ttEdDeveokzwjvj
L$_16_blocks_overflow_ttEdDeveokzwjvj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ttEdDeveokzwjvj:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_adCDvphdggwbeDF

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_adCDvphdggwbeDF
	jb	L$_last_num_blocks_is_7_1_adCDvphdggwbeDF


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_adCDvphdggwbeDF
	jb	L$_last_num_blocks_is_11_9_adCDvphdggwbeDF


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_adCDvphdggwbeDF
	ja	L$_last_num_blocks_is_16_adCDvphdggwbeDF
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_adCDvphdggwbeDF
	jmp	L$_last_num_blocks_is_13_adCDvphdggwbeDF

L$_last_num_blocks_is_11_9_adCDvphdggwbeDF:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_adCDvphdggwbeDF
	ja	L$_last_num_blocks_is_11_adCDvphdggwbeDF
	jmp	L$_last_num_blocks_is_9_adCDvphdggwbeDF

L$_last_num_blocks_is_7_1_adCDvphdggwbeDF:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_adCDvphdggwbeDF
	jb	L$_last_num_blocks_is_3_1_adCDvphdggwbeDF

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_adCDvphdggwbeDF
	je	L$_last_num_blocks_is_6_adCDvphdggwbeDF
	jmp	L$_last_num_blocks_is_5_adCDvphdggwbeDF

L$_last_num_blocks_is_3_1_adCDvphdggwbeDF:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_adCDvphdggwbeDF
	je	L$_last_num_blocks_is_2_adCDvphdggwbeDF
L$_last_num_blocks_is_1_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_EDvcjlCCaDgDyxw
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_EDvcjlCCaDgDyxw

L$_16_blocks_overflow_EDvcjlCCaDgDyxw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_EDvcjlCCaDgDyxw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qkcEscxlhmakatj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qkcEscxlhmakatj
L$_small_initial_partial_block_qkcEscxlhmakatj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_qkcEscxlhmakatj
L$_small_initial_compute_done_qkcEscxlhmakatj:
L$_after_reduction_qkcEscxlhmakatj:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_2_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_ArdpfokFyGsmqux
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_ArdpfokFyGsmqux

L$_16_blocks_overflow_ArdpfokFyGsmqux:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_ArdpfokFyGsmqux:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aCimfxezABkmaCA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aCimfxezABkmaCA
L$_small_initial_partial_block_aCimfxezABkmaCA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aCimfxezABkmaCA:

	orq	%r8,%r8
	je	L$_after_reduction_aCimfxezABkmaCA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aCimfxezABkmaCA:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_3_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_FtnABdqClitCeBk
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_FtnABdqClitCeBk

L$_16_blocks_overflow_FtnABdqClitCeBk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_FtnABdqClitCeBk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_womeAiEkGGsnimv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_womeAiEkGGsnimv
L$_small_initial_partial_block_womeAiEkGGsnimv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_womeAiEkGGsnimv:

	orq	%r8,%r8
	je	L$_after_reduction_womeAiEkGGsnimv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_womeAiEkGGsnimv:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_4_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_wojbBbhlimxxsou
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_wojbBbhlimxxsou

L$_16_blocks_overflow_wojbBbhlimxxsou:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_wojbBbhlimxxsou:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_Esafjhsegggaejt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_Esafjhsegggaejt
L$_small_initial_partial_block_Esafjhsegggaejt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_Esafjhsegggaejt:

	orq	%r8,%r8
	je	L$_after_reduction_Esafjhsegggaejt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_Esafjhsegggaejt:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_5_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_zACmfbGfAzhsCqD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_zACmfbGfAzhsCqD

L$_16_blocks_overflow_zACmfbGfAzhsCqD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_zACmfbGfAzhsCqD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kDmfjnyncEfecbw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kDmfjnyncEfecbw
L$_small_initial_partial_block_kDmfjnyncEfecbw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kDmfjnyncEfecbw:

	orq	%r8,%r8
	je	L$_after_reduction_kDmfjnyncEfecbw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kDmfjnyncEfecbw:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_6_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_hyGxCwylgxCljls
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_hyGxCwylgxCljls

L$_16_blocks_overflow_hyGxCwylgxCljls:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_hyGxCwylgxCljls:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_orjoyewmfueDabz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_orjoyewmfueDabz
L$_small_initial_partial_block_orjoyewmfueDabz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_orjoyewmfueDabz:

	orq	%r8,%r8
	je	L$_after_reduction_orjoyewmfueDabz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_orjoyewmfueDabz:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_7_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_jmurzskvowncEmc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_jmurzskvowncEmc

L$_16_blocks_overflow_jmurzskvowncEmc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_jmurzskvowncEmc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gcFfntftwgcCqyj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gcFfntftwgcCqyj
L$_small_initial_partial_block_gcFfntftwgcCqyj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gcFfntftwgcCqyj:

	orq	%r8,%r8
	je	L$_after_reduction_gcFfntftwgcCqyj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gcFfntftwgcCqyj:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_8_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_cteEztzBypxGnfb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_cteEztzBypxGnfb

L$_16_blocks_overflow_cteEztzBypxGnfb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_cteEztzBypxGnfb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CrjmqwtpmEvpzoo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CrjmqwtpmEvpzoo
L$_small_initial_partial_block_CrjmqwtpmEvpzoo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CrjmqwtpmEvpzoo:

	orq	%r8,%r8
	je	L$_after_reduction_CrjmqwtpmEvpzoo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CrjmqwtpmEvpzoo:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_9_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_xfsxCvuqtsbCAix
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_xfsxCvuqtsbCAix

L$_16_blocks_overflow_xfsxCvuqtsbCAix:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_xfsxCvuqtsbCAix:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qeduGtbgBptGbbx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qeduGtbgBptGbbx
L$_small_initial_partial_block_qeduGtbgBptGbbx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qeduGtbgBptGbbx:

	orq	%r8,%r8
	je	L$_after_reduction_qeduGtbgBptGbbx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qeduGtbgBptGbbx:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_10_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_ryDargrDlFdmwfr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_ryDargrDlFdmwfr

L$_16_blocks_overflow_ryDargrDlFdmwfr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_ryDargrDlFdmwfr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_Esyocvslanwklgx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_Esyocvslanwklgx
L$_small_initial_partial_block_Esyocvslanwklgx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_Esyocvslanwklgx:

	orq	%r8,%r8
	je	L$_after_reduction_Esyocvslanwklgx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_Esyocvslanwklgx:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_11_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_cCAqebGzjbneliy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_cCAqebGzjbneliy

L$_16_blocks_overflow_cCAqebGzjbneliy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_cCAqebGzjbneliy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ctFpnAAwhifvsaz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ctFpnAAwhifvsaz
L$_small_initial_partial_block_ctFpnAAwhifvsaz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ctFpnAAwhifvsaz:

	orq	%r8,%r8
	je	L$_after_reduction_ctFpnAAwhifvsaz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ctFpnAAwhifvsaz:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_12_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_CFCAGmDiyBEaApq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_CFCAGmDiyBEaApq

L$_16_blocks_overflow_CFCAGmDiyBEaApq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_CFCAGmDiyBEaApq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fxwnuAheqzDqEel





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fxwnuAheqzDqEel
L$_small_initial_partial_block_fxwnuAheqzDqEel:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fxwnuAheqzDqEel:

	orq	%r8,%r8
	je	L$_after_reduction_fxwnuAheqzDqEel
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fxwnuAheqzDqEel:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_13_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_coojqFzzkkaCwfp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_coojqFzzkkaCwfp

L$_16_blocks_overflow_coojqFzzkkaCwfp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_coojqFzzkkaCwfp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yupfsnshyyGhEmG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yupfsnshyyGhEmG
L$_small_initial_partial_block_yupfsnshyyGhEmG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yupfsnshyyGhEmG:

	orq	%r8,%r8
	je	L$_after_reduction_yupfsnshyyGhEmG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yupfsnshyyGhEmG:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_14_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_FvwuAghgrjCFtst
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_FvwuAghgrjCFtst

L$_16_blocks_overflow_FvwuAghgrjCFtst:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_FvwuAghgrjCFtst:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AFhjhGcsECpAttj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AFhjhGcsECpAttj
L$_small_initial_partial_block_AFhjhGcsECpAttj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AFhjhGcsECpAttj:

	orq	%r8,%r8
	je	L$_after_reduction_AFhjhGcsECpAttj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AFhjhGcsECpAttj:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_15_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_GEkodqrteGojEjw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_GEkodqrteGojEjw

L$_16_blocks_overflow_GEkodqrteGojEjw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_GEkodqrteGojEjw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fEdjkebwmuvneun





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fEdjkebwmuvneun
L$_small_initial_partial_block_fEdjkebwmuvneun:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fEdjkebwmuvneun:

	orq	%r8,%r8
	je	L$_after_reduction_fEdjkebwmuvneun
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fEdjkebwmuvneun:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_16_adCDvphdggwbeDF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_ypakmnngokctmsB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ypakmnngokctmsB

L$_16_blocks_overflow_ypakmnngokctmsB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ypakmnngokctmsB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_mooGqdimdiFuDcx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mooGqdimdiFuDcx:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mooGqdimdiFuDcx:
	jmp	L$_last_blocks_done_adCDvphdggwbeDF
L$_last_num_blocks_is_0_adCDvphdggwbeDF:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_adCDvphdggwbeDF:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_pqhBnagrlByuFgo
L$_encrypt_16_blocks_pqhBnagrlByuFgo:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_gbAlqEstDkAeAFm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_gbAlqEstDkAeAFm
L$_16_blocks_overflow_gbAlqEstDkAeAFm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_gbAlqEstDkAeAFm:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_eFFfFmwbnBsbEFl

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_eFFfFmwbnBsbEFl
	jb	L$_last_num_blocks_is_7_1_eFFfFmwbnBsbEFl


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_eFFfFmwbnBsbEFl
	jb	L$_last_num_blocks_is_11_9_eFFfFmwbnBsbEFl


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_eFFfFmwbnBsbEFl
	ja	L$_last_num_blocks_is_16_eFFfFmwbnBsbEFl
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_eFFfFmwbnBsbEFl
	jmp	L$_last_num_blocks_is_13_eFFfFmwbnBsbEFl

L$_last_num_blocks_is_11_9_eFFfFmwbnBsbEFl:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_eFFfFmwbnBsbEFl
	ja	L$_last_num_blocks_is_11_eFFfFmwbnBsbEFl
	jmp	L$_last_num_blocks_is_9_eFFfFmwbnBsbEFl

L$_last_num_blocks_is_7_1_eFFfFmwbnBsbEFl:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_eFFfFmwbnBsbEFl
	jb	L$_last_num_blocks_is_3_1_eFFfFmwbnBsbEFl

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_eFFfFmwbnBsbEFl
	je	L$_last_num_blocks_is_6_eFFfFmwbnBsbEFl
	jmp	L$_last_num_blocks_is_5_eFFfFmwbnBsbEFl

L$_last_num_blocks_is_3_1_eFFfFmwbnBsbEFl:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_eFFfFmwbnBsbEFl
	je	L$_last_num_blocks_is_2_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_1_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_kqoxCnzdjyFcDEg
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_kqoxCnzdjyFcDEg

L$_16_blocks_overflow_kqoxCnzdjyFcDEg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_kqoxCnzdjyFcDEg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BpniqfEkAdjodcv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BpniqfEkAdjodcv
L$_small_initial_partial_block_BpniqfEkAdjodcv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_BpniqfEkAdjodcv
L$_small_initial_compute_done_BpniqfEkAdjodcv:
L$_after_reduction_BpniqfEkAdjodcv:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_2_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_vbrAxDBEivfdfBp
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_vbrAxDBEivfdfBp

L$_16_blocks_overflow_vbrAxDBEivfdfBp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_vbrAxDBEivfdfBp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nzcqbdcgmtwvsGg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nzcqbdcgmtwvsGg
L$_small_initial_partial_block_nzcqbdcgmtwvsGg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nzcqbdcgmtwvsGg:

	orq	%r8,%r8
	je	L$_after_reduction_nzcqbdcgmtwvsGg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nzcqbdcgmtwvsGg:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_3_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_qenFpFfhcxvtzAb
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_qenFpFfhcxvtzAb

L$_16_blocks_overflow_qenFpFfhcxvtzAb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_qenFpFfhcxvtzAb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_njDkxfqGBwdtDjt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_njDkxfqGBwdtDjt
L$_small_initial_partial_block_njDkxfqGBwdtDjt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_njDkxfqGBwdtDjt:

	orq	%r8,%r8
	je	L$_after_reduction_njDkxfqGBwdtDjt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_njDkxfqGBwdtDjt:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_4_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_jauFCBryicBxDqz
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_jauFCBryicBxDqz

L$_16_blocks_overflow_jauFCBryicBxDqz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_jauFCBryicBxDqz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_orvsAzFFiampsvA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_orvsAzFFiampsvA
L$_small_initial_partial_block_orvsAzFFiampsvA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_orvsAzFFiampsvA:

	orq	%r8,%r8
	je	L$_after_reduction_orvsAzFFiampsvA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_orvsAzFFiampsvA:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_5_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_eCAhEwlgcfGhDfi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_eCAhEwlgcfGhDfi

L$_16_blocks_overflow_eCAhEwlgcfGhDfi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_eCAhEwlgcfGhDfi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BszGCwGiAorgmnw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BszGCwGiAorgmnw
L$_small_initial_partial_block_BszGCwGiAorgmnw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BszGCwGiAorgmnw:

	orq	%r8,%r8
	je	L$_after_reduction_BszGCwGiAorgmnw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BszGCwGiAorgmnw:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_6_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_DBegpBoackhwqix
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_DBegpBoackhwqix

L$_16_blocks_overflow_DBegpBoackhwqix:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_DBegpBoackhwqix:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ryvmmFsvDjbnAkl





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ryvmmFsvDjbnAkl
L$_small_initial_partial_block_ryvmmFsvDjbnAkl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ryvmmFsvDjbnAkl:

	orq	%r8,%r8
	je	L$_after_reduction_ryvmmFsvDjbnAkl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ryvmmFsvDjbnAkl:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_7_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_oEhnEvfkqoDpDxm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_oEhnEvfkqoDpDxm

L$_16_blocks_overflow_oEhnEvfkqoDpDxm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_oEhnEvfkqoDpDxm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iadepEenFavjrjD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iadepEenFavjrjD
L$_small_initial_partial_block_iadepEenFavjrjD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iadepEenFavjrjD:

	orq	%r8,%r8
	je	L$_after_reduction_iadepEenFavjrjD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_iadepEenFavjrjD:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_8_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_AxkAzctjfdzoffq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_AxkAzctjfdzoffq

L$_16_blocks_overflow_AxkAzctjfdzoffq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_AxkAzctjfdzoffq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BgDxnrooczxizfv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BgDxnrooczxizfv
L$_small_initial_partial_block_BgDxnrooczxizfv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BgDxnrooczxizfv:

	orq	%r8,%r8
	je	L$_after_reduction_BgDxnrooczxizfv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BgDxnrooczxizfv:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_9_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_pzeEiCouBwqnmmt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_pzeEiCouBwqnmmt

L$_16_blocks_overflow_pzeEiCouBwqnmmt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_pzeEiCouBwqnmmt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qkDAGegsvfDnuxc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qkDAGegsvfDnuxc
L$_small_initial_partial_block_qkDAGegsvfDnuxc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qkDAGegsvfDnuxc:

	orq	%r8,%r8
	je	L$_after_reduction_qkDAGegsvfDnuxc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qkDAGegsvfDnuxc:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_10_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_aveAcyDjbfibhfx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_aveAcyDjbfibhfx

L$_16_blocks_overflow_aveAcyDjbfibhfx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_aveAcyDjbfibhfx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yBzgGDpaiGEnCDs





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yBzgGDpaiGEnCDs
L$_small_initial_partial_block_yBzgGDpaiGEnCDs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yBzgGDpaiGEnCDs:

	orq	%r8,%r8
	je	L$_after_reduction_yBzgGDpaiGEnCDs
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yBzgGDpaiGEnCDs:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_11_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_gCmxrbAyftsaFni
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_gCmxrbAyftsaFni

L$_16_blocks_overflow_gCmxrbAyftsaFni:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_gCmxrbAyftsaFni:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uDnjcrslApzimbl





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uDnjcrslApzimbl
L$_small_initial_partial_block_uDnjcrslApzimbl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uDnjcrslApzimbl:

	orq	%r8,%r8
	je	L$_after_reduction_uDnjcrslApzimbl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uDnjcrslApzimbl:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_12_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_AcGteDatpcvhyGk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_AcGteDatpcvhyGk

L$_16_blocks_overflow_AcGteDatpcvhyGk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_AcGteDatpcvhyGk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_luAiDtDsFzFxvaC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_luAiDtDsFzFxvaC
L$_small_initial_partial_block_luAiDtDsFzFxvaC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_luAiDtDsFzFxvaC:

	orq	%r8,%r8
	je	L$_after_reduction_luAiDtDsFzFxvaC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_luAiDtDsFzFxvaC:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_13_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_nvfqmAgfvfCjAge
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_nvfqmAgfvfCjAge

L$_16_blocks_overflow_nvfqmAgfvfCjAge:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_nvfqmAgfvfCjAge:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dGreuszgfgoBlae





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dGreuszgfgoBlae
L$_small_initial_partial_block_dGreuszgfgoBlae:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dGreuszgfgoBlae:

	orq	%r8,%r8
	je	L$_after_reduction_dGreuszgfgoBlae
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_dGreuszgfgoBlae:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_14_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_AEAzvzBhyCmbtlu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_AEAzvzBhyCmbtlu

L$_16_blocks_overflow_AEAzvzBhyCmbtlu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_AEAzvzBhyCmbtlu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_sjDbdcezhGdEqBA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_sjDbdcezhGdEqBA
L$_small_initial_partial_block_sjDbdcezhGdEqBA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_sjDbdcezhGdEqBA:

	orq	%r8,%r8
	je	L$_after_reduction_sjDbdcezhGdEqBA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_sjDbdcezhGdEqBA:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_15_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_byCxihepwumqAsg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_byCxihepwumqAsg

L$_16_blocks_overflow_byCxihepwumqAsg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_byCxihepwumqAsg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yeanguzGbEqnlht





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yeanguzGbEqnlht
L$_small_initial_partial_block_yeanguzGbEqnlht:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yeanguzGbEqnlht:

	orq	%r8,%r8
	je	L$_after_reduction_yeanguzGbEqnlht
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yeanguzGbEqnlht:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_16_eFFfFmwbnBsbEFl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_arAhCrcxgCfoyct
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_arAhCrcxgCfoyct

L$_16_blocks_overflow_arAhCrcxgCfoyct:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_arAhCrcxgCfoyct:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_pyemhdlnxCxrvfb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pyemhdlnxCxrvfb:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pyemhdlnxCxrvfb:
	jmp	L$_last_blocks_done_eFFfFmwbnBsbEFl
L$_last_num_blocks_is_0_eFFfFmwbnBsbEFl:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_eFFfFmwbnBsbEFl:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_pqhBnagrlByuFgo

L$_message_below_32_blocks_pqhBnagrlByuFgo:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_olhzyzBbqgFEjiD
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
L$_skip_hkeys_precomputation_olhzyzBbqgFEjiD:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_pnBsukjBCtbziag

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_pnBsukjBCtbziag
	jb	L$_last_num_blocks_is_7_1_pnBsukjBCtbziag


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_pnBsukjBCtbziag
	jb	L$_last_num_blocks_is_11_9_pnBsukjBCtbziag


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_pnBsukjBCtbziag
	ja	L$_last_num_blocks_is_16_pnBsukjBCtbziag
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_pnBsukjBCtbziag
	jmp	L$_last_num_blocks_is_13_pnBsukjBCtbziag

L$_last_num_blocks_is_11_9_pnBsukjBCtbziag:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_pnBsukjBCtbziag
	ja	L$_last_num_blocks_is_11_pnBsukjBCtbziag
	jmp	L$_last_num_blocks_is_9_pnBsukjBCtbziag

L$_last_num_blocks_is_7_1_pnBsukjBCtbziag:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_pnBsukjBCtbziag
	jb	L$_last_num_blocks_is_3_1_pnBsukjBCtbziag

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_pnBsukjBCtbziag
	je	L$_last_num_blocks_is_6_pnBsukjBCtbziag
	jmp	L$_last_num_blocks_is_5_pnBsukjBCtbziag

L$_last_num_blocks_is_3_1_pnBsukjBCtbziag:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_pnBsukjBCtbziag
	je	L$_last_num_blocks_is_2_pnBsukjBCtbziag
L$_last_num_blocks_is_1_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_miybftpuFpilyub
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_miybftpuFpilyub

L$_16_blocks_overflow_miybftpuFpilyub:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_miybftpuFpilyub:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jrrtplwbggxGtEG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jrrtplwbggxGtEG
L$_small_initial_partial_block_jrrtplwbggxGtEG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_jrrtplwbggxGtEG
L$_small_initial_compute_done_jrrtplwbggxGtEG:
L$_after_reduction_jrrtplwbggxGtEG:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_2_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_xGylsubmApCfbCo
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_xGylsubmApCfbCo

L$_16_blocks_overflow_xGylsubmApCfbCo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_xGylsubmApCfbCo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aeiufdoiFbGGalv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aeiufdoiFbGGalv
L$_small_initial_partial_block_aeiufdoiFbGGalv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aeiufdoiFbGGalv:

	orq	%r8,%r8
	je	L$_after_reduction_aeiufdoiFbGGalv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aeiufdoiFbGGalv:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_3_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_BmyaDqpECvcukje
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_BmyaDqpECvcukje

L$_16_blocks_overflow_BmyaDqpECvcukje:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_BmyaDqpECvcukje:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FdomyGgtozoxdgi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FdomyGgtozoxdgi
L$_small_initial_partial_block_FdomyGgtozoxdgi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FdomyGgtozoxdgi:

	orq	%r8,%r8
	je	L$_after_reduction_FdomyGgtozoxdgi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FdomyGgtozoxdgi:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_4_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_ailDurrkgodjkAx
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_ailDurrkgodjkAx

L$_16_blocks_overflow_ailDurrkgodjkAx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_ailDurrkgodjkAx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FzrwGrwogsweuGA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FzrwGrwogsweuGA
L$_small_initial_partial_block_FzrwGrwogsweuGA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FzrwGrwogsweuGA:

	orq	%r8,%r8
	je	L$_after_reduction_FzrwGrwogsweuGA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FzrwGrwogsweuGA:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_5_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_vCtBlsBGjwioFfe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_vCtBlsBGjwioFfe

L$_16_blocks_overflow_vCtBlsBGjwioFfe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_vCtBlsBGjwioFfe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xbiwDbvvivanBue





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xbiwDbvvivanBue
L$_small_initial_partial_block_xbiwDbvvivanBue:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xbiwDbvvivanBue:

	orq	%r8,%r8
	je	L$_after_reduction_xbiwDbvvivanBue
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xbiwDbvvivanBue:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_6_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_dauqFrAuEeaqssd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_dauqFrAuEeaqssd

L$_16_blocks_overflow_dauqFrAuEeaqssd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_dauqFrAuEeaqssd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hzgjFrFwFgiulFA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hzgjFrFwFgiulFA
L$_small_initial_partial_block_hzgjFrFwFgiulFA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hzgjFrFwFgiulFA:

	orq	%r8,%r8
	je	L$_after_reduction_hzgjFrFwFgiulFA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hzgjFrFwFgiulFA:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_7_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_xbtAfwuwbpuotzC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_xbtAfwuwbpuotzC

L$_16_blocks_overflow_xbtAfwuwbpuotzC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_xbtAfwuwbpuotzC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_sCcFhtcfBgdfrda





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_sCcFhtcfBgdfrda
L$_small_initial_partial_block_sCcFhtcfBgdfrda:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_sCcFhtcfBgdfrda:

	orq	%r8,%r8
	je	L$_after_reduction_sCcFhtcfBgdfrda
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_sCcFhtcfBgdfrda:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_8_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_ugBFmhkBFztEpfC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_ugBFmhkBFztEpfC

L$_16_blocks_overflow_ugBFmhkBFztEpfC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_ugBFmhkBFztEpfC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_wGzcDrvFDifgBqG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_wGzcDrvFDifgBqG
L$_small_initial_partial_block_wGzcDrvFDifgBqG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_wGzcDrvFDifgBqG:

	orq	%r8,%r8
	je	L$_after_reduction_wGzcDrvFDifgBqG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_wGzcDrvFDifgBqG:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_9_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_khglABbwabffjGv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_khglABbwabffjGv

L$_16_blocks_overflow_khglABbwabffjGv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_khglABbwabffjGv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CoyGueyhkmanCbs





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CoyGueyhkmanCbs
L$_small_initial_partial_block_CoyGueyhkmanCbs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CoyGueyhkmanCbs:

	orq	%r8,%r8
	je	L$_after_reduction_CoyGueyhkmanCbs
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CoyGueyhkmanCbs:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_10_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_jxzvibldjuedCAw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_jxzvibldjuedCAw

L$_16_blocks_overflow_jxzvibldjuedCAw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_jxzvibldjuedCAw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_odDseeFGnimuatl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_odDseeFGnimuatl
L$_small_initial_partial_block_odDseeFGnimuatl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_odDseeFGnimuatl:

	orq	%r8,%r8
	je	L$_after_reduction_odDseeFGnimuatl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_odDseeFGnimuatl:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_11_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_GnFwolEEacrCygp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_GnFwolEEacrCygp

L$_16_blocks_overflow_GnFwolEEacrCygp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_GnFwolEEacrCygp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ngkBBxivkftzFwE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ngkBBxivkftzFwE
L$_small_initial_partial_block_ngkBBxivkftzFwE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ngkBBxivkftzFwE:

	orq	%r8,%r8
	je	L$_after_reduction_ngkBBxivkftzFwE
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ngkBBxivkftzFwE:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_12_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_mlhsGsBrwibltgG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_mlhsGsBrwibltgG

L$_16_blocks_overflow_mlhsGsBrwibltgG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_mlhsGsBrwibltgG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EkwchAjpcpEwvfu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EkwchAjpcpEwvfu
L$_small_initial_partial_block_EkwchAjpcpEwvfu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EkwchAjpcpEwvfu:

	orq	%r8,%r8
	je	L$_after_reduction_EkwchAjpcpEwvfu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EkwchAjpcpEwvfu:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_13_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_simhhaBbxljEDty
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_simhhaBbxljEDty

L$_16_blocks_overflow_simhhaBbxljEDty:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_simhhaBbxljEDty:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qFCngjgvmDEvDip





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qFCngjgvmDEvDip
L$_small_initial_partial_block_qFCngjgvmDEvDip:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qFCngjgvmDEvDip:

	orq	%r8,%r8
	je	L$_after_reduction_qFCngjgvmDEvDip
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qFCngjgvmDEvDip:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_14_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_utDFfxdpBhnjwow
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_utDFfxdpBhnjwow

L$_16_blocks_overflow_utDFfxdpBhnjwow:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_utDFfxdpBhnjwow:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hutczdGnygzgiBG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hutczdGnygzgiBG
L$_small_initial_partial_block_hutczdGnygzgiBG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hutczdGnygzgiBG:

	orq	%r8,%r8
	je	L$_after_reduction_hutczdGnygzgiBG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hutczdGnygzgiBG:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_15_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_agcuAcAnhGhyFui
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_agcuAcAnhGhyFui

L$_16_blocks_overflow_agcuAcAnhGhyFui:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_agcuAcAnhGhyFui:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FctuEybeFdlzmhq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FctuEybeFdlzmhq
L$_small_initial_partial_block_FctuEybeFdlzmhq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FctuEybeFdlzmhq:

	orq	%r8,%r8
	je	L$_after_reduction_FctuEybeFdlzmhq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FctuEybeFdlzmhq:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_16_pnBsukjBCtbziag:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_luDFptAeAswCGBD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_luDFptAeAswCGBD

L$_16_blocks_overflow_luDFptAeAswCGBD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_luDFptAeAswCGBD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_wCBkbglkxDeCGfn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_wCBkbglkxDeCGfn:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_wCBkbglkxDeCGfn:
	jmp	L$_last_blocks_done_pnBsukjBCtbziag
L$_last_num_blocks_is_0_pnBsukjBCtbziag:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_pnBsukjBCtbziag:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_pqhBnagrlByuFgo

L$_message_below_equal_16_blocks_pqhBnagrlByuFgo:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	L$_small_initial_num_blocks_is_8_DcolglzEwcqkjwg
	jl	L$_small_initial_num_blocks_is_7_1_DcolglzEwcqkjwg


	cmpq	$12,%r12
	je	L$_small_initial_num_blocks_is_12_DcolglzEwcqkjwg
	jl	L$_small_initial_num_blocks_is_11_9_DcolglzEwcqkjwg


	cmpq	$16,%r12
	je	L$_small_initial_num_blocks_is_16_DcolglzEwcqkjwg
	cmpq	$15,%r12
	je	L$_small_initial_num_blocks_is_15_DcolglzEwcqkjwg
	cmpq	$14,%r12
	je	L$_small_initial_num_blocks_is_14_DcolglzEwcqkjwg
	jmp	L$_small_initial_num_blocks_is_13_DcolglzEwcqkjwg

L$_small_initial_num_blocks_is_11_9_DcolglzEwcqkjwg:

	cmpq	$11,%r12
	je	L$_small_initial_num_blocks_is_11_DcolglzEwcqkjwg
	cmpq	$10,%r12
	je	L$_small_initial_num_blocks_is_10_DcolglzEwcqkjwg
	jmp	L$_small_initial_num_blocks_is_9_DcolglzEwcqkjwg

L$_small_initial_num_blocks_is_7_1_DcolglzEwcqkjwg:
	cmpq	$4,%r12
	je	L$_small_initial_num_blocks_is_4_DcolglzEwcqkjwg
	jl	L$_small_initial_num_blocks_is_3_1_DcolglzEwcqkjwg

	cmpq	$7,%r12
	je	L$_small_initial_num_blocks_is_7_DcolglzEwcqkjwg
	cmpq	$6,%r12
	je	L$_small_initial_num_blocks_is_6_DcolglzEwcqkjwg
	jmp	L$_small_initial_num_blocks_is_5_DcolglzEwcqkjwg

L$_small_initial_num_blocks_is_3_1_DcolglzEwcqkjwg:

	cmpq	$3,%r12
	je	L$_small_initial_num_blocks_is_3_DcolglzEwcqkjwg
	cmpq	$2,%r12
	je	L$_small_initial_num_blocks_is_2_DcolglzEwcqkjwg





L$_small_initial_num_blocks_is_1_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GxFfEvnnbjgAthk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GxFfEvnnbjgAthk
L$_small_initial_partial_block_GxFfEvnnbjgAthk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	L$_after_reduction_GxFfEvnnbjgAthk
L$_small_initial_compute_done_GxFfEvnnbjgAthk:
L$_after_reduction_GxFfEvnnbjgAthk:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_2_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xlrqCugcGlnnCGm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xlrqCugcGlnnCGm
L$_small_initial_partial_block_xlrqCugcGlnnCGm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xlrqCugcGlnnCGm:

	orq	%r8,%r8
	je	L$_after_reduction_xlrqCugcGlnnCGm
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_xlrqCugcGlnnCGm:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_3_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_buzEbdkCqxEcbbg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_buzEbdkCqxEcbbg
L$_small_initial_partial_block_buzEbdkCqxEcbbg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_buzEbdkCqxEcbbg:

	orq	%r8,%r8
	je	L$_after_reduction_buzEbdkCqxEcbbg
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_buzEbdkCqxEcbbg:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_4_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iAwhgwGgskCFqoG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iAwhgwGgskCFqoG
L$_small_initial_partial_block_iAwhgwGgskCFqoG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iAwhgwGgskCFqoG:

	orq	%r8,%r8
	je	L$_after_reduction_iAwhgwGgskCFqoG
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_iAwhgwGgskCFqoG:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_5_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aFzkebtlFcAzqwy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aFzkebtlFcAzqwy
L$_small_initial_partial_block_aFzkebtlFcAzqwy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aFzkebtlFcAzqwy:

	orq	%r8,%r8
	je	L$_after_reduction_aFzkebtlFcAzqwy
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_aFzkebtlFcAzqwy:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_6_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rasnFsGdftCeAky





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rasnFsGdftCeAky
L$_small_initial_partial_block_rasnFsGdftCeAky:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rasnFsGdftCeAky:

	orq	%r8,%r8
	je	L$_after_reduction_rasnFsGdftCeAky
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_rasnFsGdftCeAky:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_7_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DBkvCmDbExwFmsj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DBkvCmDbExwFmsj
L$_small_initial_partial_block_DBkvCmDbExwFmsj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DBkvCmDbExwFmsj:

	orq	%r8,%r8
	je	L$_after_reduction_DBkvCmDbExwFmsj
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_DBkvCmDbExwFmsj:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_8_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_foBypqaEgqgmjBr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_foBypqaEgqgmjBr
L$_small_initial_partial_block_foBypqaEgqgmjBr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_foBypqaEgqgmjBr:

	orq	%r8,%r8
	je	L$_after_reduction_foBypqaEgqgmjBr
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_foBypqaEgqgmjBr:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_9_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ByAprmtmwutefFa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ByAprmtmwutefFa
L$_small_initial_partial_block_ByAprmtmwutefFa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ByAprmtmwutefFa:

	orq	%r8,%r8
	je	L$_after_reduction_ByAprmtmwutefFa
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ByAprmtmwutefFa:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_10_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pqbwysAahemfyjG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pqbwysAahemfyjG
L$_small_initial_partial_block_pqbwysAahemfyjG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pqbwysAahemfyjG:

	orq	%r8,%r8
	je	L$_after_reduction_pqbwysAahemfyjG
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_pqbwysAahemfyjG:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_11_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_atpanbGmykedEhe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_atpanbGmykedEhe
L$_small_initial_partial_block_atpanbGmykedEhe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_atpanbGmykedEhe:

	orq	%r8,%r8
	je	L$_after_reduction_atpanbGmykedEhe
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_atpanbGmykedEhe:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_12_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_apyrehwdDEiAtCm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_apyrehwdDEiAtCm
L$_small_initial_partial_block_apyrehwdDEiAtCm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_apyrehwdDEiAtCm:

	orq	%r8,%r8
	je	L$_after_reduction_apyrehwdDEiAtCm
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_apyrehwdDEiAtCm:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_13_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lzEEljrxkdzipah





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lzEEljrxkdzipah
L$_small_initial_partial_block_lzEEljrxkdzipah:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lzEEljrxkdzipah:

	orq	%r8,%r8
	je	L$_after_reduction_lzEEljrxkdzipah
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_lzEEljrxkdzipah:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_14_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hzlxFmDtvBecayi





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hzlxFmDtvBecayi
L$_small_initial_partial_block_hzlxFmDtvBecayi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hzlxFmDtvBecayi:

	orq	%r8,%r8
	je	L$_after_reduction_hzlxFmDtvBecayi
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_hzlxFmDtvBecayi:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_15_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rqAqojGccxqwddk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rqAqojGccxqwddk
L$_small_initial_partial_block_rqAqojGccxqwddk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rqAqojGccxqwddk:

	orq	%r8,%r8
	je	L$_after_reduction_rqAqojGccxqwddk
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_rqAqojGccxqwddk:
	jmp	L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg
L$_small_initial_num_blocks_is_16_DcolglzEwcqkjwg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_rFFdtAybuFbwsjy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rFFdtAybuFbwsjy:
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_rFFdtAybuFbwsjy:
L$_small_initial_blocks_encrypted_DcolglzEwcqkjwg:
L$_ghash_done_pqhBnagrlByuFgo:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
L$_enc_dec_done_pqhBnagrlByuFgo:
	jmp	L$exit_gcm_encrypt
.p2align	5
L$aes_gcm_encrypt_192_avx512:
	orq	%r8,%r8
	je	L$_enc_dec_done_yjdkbBaokEBheCl
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	L$_partial_block_done_kcjAFvfqqbvxcAl
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	L$_no_extra_mask_kcjAFvfqqbvxcAl
	subq	%r13,%r12
L$_no_extra_mask_kcjAFvfqqbvxcAl:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	L$_partial_incomplete_kcjAFvfqqbvxcAl

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	L$_enc_dec_done_kcjAFvfqqbvxcAl

L$_partial_incomplete_kcjAFvfqqbvxcAl:
	addq	%r8,(%rdx)
	movq	%r8,%r11

L$_enc_dec_done_kcjAFvfqqbvxcAl:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
L$_partial_block_done_kcjAFvfqqbvxcAl:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	L$_enc_dec_done_yjdkbBaokEBheCl
	cmpq	$256,%r8
	jbe	L$_message_below_equal_16_blocks_yjdkbBaokEBheCl

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_zlfCxqltxuhdokE
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_zlfCxqltxuhdokE
L$_next_16_overflow_zlfCxqltxuhdokE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_zlfCxqltxuhdokE:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_FBcjhrshDhusdgd

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
L$_skip_hkeys_precomputation_FBcjhrshDhusdgd:
	cmpq	$512,%r8
	jb	L$_message_below_32_blocks_yjdkbBaokEBheCl



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_zbzfzituopaodeu
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_zbzfzituopaodeu
L$_next_16_overflow_zbzfzituopaodeu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_zbzfzituopaodeu:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_BCDByuAxhBFilFw
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
L$_skip_hkeys_precomputation_BCDByuAxhBFilFw:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	L$_no_more_big_nblocks_yjdkbBaokEBheCl
L$_encrypt_big_nblocks_yjdkbBaokEBheCl:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_lmaEvBrytuhrtdz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_lmaEvBrytuhrtdz
L$_16_blocks_overflow_lmaEvBrytuhrtdz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_lmaEvBrytuhrtdz:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_pweskeedjnCxAao
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_pweskeedjnCxAao
L$_16_blocks_overflow_pweskeedjnCxAao:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_pweskeedjnCxAao:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_yaqDmEdutjjGayc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_yaqDmEdutjjGayc
L$_16_blocks_overflow_yaqDmEdutjjGayc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_yaqDmEdutjjGayc:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	L$_encrypt_big_nblocks_yjdkbBaokEBheCl

L$_no_more_big_nblocks_yjdkbBaokEBheCl:

	cmpq	$512,%r8
	jae	L$_encrypt_32_blocks_yjdkbBaokEBheCl

	cmpq	$256,%r8
	jae	L$_encrypt_16_blocks_yjdkbBaokEBheCl
L$_encrypt_0_blocks_ghash_32_yjdkbBaokEBheCl:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_imghgAguiukulsB

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_imghgAguiukulsB
	jb	L$_last_num_blocks_is_7_1_imghgAguiukulsB


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_imghgAguiukulsB
	jb	L$_last_num_blocks_is_11_9_imghgAguiukulsB


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_imghgAguiukulsB
	ja	L$_last_num_blocks_is_16_imghgAguiukulsB
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_imghgAguiukulsB
	jmp	L$_last_num_blocks_is_13_imghgAguiukulsB

L$_last_num_blocks_is_11_9_imghgAguiukulsB:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_imghgAguiukulsB
	ja	L$_last_num_blocks_is_11_imghgAguiukulsB
	jmp	L$_last_num_blocks_is_9_imghgAguiukulsB

L$_last_num_blocks_is_7_1_imghgAguiukulsB:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_imghgAguiukulsB
	jb	L$_last_num_blocks_is_3_1_imghgAguiukulsB

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_imghgAguiukulsB
	je	L$_last_num_blocks_is_6_imghgAguiukulsB
	jmp	L$_last_num_blocks_is_5_imghgAguiukulsB

L$_last_num_blocks_is_3_1_imghgAguiukulsB:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_imghgAguiukulsB
	je	L$_last_num_blocks_is_2_imghgAguiukulsB
L$_last_num_blocks_is_1_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_pjegzxiwCEldvAb
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_pjegzxiwCEldvAb

L$_16_blocks_overflow_pjegzxiwCEldvAb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_pjegzxiwCEldvAb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vsECDccwFvoAota





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vsECDccwFvoAota
L$_small_initial_partial_block_vsECDccwFvoAota:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_vsECDccwFvoAota
L$_small_initial_compute_done_vsECDccwFvoAota:
L$_after_reduction_vsECDccwFvoAota:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_2_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_AyCCCzDEsmodpoo
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_AyCCCzDEsmodpoo

L$_16_blocks_overflow_AyCCCzDEsmodpoo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_AyCCCzDEsmodpoo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CtDijhprGinCghi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CtDijhprGinCghi
L$_small_initial_partial_block_CtDijhprGinCghi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CtDijhprGinCghi:

	orq	%r8,%r8
	je	L$_after_reduction_CtDijhprGinCghi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CtDijhprGinCghi:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_3_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_dmhwtjtzAwmmrqf
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_dmhwtjtzAwmmrqf

L$_16_blocks_overflow_dmhwtjtzAwmmrqf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_dmhwtjtzAwmmrqf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_afdgxamiGxzyevp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_afdgxamiGxzyevp
L$_small_initial_partial_block_afdgxamiGxzyevp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_afdgxamiGxzyevp:

	orq	%r8,%r8
	je	L$_after_reduction_afdgxamiGxzyevp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_afdgxamiGxzyevp:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_4_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_bimAseeaAoisuam
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_bimAseeaAoisuam

L$_16_blocks_overflow_bimAseeaAoisuam:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_bimAseeaAoisuam:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GnstrryusmBmDgu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GnstrryusmBmDgu
L$_small_initial_partial_block_GnstrryusmBmDgu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GnstrryusmBmDgu:

	orq	%r8,%r8
	je	L$_after_reduction_GnstrryusmBmDgu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GnstrryusmBmDgu:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_5_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_ftxkirctixEBkFg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_ftxkirctixEBkFg

L$_16_blocks_overflow_ftxkirctixEBkFg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_ftxkirctixEBkFg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BwrkgEqqlhjBwtC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BwrkgEqqlhjBwtC
L$_small_initial_partial_block_BwrkgEqqlhjBwtC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BwrkgEqqlhjBwtC:

	orq	%r8,%r8
	je	L$_after_reduction_BwrkgEqqlhjBwtC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BwrkgEqqlhjBwtC:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_6_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_ycypjazmCElxjif
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_ycypjazmCElxjif

L$_16_blocks_overflow_ycypjazmCElxjif:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_ycypjazmCElxjif:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bcdsftDwdDnpiBu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bcdsftDwdDnpiBu
L$_small_initial_partial_block_bcdsftDwdDnpiBu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bcdsftDwdDnpiBu:

	orq	%r8,%r8
	je	L$_after_reduction_bcdsftDwdDnpiBu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_bcdsftDwdDnpiBu:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_7_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_egjtaybmFaruble
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_egjtaybmFaruble

L$_16_blocks_overflow_egjtaybmFaruble:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_egjtaybmFaruble:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rpnabejEzElhAEG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rpnabejEzElhAEG
L$_small_initial_partial_block_rpnabejEzElhAEG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rpnabejEzElhAEG:

	orq	%r8,%r8
	je	L$_after_reduction_rpnabejEzElhAEG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rpnabejEzElhAEG:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_8_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_dlxwAmtGeugEzsB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_dlxwAmtGeugEzsB

L$_16_blocks_overflow_dlxwAmtGeugEzsB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_dlxwAmtGeugEzsB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vnCbvkCcdBisdhd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vnCbvkCcdBisdhd
L$_small_initial_partial_block_vnCbvkCcdBisdhd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vnCbvkCcdBisdhd:

	orq	%r8,%r8
	je	L$_after_reduction_vnCbvkCcdBisdhd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vnCbvkCcdBisdhd:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_9_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_DBsruncecEalBCl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_DBsruncecEalBCl

L$_16_blocks_overflow_DBsruncecEalBCl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_DBsruncecEalBCl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zbazDmjaAApdptF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zbazDmjaAApdptF
L$_small_initial_partial_block_zbazDmjaAApdptF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zbazDmjaAApdptF:

	orq	%r8,%r8
	je	L$_after_reduction_zbazDmjaAApdptF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zbazDmjaAApdptF:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_10_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_mcyjkEvGcsazAwh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_mcyjkEvGcsazAwh

L$_16_blocks_overflow_mcyjkEvGcsazAwh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_mcyjkEvGcsazAwh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FGCBgijjGbbrdxg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FGCBgijjGbbrdxg
L$_small_initial_partial_block_FGCBgijjGbbrdxg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FGCBgijjGbbrdxg:

	orq	%r8,%r8
	je	L$_after_reduction_FGCBgijjGbbrdxg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FGCBgijjGbbrdxg:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_11_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_xFimtGuafkvDDlk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_xFimtGuafkvDDlk

L$_16_blocks_overflow_xFimtGuafkvDDlk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_xFimtGuafkvDDlk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xbkvemkifgzelgB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xbkvemkifgzelgB
L$_small_initial_partial_block_xbkvemkifgzelgB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xbkvemkifgzelgB:

	orq	%r8,%r8
	je	L$_after_reduction_xbkvemkifgzelgB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xbkvemkifgzelgB:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_12_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_sbEgGdozlhlbEAj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_sbEgGdozlhlbEAj

L$_16_blocks_overflow_sbEgGdozlhlbEAj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_sbEgGdozlhlbEAj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xcdGqvsrgGrGxqC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xcdGqvsrgGrGxqC
L$_small_initial_partial_block_xcdGqvsrgGrGxqC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xcdGqvsrgGrGxqC:

	orq	%r8,%r8
	je	L$_after_reduction_xcdGqvsrgGrGxqC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xcdGqvsrgGrGxqC:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_13_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_cjqrgdepAlmyFcf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_cjqrgdepAlmyFcf

L$_16_blocks_overflow_cjqrgdepAlmyFcf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_cjqrgdepAlmyFcf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_eEjaGEgmnbuqkfx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_eEjaGEgmnbuqkfx
L$_small_initial_partial_block_eEjaGEgmnbuqkfx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_eEjaGEgmnbuqkfx:

	orq	%r8,%r8
	je	L$_after_reduction_eEjaGEgmnbuqkfx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_eEjaGEgmnbuqkfx:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_14_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_fnfDjBkyakvnqzG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_fnfDjBkyakvnqzG

L$_16_blocks_overflow_fnfDjBkyakvnqzG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_fnfDjBkyakvnqzG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aebAnhzjDnyxcbm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aebAnhzjDnyxcbm
L$_small_initial_partial_block_aebAnhzjDnyxcbm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aebAnhzjDnyxcbm:

	orq	%r8,%r8
	je	L$_after_reduction_aebAnhzjDnyxcbm
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aebAnhzjDnyxcbm:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_15_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_uoidxexbCasEAaf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_uoidxexbCasEAaf

L$_16_blocks_overflow_uoidxexbCasEAaf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_uoidxexbCasEAaf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pzwGctxisGzCraj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pzwGctxisGzCraj
L$_small_initial_partial_block_pzwGctxisGzCraj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pzwGctxisGzCraj:

	orq	%r8,%r8
	je	L$_after_reduction_pzwGctxisGzCraj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pzwGctxisGzCraj:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_16_imghgAguiukulsB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_BdGxEoEbvvhkgnA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_BdGxEoEbvvhkgnA

L$_16_blocks_overflow_BdGxEoEbvvhkgnA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_BdGxEoEbvvhkgnA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_mzqjpxmyvqlprty:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mzqjpxmyvqlprty:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mzqjpxmyvqlprty:
	jmp	L$_last_blocks_done_imghgAguiukulsB
L$_last_num_blocks_is_0_imghgAguiukulsB:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_imghgAguiukulsB:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yjdkbBaokEBheCl
L$_encrypt_32_blocks_yjdkbBaokEBheCl:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_ojsgfBeqeGsuAws
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ojsgfBeqeGsuAws
L$_16_blocks_overflow_ojsgfBeqeGsuAws:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ojsgfBeqeGsuAws:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_tgnEavAFhDEhzjs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_tgnEavAFhDEhzjs
L$_16_blocks_overflow_tgnEavAFhDEhzjs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_tgnEavAFhDEhzjs:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_BzuiFDxlabjytFj

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_BzuiFDxlabjytFj
	jb	L$_last_num_blocks_is_7_1_BzuiFDxlabjytFj


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_BzuiFDxlabjytFj
	jb	L$_last_num_blocks_is_11_9_BzuiFDxlabjytFj


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_BzuiFDxlabjytFj
	ja	L$_last_num_blocks_is_16_BzuiFDxlabjytFj
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_BzuiFDxlabjytFj
	jmp	L$_last_num_blocks_is_13_BzuiFDxlabjytFj

L$_last_num_blocks_is_11_9_BzuiFDxlabjytFj:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_BzuiFDxlabjytFj
	ja	L$_last_num_blocks_is_11_BzuiFDxlabjytFj
	jmp	L$_last_num_blocks_is_9_BzuiFDxlabjytFj

L$_last_num_blocks_is_7_1_BzuiFDxlabjytFj:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_BzuiFDxlabjytFj
	jb	L$_last_num_blocks_is_3_1_BzuiFDxlabjytFj

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_BzuiFDxlabjytFj
	je	L$_last_num_blocks_is_6_BzuiFDxlabjytFj
	jmp	L$_last_num_blocks_is_5_BzuiFDxlabjytFj

L$_last_num_blocks_is_3_1_BzuiFDxlabjytFj:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_BzuiFDxlabjytFj
	je	L$_last_num_blocks_is_2_BzuiFDxlabjytFj
L$_last_num_blocks_is_1_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_fimhdffGkvnhfuw
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_fimhdffGkvnhfuw

L$_16_blocks_overflow_fimhdffGkvnhfuw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_fimhdffGkvnhfuw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_eEBhFDgjmGmkodc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_eEBhFDgjmGmkodc
L$_small_initial_partial_block_eEBhFDgjmGmkodc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_eEBhFDgjmGmkodc
L$_small_initial_compute_done_eEBhFDgjmGmkodc:
L$_after_reduction_eEBhFDgjmGmkodc:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_2_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_oamiwDyoriqzbEp
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_oamiwDyoriqzbEp

L$_16_blocks_overflow_oamiwDyoriqzbEp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_oamiwDyoriqzbEp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_niDysBcvniowqBd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_niDysBcvniowqBd
L$_small_initial_partial_block_niDysBcvniowqBd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_niDysBcvniowqBd:

	orq	%r8,%r8
	je	L$_after_reduction_niDysBcvniowqBd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_niDysBcvniowqBd:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_3_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_fdiuDpzsqntuycg
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_fdiuDpzsqntuycg

L$_16_blocks_overflow_fdiuDpzsqntuycg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_fdiuDpzsqntuycg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uFtboiAtEnbljcd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uFtboiAtEnbljcd
L$_small_initial_partial_block_uFtboiAtEnbljcd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uFtboiAtEnbljcd:

	orq	%r8,%r8
	je	L$_after_reduction_uFtboiAtEnbljcd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uFtboiAtEnbljcd:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_4_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_gjvappmFfGgmeEy
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_gjvappmFfGgmeEy

L$_16_blocks_overflow_gjvappmFfGgmeEy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_gjvappmFfGgmeEy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BFFpBossfmoFhov





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BFFpBossfmoFhov
L$_small_initial_partial_block_BFFpBossfmoFhov:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BFFpBossfmoFhov:

	orq	%r8,%r8
	je	L$_after_reduction_BFFpBossfmoFhov
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BFFpBossfmoFhov:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_5_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_aEmhCdusujfrwDj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_aEmhCdusujfrwDj

L$_16_blocks_overflow_aEmhCdusujfrwDj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_aEmhCdusujfrwDj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nvtldlzanAjoGbG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nvtldlzanAjoGbG
L$_small_initial_partial_block_nvtldlzanAjoGbG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nvtldlzanAjoGbG:

	orq	%r8,%r8
	je	L$_after_reduction_nvtldlzanAjoGbG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nvtldlzanAjoGbG:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_6_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_fekGmjnnghlqsfw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_fekGmjnnghlqsfw

L$_16_blocks_overflow_fekGmjnnghlqsfw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_fekGmjnnghlqsfw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_evvqerraboGblvt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_evvqerraboGblvt
L$_small_initial_partial_block_evvqerraboGblvt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_evvqerraboGblvt:

	orq	%r8,%r8
	je	L$_after_reduction_evvqerraboGblvt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_evvqerraboGblvt:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_7_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_euirEzmiCnzjcke
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_euirEzmiCnzjcke

L$_16_blocks_overflow_euirEzmiCnzjcke:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_euirEzmiCnzjcke:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vxCtvfafqDjisBn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vxCtvfafqDjisBn
L$_small_initial_partial_block_vxCtvfafqDjisBn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vxCtvfafqDjisBn:

	orq	%r8,%r8
	je	L$_after_reduction_vxCtvfafqDjisBn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vxCtvfafqDjisBn:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_8_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_poDEujaeaxFdxDs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_poDEujaeaxFdxDs

L$_16_blocks_overflow_poDEujaeaxFdxDs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_poDEujaeaxFdxDs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CthCluszwAaFCeB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CthCluszwAaFCeB
L$_small_initial_partial_block_CthCluszwAaFCeB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CthCluszwAaFCeB:

	orq	%r8,%r8
	je	L$_after_reduction_CthCluszwAaFCeB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CthCluszwAaFCeB:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_9_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_bisvgihytDjaEDa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_bisvgihytDjaEDa

L$_16_blocks_overflow_bisvgihytDjaEDa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_bisvgihytDjaEDa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aktGusxuctdlkEq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aktGusxuctdlkEq
L$_small_initial_partial_block_aktGusxuctdlkEq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aktGusxuctdlkEq:

	orq	%r8,%r8
	je	L$_after_reduction_aktGusxuctdlkEq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aktGusxuctdlkEq:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_10_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_EDzfreAADubmxif
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_EDzfreAADubmxif

L$_16_blocks_overflow_EDzfreAADubmxif:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_EDzfreAADubmxif:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yEnzFtzgetnlBae





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yEnzFtzgetnlBae
L$_small_initial_partial_block_yEnzFtzgetnlBae:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yEnzFtzgetnlBae:

	orq	%r8,%r8
	je	L$_after_reduction_yEnzFtzgetnlBae
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yEnzFtzgetnlBae:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_11_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_yvnmnyCtDrAjhnD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_yvnmnyCtDrAjhnD

L$_16_blocks_overflow_yvnmnyCtDrAjhnD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_yvnmnyCtDrAjhnD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CegxlpGktuzAtiy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CegxlpGktuzAtiy
L$_small_initial_partial_block_CegxlpGktuzAtiy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CegxlpGktuzAtiy:

	orq	%r8,%r8
	je	L$_after_reduction_CegxlpGktuzAtiy
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CegxlpGktuzAtiy:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_12_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_zcCCAFticBGlEbm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_zcCCAFticBGlEbm

L$_16_blocks_overflow_zcCCAFticBGlEbm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_zcCCAFticBGlEbm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tqfxAcDazxnsjyv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tqfxAcDazxnsjyv
L$_small_initial_partial_block_tqfxAcDazxnsjyv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tqfxAcDazxnsjyv:

	orq	%r8,%r8
	je	L$_after_reduction_tqfxAcDazxnsjyv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_tqfxAcDazxnsjyv:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_13_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_ddubByAisupcyee
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_ddubByAisupcyee

L$_16_blocks_overflow_ddubByAisupcyee:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_ddubByAisupcyee:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uAxqttiEzzElAdg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uAxqttiEzzElAdg
L$_small_initial_partial_block_uAxqttiEzzElAdg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uAxqttiEzzElAdg:

	orq	%r8,%r8
	je	L$_after_reduction_uAxqttiEzzElAdg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uAxqttiEzzElAdg:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_14_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_pmeloCqalhjattk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_pmeloCqalhjattk

L$_16_blocks_overflow_pmeloCqalhjattk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_pmeloCqalhjattk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rjkdpEzebrFyxwA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rjkdpEzebrFyxwA
L$_small_initial_partial_block_rjkdpEzebrFyxwA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rjkdpEzebrFyxwA:

	orq	%r8,%r8
	je	L$_after_reduction_rjkdpEzebrFyxwA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rjkdpEzebrFyxwA:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_15_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_zCtekflrEfjrbkn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_zCtekflrEfjrbkn

L$_16_blocks_overflow_zCtekflrEfjrbkn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_zCtekflrEfjrbkn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_omaCjiyGmqskvAn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_omaCjiyGmqskvAn
L$_small_initial_partial_block_omaCjiyGmqskvAn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_omaCjiyGmqskvAn:

	orq	%r8,%r8
	je	L$_after_reduction_omaCjiyGmqskvAn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_omaCjiyGmqskvAn:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_16_BzuiFDxlabjytFj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_xtDeAaenEgrfnBA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_xtDeAaenEgrfnBA

L$_16_blocks_overflow_xtDeAaenEgrfnBA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_xtDeAaenEgrfnBA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_ArpyzDappnEDdye:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ArpyzDappnEDdye:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ArpyzDappnEDdye:
	jmp	L$_last_blocks_done_BzuiFDxlabjytFj
L$_last_num_blocks_is_0_BzuiFDxlabjytFj:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_BzuiFDxlabjytFj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yjdkbBaokEBheCl
L$_encrypt_16_blocks_yjdkbBaokEBheCl:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_mevqtFadudylFvm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_mevqtFadudylFvm
L$_16_blocks_overflow_mevqtFadudylFvm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_mevqtFadudylFvm:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_DBFuiCodmsvobng

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_DBFuiCodmsvobng
	jb	L$_last_num_blocks_is_7_1_DBFuiCodmsvobng


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_DBFuiCodmsvobng
	jb	L$_last_num_blocks_is_11_9_DBFuiCodmsvobng


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_DBFuiCodmsvobng
	ja	L$_last_num_blocks_is_16_DBFuiCodmsvobng
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_DBFuiCodmsvobng
	jmp	L$_last_num_blocks_is_13_DBFuiCodmsvobng

L$_last_num_blocks_is_11_9_DBFuiCodmsvobng:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_DBFuiCodmsvobng
	ja	L$_last_num_blocks_is_11_DBFuiCodmsvobng
	jmp	L$_last_num_blocks_is_9_DBFuiCodmsvobng

L$_last_num_blocks_is_7_1_DBFuiCodmsvobng:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_DBFuiCodmsvobng
	jb	L$_last_num_blocks_is_3_1_DBFuiCodmsvobng

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_DBFuiCodmsvobng
	je	L$_last_num_blocks_is_6_DBFuiCodmsvobng
	jmp	L$_last_num_blocks_is_5_DBFuiCodmsvobng

L$_last_num_blocks_is_3_1_DBFuiCodmsvobng:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_DBFuiCodmsvobng
	je	L$_last_num_blocks_is_2_DBFuiCodmsvobng
L$_last_num_blocks_is_1_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_echdurxfbcgnCap
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_echdurxfbcgnCap

L$_16_blocks_overflow_echdurxfbcgnCap:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_echdurxfbcgnCap:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pooipltxFpakCDa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pooipltxFpakCDa
L$_small_initial_partial_block_pooipltxFpakCDa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_pooipltxFpakCDa
L$_small_initial_compute_done_pooipltxFpakCDa:
L$_after_reduction_pooipltxFpakCDa:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_2_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_BqpqDhmoDxwdxBx
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_BqpqDhmoDxwdxBx

L$_16_blocks_overflow_BqpqDhmoDxwdxBx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_BqpqDhmoDxwdxBx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GBuofflhzskasjs





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GBuofflhzskasjs
L$_small_initial_partial_block_GBuofflhzskasjs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GBuofflhzskasjs:

	orq	%r8,%r8
	je	L$_after_reduction_GBuofflhzskasjs
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GBuofflhzskasjs:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_3_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_BsBqDmlaGBiCAzf
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_BsBqDmlaGBiCAzf

L$_16_blocks_overflow_BsBqDmlaGBiCAzf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_BsBqDmlaGBiCAzf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bxcxfajeeEkihFC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bxcxfajeeEkihFC
L$_small_initial_partial_block_bxcxfajeeEkihFC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bxcxfajeeEkihFC:

	orq	%r8,%r8
	je	L$_after_reduction_bxcxfajeeEkihFC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_bxcxfajeeEkihFC:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_4_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_wktbytozAdtgfxn
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_wktbytozAdtgfxn

L$_16_blocks_overflow_wktbytozAdtgfxn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_wktbytozAdtgfxn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qwbssyEfgnnDzgB





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qwbssyEfgnnDzgB
L$_small_initial_partial_block_qwbssyEfgnnDzgB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qwbssyEfgnnDzgB:

	orq	%r8,%r8
	je	L$_after_reduction_qwbssyEfgnnDzgB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qwbssyEfgnnDzgB:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_5_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_hiqubBFAvwkFbED
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_hiqubBFAvwkFbED

L$_16_blocks_overflow_hiqubBFAvwkFbED:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_hiqubBFAvwkFbED:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DCFgkFpDjsyEhax





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DCFgkFpDjsyEhax
L$_small_initial_partial_block_DCFgkFpDjsyEhax:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DCFgkFpDjsyEhax:

	orq	%r8,%r8
	je	L$_after_reduction_DCFgkFpDjsyEhax
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DCFgkFpDjsyEhax:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_6_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_GjfwBrkancFnhew
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_GjfwBrkancFnhew

L$_16_blocks_overflow_GjfwBrkancFnhew:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_GjfwBrkancFnhew:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ddpcaoEfcvktwrD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ddpcaoEfcvktwrD
L$_small_initial_partial_block_ddpcaoEfcvktwrD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ddpcaoEfcvktwrD:

	orq	%r8,%r8
	je	L$_after_reduction_ddpcaoEfcvktwrD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ddpcaoEfcvktwrD:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_7_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_yzDkyathasvkfwn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_yzDkyathasvkfwn

L$_16_blocks_overflow_yzDkyathasvkfwn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_yzDkyathasvkfwn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zapobEomrgoqjfz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zapobEomrgoqjfz
L$_small_initial_partial_block_zapobEomrgoqjfz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zapobEomrgoqjfz:

	orq	%r8,%r8
	je	L$_after_reduction_zapobEomrgoqjfz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zapobEomrgoqjfz:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_8_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_dzihsABgfdvrxvE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_dzihsABgfdvrxvE

L$_16_blocks_overflow_dzihsABgfdvrxvE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_dzihsABgfdvrxvE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_eotrFxetsmvmCbr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_eotrFxetsmvmCbr
L$_small_initial_partial_block_eotrFxetsmvmCbr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_eotrFxetsmvmCbr:

	orq	%r8,%r8
	je	L$_after_reduction_eotrFxetsmvmCbr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_eotrFxetsmvmCbr:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_9_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_axfhehDkdjuzoyB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_axfhehDkdjuzoyB

L$_16_blocks_overflow_axfhehDkdjuzoyB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_axfhehDkdjuzoyB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uDyrjjrivFnqADr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uDyrjjrivFnqADr
L$_small_initial_partial_block_uDyrjjrivFnqADr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uDyrjjrivFnqADr:

	orq	%r8,%r8
	je	L$_after_reduction_uDyrjjrivFnqADr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uDyrjjrivFnqADr:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_10_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_vqDbdCczgkCFBBv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_vqDbdCczgkCFBBv

L$_16_blocks_overflow_vqDbdCczgkCFBBv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_vqDbdCczgkCFBBv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yEAgtnlzvgwispx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yEAgtnlzvgwispx
L$_small_initial_partial_block_yEAgtnlzvgwispx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yEAgtnlzvgwispx:

	orq	%r8,%r8
	je	L$_after_reduction_yEAgtnlzvgwispx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yEAgtnlzvgwispx:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_11_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_wgymkFDwGkhFviF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_wgymkFDwGkhFviF

L$_16_blocks_overflow_wgymkFDwGkhFviF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_wgymkFDwGkhFviF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kiGDGpkaarDomqd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kiGDGpkaarDomqd
L$_small_initial_partial_block_kiGDGpkaarDomqd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kiGDGpkaarDomqd:

	orq	%r8,%r8
	je	L$_after_reduction_kiGDGpkaarDomqd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kiGDGpkaarDomqd:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_12_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_fodzjDeCysfExax
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_fodzjDeCysfExax

L$_16_blocks_overflow_fodzjDeCysfExax:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_fodzjDeCysfExax:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cgcznsaclDdbjye





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cgcznsaclDdbjye
L$_small_initial_partial_block_cgcznsaclDdbjye:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cgcznsaclDdbjye:

	orq	%r8,%r8
	je	L$_after_reduction_cgcznsaclDdbjye
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_cgcznsaclDdbjye:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_13_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_iFjzgijlvzyzdsw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_iFjzgijlvzyzdsw

L$_16_blocks_overflow_iFjzgijlvzyzdsw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_iFjzgijlvzyzdsw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tfbzxBkuggnxxlA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tfbzxBkuggnxxlA
L$_small_initial_partial_block_tfbzxBkuggnxxlA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tfbzxBkuggnxxlA:

	orq	%r8,%r8
	je	L$_after_reduction_tfbzxBkuggnxxlA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_tfbzxBkuggnxxlA:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_14_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_sGjbbhrkmipxBzG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_sGjbbhrkmipxBzG

L$_16_blocks_overflow_sGjbbhrkmipxBzG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_sGjbbhrkmipxBzG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kndgBDpBlegDcox





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kndgBDpBlegDcox
L$_small_initial_partial_block_kndgBDpBlegDcox:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kndgBDpBlegDcox:

	orq	%r8,%r8
	je	L$_after_reduction_kndgBDpBlegDcox
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kndgBDpBlegDcox:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_15_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_BpGdogsuzmlddFy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_BpGdogsuzmlddFy

L$_16_blocks_overflow_BpGdogsuzmlddFy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_BpGdogsuzmlddFy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FbdbwzekAegdubo





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FbdbwzekAegdubo
L$_small_initial_partial_block_FbdbwzekAegdubo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FbdbwzekAegdubo:

	orq	%r8,%r8
	je	L$_after_reduction_FbdbwzekAegdubo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FbdbwzekAegdubo:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_16_DBFuiCodmsvobng:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_kmFBlvjFtbAkyql
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_kmFBlvjFtbAkyql

L$_16_blocks_overflow_kmFBlvjFtbAkyql:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_kmFBlvjFtbAkyql:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_CawvmyhpCBzranf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CawvmyhpCBzranf:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CawvmyhpCBzranf:
	jmp	L$_last_blocks_done_DBFuiCodmsvobng
L$_last_num_blocks_is_0_DBFuiCodmsvobng:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_DBFuiCodmsvobng:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yjdkbBaokEBheCl

L$_message_below_32_blocks_yjdkbBaokEBheCl:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_kGDveAGumACnpqp
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
L$_skip_hkeys_precomputation_kGDveAGumACnpqp:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_wFefnbayxEddegz

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_wFefnbayxEddegz
	jb	L$_last_num_blocks_is_7_1_wFefnbayxEddegz


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_wFefnbayxEddegz
	jb	L$_last_num_blocks_is_11_9_wFefnbayxEddegz


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_wFefnbayxEddegz
	ja	L$_last_num_blocks_is_16_wFefnbayxEddegz
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_wFefnbayxEddegz
	jmp	L$_last_num_blocks_is_13_wFefnbayxEddegz

L$_last_num_blocks_is_11_9_wFefnbayxEddegz:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_wFefnbayxEddegz
	ja	L$_last_num_blocks_is_11_wFefnbayxEddegz
	jmp	L$_last_num_blocks_is_9_wFefnbayxEddegz

L$_last_num_blocks_is_7_1_wFefnbayxEddegz:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_wFefnbayxEddegz
	jb	L$_last_num_blocks_is_3_1_wFefnbayxEddegz

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_wFefnbayxEddegz
	je	L$_last_num_blocks_is_6_wFefnbayxEddegz
	jmp	L$_last_num_blocks_is_5_wFefnbayxEddegz

L$_last_num_blocks_is_3_1_wFefnbayxEddegz:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_wFefnbayxEddegz
	je	L$_last_num_blocks_is_2_wFefnbayxEddegz
L$_last_num_blocks_is_1_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_ckBmyrjcyhAqyvc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_ckBmyrjcyhAqyvc

L$_16_blocks_overflow_ckBmyrjcyhAqyvc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_ckBmyrjcyhAqyvc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BthriEdboxggmvh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BthriEdboxggmvh
L$_small_initial_partial_block_BthriEdboxggmvh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_BthriEdboxggmvh
L$_small_initial_compute_done_BthriEdboxggmvh:
L$_after_reduction_BthriEdboxggmvh:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_2_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_GlxwjuccBypdwyt
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_GlxwjuccBypdwyt

L$_16_blocks_overflow_GlxwjuccBypdwyt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_GlxwjuccBypdwyt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fiyFnzltmfClBDp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fiyFnzltmfClBDp
L$_small_initial_partial_block_fiyFnzltmfClBDp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fiyFnzltmfClBDp:

	orq	%r8,%r8
	je	L$_after_reduction_fiyFnzltmfClBDp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fiyFnzltmfClBDp:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_3_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_jwGlnAocfvgrmbD
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_jwGlnAocfvgrmbD

L$_16_blocks_overflow_jwGlnAocfvgrmbD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_jwGlnAocfvgrmbD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EdaBjEppspoGzCa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EdaBjEppspoGzCa
L$_small_initial_partial_block_EdaBjEppspoGzCa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EdaBjEppspoGzCa:

	orq	%r8,%r8
	je	L$_after_reduction_EdaBjEppspoGzCa
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EdaBjEppspoGzCa:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_4_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_dymCuGwtBywxEqj
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_dymCuGwtBywxEqj

L$_16_blocks_overflow_dymCuGwtBywxEqj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_dymCuGwtBywxEqj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yqqbrEofthunnyr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yqqbrEofthunnyr
L$_small_initial_partial_block_yqqbrEofthunnyr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yqqbrEofthunnyr:

	orq	%r8,%r8
	je	L$_after_reduction_yqqbrEofthunnyr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yqqbrEofthunnyr:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_5_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_ChiDwFfiFrrxCAb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_ChiDwFfiFrrxCAb

L$_16_blocks_overflow_ChiDwFfiFrrxCAb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_ChiDwFfiFrrxCAb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AFBFbgrFcuEEApt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AFBFbgrFcuEEApt
L$_small_initial_partial_block_AFBFbgrFcuEEApt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AFBFbgrFcuEEApt:

	orq	%r8,%r8
	je	L$_after_reduction_AFBFbgrFcuEEApt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AFBFbgrFcuEEApt:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_6_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_cfkcujheltedsrp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_cfkcujheltedsrp

L$_16_blocks_overflow_cfkcujheltedsrp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_cfkcujheltedsrp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ijoGalGdAfnbeyh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ijoGalGdAfnbeyh
L$_small_initial_partial_block_ijoGalGdAfnbeyh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ijoGalGdAfnbeyh:

	orq	%r8,%r8
	je	L$_after_reduction_ijoGalGdAfnbeyh
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ijoGalGdAfnbeyh:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_7_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_uchnxdzvsguwsdq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_uchnxdzvsguwsdq

L$_16_blocks_overflow_uchnxdzvsguwsdq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_uchnxdzvsguwsdq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BuyoEdwBoctEfhe





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BuyoEdwBoctEfhe
L$_small_initial_partial_block_BuyoEdwBoctEfhe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BuyoEdwBoctEfhe:

	orq	%r8,%r8
	je	L$_after_reduction_BuyoEdwBoctEfhe
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BuyoEdwBoctEfhe:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_8_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_GytjkxjvDmEbyrg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_GytjkxjvDmEbyrg

L$_16_blocks_overflow_GytjkxjvDmEbyrg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_GytjkxjvDmEbyrg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_oDcvGrwAnchxcie





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_oDcvGrwAnchxcie
L$_small_initial_partial_block_oDcvGrwAnchxcie:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_oDcvGrwAnchxcie:

	orq	%r8,%r8
	je	L$_after_reduction_oDcvGrwAnchxcie
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_oDcvGrwAnchxcie:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_9_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_wnsAADbwoxuhbbx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_wnsAADbwoxuhbbx

L$_16_blocks_overflow_wnsAADbwoxuhbbx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_wnsAADbwoxuhbbx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bipskkbiwqAzdcw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bipskkbiwqAzdcw
L$_small_initial_partial_block_bipskkbiwqAzdcw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bipskkbiwqAzdcw:

	orq	%r8,%r8
	je	L$_after_reduction_bipskkbiwqAzdcw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_bipskkbiwqAzdcw:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_10_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_rmxggGlslsrucvB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_rmxggGlslsrucvB

L$_16_blocks_overflow_rmxggGlslsrucvB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_rmxggGlslsrucvB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dEwADddtwcwFtip





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dEwADddtwcwFtip
L$_small_initial_partial_block_dEwADddtwcwFtip:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dEwADddtwcwFtip:

	orq	%r8,%r8
	je	L$_after_reduction_dEwADddtwcwFtip
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_dEwADddtwcwFtip:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_11_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_GCnAmthBjmzElAq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_GCnAmthBjmzElAq

L$_16_blocks_overflow_GCnAmthBjmzElAq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_GCnAmthBjmzElAq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lsmeqFnbwttCzgC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lsmeqFnbwttCzgC
L$_small_initial_partial_block_lsmeqFnbwttCzgC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lsmeqFnbwttCzgC:

	orq	%r8,%r8
	je	L$_after_reduction_lsmeqFnbwttCzgC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_lsmeqFnbwttCzgC:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_12_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_aGdjturnfFsuvom
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_aGdjturnfFsuvom

L$_16_blocks_overflow_aGdjturnfFsuvom:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_aGdjturnfFsuvom:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pyuDGhthxcEAryo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pyuDGhthxcEAryo
L$_small_initial_partial_block_pyuDGhthxcEAryo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pyuDGhthxcEAryo:

	orq	%r8,%r8
	je	L$_after_reduction_pyuDGhthxcEAryo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pyuDGhthxcEAryo:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_13_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_EsvwkyaFCGFtwtz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_EsvwkyaFCGFtwtz

L$_16_blocks_overflow_EsvwkyaFCGFtwtz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_EsvwkyaFCGFtwtz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GrfpbuiEvckfzFj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GrfpbuiEvckfzFj
L$_small_initial_partial_block_GrfpbuiEvckfzFj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GrfpbuiEvckfzFj:

	orq	%r8,%r8
	je	L$_after_reduction_GrfpbuiEvckfzFj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GrfpbuiEvckfzFj:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_14_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_BzcekexlGobtaoB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_BzcekexlGobtaoB

L$_16_blocks_overflow_BzcekexlGobtaoB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_BzcekexlGobtaoB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zhBzprrianDbwzr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zhBzprrianDbwzr
L$_small_initial_partial_block_zhBzprrianDbwzr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zhBzprrianDbwzr:

	orq	%r8,%r8
	je	L$_after_reduction_zhBzprrianDbwzr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zhBzprrianDbwzr:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_15_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_DtgwgmGaCnuBACv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_DtgwgmGaCnuBACv

L$_16_blocks_overflow_DtgwgmGaCnuBACv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_DtgwgmGaCnuBACv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GuyvaepvfEkvqxi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GuyvaepvfEkvqxi
L$_small_initial_partial_block_GuyvaepvfEkvqxi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GuyvaepvfEkvqxi:

	orq	%r8,%r8
	je	L$_after_reduction_GuyvaepvfEkvqxi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GuyvaepvfEkvqxi:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_16_wFefnbayxEddegz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_ApeaqAytGABCbBc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ApeaqAytGABCbBc

L$_16_blocks_overflow_ApeaqAytGABCbBc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ApeaqAytGABCbBc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_dsdszoEwhlhthFE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dsdszoEwhlhthFE:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_dsdszoEwhlhthFE:
	jmp	L$_last_blocks_done_wFefnbayxEddegz
L$_last_num_blocks_is_0_wFefnbayxEddegz:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_wFefnbayxEddegz:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yjdkbBaokEBheCl

L$_message_below_equal_16_blocks_yjdkbBaokEBheCl:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	L$_small_initial_num_blocks_is_8_mxsiknFqjzeykFm
	jl	L$_small_initial_num_blocks_is_7_1_mxsiknFqjzeykFm


	cmpq	$12,%r12
	je	L$_small_initial_num_blocks_is_12_mxsiknFqjzeykFm
	jl	L$_small_initial_num_blocks_is_11_9_mxsiknFqjzeykFm


	cmpq	$16,%r12
	je	L$_small_initial_num_blocks_is_16_mxsiknFqjzeykFm
	cmpq	$15,%r12
	je	L$_small_initial_num_blocks_is_15_mxsiknFqjzeykFm
	cmpq	$14,%r12
	je	L$_small_initial_num_blocks_is_14_mxsiknFqjzeykFm
	jmp	L$_small_initial_num_blocks_is_13_mxsiknFqjzeykFm

L$_small_initial_num_blocks_is_11_9_mxsiknFqjzeykFm:

	cmpq	$11,%r12
	je	L$_small_initial_num_blocks_is_11_mxsiknFqjzeykFm
	cmpq	$10,%r12
	je	L$_small_initial_num_blocks_is_10_mxsiknFqjzeykFm
	jmp	L$_small_initial_num_blocks_is_9_mxsiknFqjzeykFm

L$_small_initial_num_blocks_is_7_1_mxsiknFqjzeykFm:
	cmpq	$4,%r12
	je	L$_small_initial_num_blocks_is_4_mxsiknFqjzeykFm
	jl	L$_small_initial_num_blocks_is_3_1_mxsiknFqjzeykFm

	cmpq	$7,%r12
	je	L$_small_initial_num_blocks_is_7_mxsiknFqjzeykFm
	cmpq	$6,%r12
	je	L$_small_initial_num_blocks_is_6_mxsiknFqjzeykFm
	jmp	L$_small_initial_num_blocks_is_5_mxsiknFqjzeykFm

L$_small_initial_num_blocks_is_3_1_mxsiknFqjzeykFm:

	cmpq	$3,%r12
	je	L$_small_initial_num_blocks_is_3_mxsiknFqjzeykFm
	cmpq	$2,%r12
	je	L$_small_initial_num_blocks_is_2_mxsiknFqjzeykFm





L$_small_initial_num_blocks_is_1_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zliyFAapFCmoGoi





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zliyFAapFCmoGoi
L$_small_initial_partial_block_zliyFAapFCmoGoi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	L$_after_reduction_zliyFAapFCmoGoi
L$_small_initial_compute_done_zliyFAapFCmoGoi:
L$_after_reduction_zliyFAapFCmoGoi:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_2_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EDzgxCsCczthCbt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EDzgxCsCczthCbt
L$_small_initial_partial_block_EDzgxCsCczthCbt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EDzgxCsCczthCbt:

	orq	%r8,%r8
	je	L$_after_reduction_EDzgxCsCczthCbt
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EDzgxCsCczthCbt:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_3_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BanzylqFkaqwnnv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BanzylqFkaqwnnv
L$_small_initial_partial_block_BanzylqFkaqwnnv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BanzylqFkaqwnnv:

	orq	%r8,%r8
	je	L$_after_reduction_BanzylqFkaqwnnv
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_BanzylqFkaqwnnv:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_4_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mrzrfwgjxzqbzhh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mrzrfwgjxzqbzhh
L$_small_initial_partial_block_mrzrfwgjxzqbzhh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mrzrfwgjxzqbzhh:

	orq	%r8,%r8
	je	L$_after_reduction_mrzrfwgjxzqbzhh
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_mrzrfwgjxzqbzhh:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_5_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nxiCDmjAFcABFvb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nxiCDmjAFcABFvb
L$_small_initial_partial_block_nxiCDmjAFcABFvb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nxiCDmjAFcABFvb:

	orq	%r8,%r8
	je	L$_after_reduction_nxiCDmjAFcABFvb
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_nxiCDmjAFcABFvb:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_6_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xgbzeBuoCthuECd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xgbzeBuoCthuECd
L$_small_initial_partial_block_xgbzeBuoCthuECd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xgbzeBuoCthuECd:

	orq	%r8,%r8
	je	L$_after_reduction_xgbzeBuoCthuECd
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_xgbzeBuoCthuECd:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_7_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EbpkiEsukfdpBqd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EbpkiEsukfdpBqd
L$_small_initial_partial_block_EbpkiEsukfdpBqd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EbpkiEsukfdpBqd:

	orq	%r8,%r8
	je	L$_after_reduction_EbpkiEsukfdpBqd
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EbpkiEsukfdpBqd:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_8_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EuykvdakCtpAdCz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EuykvdakCtpAdCz
L$_small_initial_partial_block_EuykvdakCtpAdCz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EuykvdakCtpAdCz:

	orq	%r8,%r8
	je	L$_after_reduction_EuykvdakCtpAdCz
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EuykvdakCtpAdCz:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_9_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tDnjczBAcprAubc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tDnjczBAcprAubc
L$_small_initial_partial_block_tDnjczBAcprAubc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tDnjczBAcprAubc:

	orq	%r8,%r8
	je	L$_after_reduction_tDnjczBAcprAubc
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_tDnjczBAcprAubc:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_10_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jlAoucmpojCsdku





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jlAoucmpojCsdku
L$_small_initial_partial_block_jlAoucmpojCsdku:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jlAoucmpojCsdku:

	orq	%r8,%r8
	je	L$_after_reduction_jlAoucmpojCsdku
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_jlAoucmpojCsdku:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_11_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_odbkfhdFuugFCAC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_odbkfhdFuugFCAC
L$_small_initial_partial_block_odbkfhdFuugFCAC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_odbkfhdFuugFCAC:

	orq	%r8,%r8
	je	L$_after_reduction_odbkfhdFuugFCAC
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_odbkfhdFuugFCAC:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_12_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jszteAFbAcgEnGG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jszteAFbAcgEnGG
L$_small_initial_partial_block_jszteAFbAcgEnGG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jszteAFbAcgEnGG:

	orq	%r8,%r8
	je	L$_after_reduction_jszteAFbAcgEnGG
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_jszteAFbAcgEnGG:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_13_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aDrtCcrFdjyqepz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aDrtCcrFdjyqepz
L$_small_initial_partial_block_aDrtCcrFdjyqepz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aDrtCcrFdjyqepz:

	orq	%r8,%r8
	je	L$_after_reduction_aDrtCcrFdjyqepz
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_aDrtCcrFdjyqepz:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_14_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gaycgyepiCwidhj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gaycgyepiCwidhj
L$_small_initial_partial_block_gaycgyepiCwidhj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gaycgyepiCwidhj:

	orq	%r8,%r8
	je	L$_after_reduction_gaycgyepiCwidhj
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_gaycgyepiCwidhj:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_15_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iGxfAedDnzytobn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iGxfAedDnzytobn
L$_small_initial_partial_block_iGxfAedDnzytobn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iGxfAedDnzytobn:

	orq	%r8,%r8
	je	L$_after_reduction_iGxfAedDnzytobn
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_iGxfAedDnzytobn:
	jmp	L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm
L$_small_initial_num_blocks_is_16_mxsiknFqjzeykFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_mEBvkwACdshgyrt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mEBvkwACdshgyrt:
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_mEBvkwACdshgyrt:
L$_small_initial_blocks_encrypted_mxsiknFqjzeykFm:
L$_ghash_done_yjdkbBaokEBheCl:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
L$_enc_dec_done_yjdkbBaokEBheCl:
	jmp	L$exit_gcm_encrypt
.p2align	5
L$aes_gcm_encrypt_256_avx512:
	orq	%r8,%r8
	je	L$_enc_dec_done_yksdxkGCqciwCCF
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	L$_partial_block_done_GijEBuygsiqFDpc
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	L$_no_extra_mask_GijEBuygsiqFDpc
	subq	%r13,%r12
L$_no_extra_mask_GijEBuygsiqFDpc:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	L$_partial_incomplete_GijEBuygsiqFDpc

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	L$_enc_dec_done_GijEBuygsiqFDpc

L$_partial_incomplete_GijEBuygsiqFDpc:
	addq	%r8,(%rdx)
	movq	%r8,%r11

L$_enc_dec_done_GijEBuygsiqFDpc:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
L$_partial_block_done_GijEBuygsiqFDpc:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	L$_enc_dec_done_yksdxkGCqciwCCF
	cmpq	$256,%r8
	jbe	L$_message_below_equal_16_blocks_yksdxkGCqciwCCF

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_EBEhwFnvBFADjEm
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_EBEhwFnvBFADjEm
L$_next_16_overflow_EBEhwFnvBFADjEm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_EBEhwFnvBFADjEm:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_igCuybFkhbEzgaq

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
L$_skip_hkeys_precomputation_igCuybFkhbEzgaq:
	cmpq	$512,%r8
	jb	L$_message_below_32_blocks_yksdxkGCqciwCCF



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_yjvBwdcgGgspEEy
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_yjvBwdcgGgspEEy
L$_next_16_overflow_yjvBwdcgGgspEEy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_yjvBwdcgGgspEEy:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_ADAyFvxjlefdjjt
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
L$_skip_hkeys_precomputation_ADAyFvxjlefdjjt:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	L$_no_more_big_nblocks_yksdxkGCqciwCCF
L$_encrypt_big_nblocks_yksdxkGCqciwCCF:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_ainiDsjbExnetjf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ainiDsjbExnetjf
L$_16_blocks_overflow_ainiDsjbExnetjf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ainiDsjbExnetjf:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_ipeGfnhCfBmxBzc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ipeGfnhCfBmxBzc
L$_16_blocks_overflow_ipeGfnhCfBmxBzc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ipeGfnhCfBmxBzc:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_nmxEaBdynGngfAz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_nmxEaBdynGngfAz
L$_16_blocks_overflow_nmxEaBdynGngfAz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_nmxEaBdynGngfAz:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	L$_encrypt_big_nblocks_yksdxkGCqciwCCF

L$_no_more_big_nblocks_yksdxkGCqciwCCF:

	cmpq	$512,%r8
	jae	L$_encrypt_32_blocks_yksdxkGCqciwCCF

	cmpq	$256,%r8
	jae	L$_encrypt_16_blocks_yksdxkGCqciwCCF
L$_encrypt_0_blocks_ghash_32_yksdxkGCqciwCCF:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_BisBiknnrtyDxam

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_BisBiknnrtyDxam
	jb	L$_last_num_blocks_is_7_1_BisBiknnrtyDxam


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_BisBiknnrtyDxam
	jb	L$_last_num_blocks_is_11_9_BisBiknnrtyDxam


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_BisBiknnrtyDxam
	ja	L$_last_num_blocks_is_16_BisBiknnrtyDxam
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_BisBiknnrtyDxam
	jmp	L$_last_num_blocks_is_13_BisBiknnrtyDxam

L$_last_num_blocks_is_11_9_BisBiknnrtyDxam:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_BisBiknnrtyDxam
	ja	L$_last_num_blocks_is_11_BisBiknnrtyDxam
	jmp	L$_last_num_blocks_is_9_BisBiknnrtyDxam

L$_last_num_blocks_is_7_1_BisBiknnrtyDxam:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_BisBiknnrtyDxam
	jb	L$_last_num_blocks_is_3_1_BisBiknnrtyDxam

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_BisBiknnrtyDxam
	je	L$_last_num_blocks_is_6_BisBiknnrtyDxam
	jmp	L$_last_num_blocks_is_5_BisBiknnrtyDxam

L$_last_num_blocks_is_3_1_BisBiknnrtyDxam:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_BisBiknnrtyDxam
	je	L$_last_num_blocks_is_2_BisBiknnrtyDxam
L$_last_num_blocks_is_1_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_exiCvfFcktAFCEu
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_exiCvfFcktAFCEu

L$_16_blocks_overflow_exiCvfFcktAFCEu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_exiCvfFcktAFCEu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DEdlkqxqzGzmmyr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DEdlkqxqzGzmmyr
L$_small_initial_partial_block_DEdlkqxqzGzmmyr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_DEdlkqxqzGzmmyr
L$_small_initial_compute_done_DEdlkqxqzGzmmyr:
L$_after_reduction_DEdlkqxqzGzmmyr:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_2_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_gwmlrgBxliBryue
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_gwmlrgBxliBryue

L$_16_blocks_overflow_gwmlrgBxliBryue:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_gwmlrgBxliBryue:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lFaBoApyjokpprG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lFaBoApyjokpprG
L$_small_initial_partial_block_lFaBoApyjokpprG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lFaBoApyjokpprG:

	orq	%r8,%r8
	je	L$_after_reduction_lFaBoApyjokpprG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_lFaBoApyjokpprG:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_3_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_iauucgwCvnuvkvo
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_iauucgwCvnuvkvo

L$_16_blocks_overflow_iauucgwCvnuvkvo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_iauucgwCvnuvkvo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_abDyypojuCBGuFb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_abDyypojuCBGuFb
L$_small_initial_partial_block_abDyypojuCBGuFb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_abDyypojuCBGuFb:

	orq	%r8,%r8
	je	L$_after_reduction_abDyypojuCBGuFb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_abDyypojuCBGuFb:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_4_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_pktGiGbBkiyGkaC
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_pktGiGbBkiyGkaC

L$_16_blocks_overflow_pktGiGbBkiyGkaC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_pktGiGbBkiyGkaC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dfsEhbzGCkrypsm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dfsEhbzGCkrypsm
L$_small_initial_partial_block_dfsEhbzGCkrypsm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dfsEhbzGCkrypsm:

	orq	%r8,%r8
	je	L$_after_reduction_dfsEhbzGCkrypsm
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_dfsEhbzGCkrypsm:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_5_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_uwrqmvqaDduuwyy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_uwrqmvqaDduuwyy

L$_16_blocks_overflow_uwrqmvqaDduuwyy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_uwrqmvqaDduuwyy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vwoFpcGjwvndavt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vwoFpcGjwvndavt
L$_small_initial_partial_block_vwoFpcGjwvndavt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vwoFpcGjwvndavt:

	orq	%r8,%r8
	je	L$_after_reduction_vwoFpcGjwvndavt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vwoFpcGjwvndavt:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_6_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_iahFobcxktdAjzy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_iahFobcxktdAjzy

L$_16_blocks_overflow_iahFobcxktdAjzy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_iahFobcxktdAjzy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aAquwxntFwzszbl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aAquwxntFwzszbl
L$_small_initial_partial_block_aAquwxntFwzszbl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aAquwxntFwzszbl:

	orq	%r8,%r8
	je	L$_after_reduction_aAquwxntFwzszbl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aAquwxntFwzszbl:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_7_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_jdempCctCqAAGeg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_jdempCctCqAAGeg

L$_16_blocks_overflow_jdempCctCqAAGeg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_jdempCctCqAAGeg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rtlpvrnlhayarno





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rtlpvrnlhayarno
L$_small_initial_partial_block_rtlpvrnlhayarno:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rtlpvrnlhayarno:

	orq	%r8,%r8
	je	L$_after_reduction_rtlpvrnlhayarno
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rtlpvrnlhayarno:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_8_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_nEGrreyvczuoGpD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_nEGrreyvczuoGpD

L$_16_blocks_overflow_nEGrreyvczuoGpD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_nEGrreyvczuoGpD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FusDyyziwkqgyvj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FusDyyziwkqgyvj
L$_small_initial_partial_block_FusDyyziwkqgyvj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FusDyyziwkqgyvj:

	orq	%r8,%r8
	je	L$_after_reduction_FusDyyziwkqgyvj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FusDyyziwkqgyvj:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_9_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_arusgtcphkAepte
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_arusgtcphkAepte

L$_16_blocks_overflow_arusgtcphkAepte:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_arusgtcphkAepte:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CtwoibnGeqzuvhw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CtwoibnGeqzuvhw
L$_small_initial_partial_block_CtwoibnGeqzuvhw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CtwoibnGeqzuvhw:

	orq	%r8,%r8
	je	L$_after_reduction_CtwoibnGeqzuvhw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CtwoibnGeqzuvhw:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_10_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_mezBEEdinmCrbfn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_mezBEEdinmCrbfn

L$_16_blocks_overflow_mezBEEdinmCrbfn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_mezBEEdinmCrbfn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_omlxasFqtDpmDCF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_omlxasFqtDpmDCF
L$_small_initial_partial_block_omlxasFqtDpmDCF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_omlxasFqtDpmDCF:

	orq	%r8,%r8
	je	L$_after_reduction_omlxasFqtDpmDCF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_omlxasFqtDpmDCF:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_11_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_nusoeumumnqoslF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_nusoeumumnqoslF

L$_16_blocks_overflow_nusoeumumnqoslF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_nusoeumumnqoslF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dvEtmnppbpGBmEw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dvEtmnppbpGBmEw
L$_small_initial_partial_block_dvEtmnppbpGBmEw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dvEtmnppbpGBmEw:

	orq	%r8,%r8
	je	L$_after_reduction_dvEtmnppbpGBmEw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_dvEtmnppbpGBmEw:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_12_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_BslgorAsprmjdFb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_BslgorAsprmjdFb

L$_16_blocks_overflow_BslgorAsprmjdFb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_BslgorAsprmjdFb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rCdzoBletdokhzy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rCdzoBletdokhzy
L$_small_initial_partial_block_rCdzoBletdokhzy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rCdzoBletdokhzy:

	orq	%r8,%r8
	je	L$_after_reduction_rCdzoBletdokhzy
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rCdzoBletdokhzy:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_13_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_aycyerivBwDwlap
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_aycyerivBwDwlap

L$_16_blocks_overflow_aycyerivBwDwlap:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_aycyerivBwDwlap:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tcqsewoFBqzyfGG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tcqsewoFBqzyfGG
L$_small_initial_partial_block_tcqsewoFBqzyfGG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tcqsewoFBqzyfGG:

	orq	%r8,%r8
	je	L$_after_reduction_tcqsewoFBqzyfGG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_tcqsewoFBqzyfGG:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_14_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_uvqyjusrEtChcmE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_uvqyjusrEtChcmE

L$_16_blocks_overflow_uvqyjusrEtChcmE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_uvqyjusrEtChcmE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kcvrzDjuzyophec





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kcvrzDjuzyophec
L$_small_initial_partial_block_kcvrzDjuzyophec:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kcvrzDjuzyophec:

	orq	%r8,%r8
	je	L$_after_reduction_kcvrzDjuzyophec
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kcvrzDjuzyophec:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_15_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_pGcGxEjcqppgDvz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_pGcGxEjcqppgDvz

L$_16_blocks_overflow_pGcGxEjcqppgDvz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_pGcGxEjcqppgDvz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_Cufdrtlkuxqilok





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_Cufdrtlkuxqilok
L$_small_initial_partial_block_Cufdrtlkuxqilok:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_Cufdrtlkuxqilok:

	orq	%r8,%r8
	je	L$_after_reduction_Cufdrtlkuxqilok
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_Cufdrtlkuxqilok:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_16_BisBiknnrtyDxam:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_zyGyaDbAdzfjvzf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_zyGyaDbAdzfjvzf

L$_16_blocks_overflow_zyGyaDbAdzfjvzf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_zyGyaDbAdzfjvzf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_nnlctsGyjvsrCyp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nnlctsGyjvsrCyp:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nnlctsGyjvsrCyp:
	jmp	L$_last_blocks_done_BisBiknnrtyDxam
L$_last_num_blocks_is_0_BisBiknnrtyDxam:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_BisBiknnrtyDxam:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yksdxkGCqciwCCF
L$_encrypt_32_blocks_yksdxkGCqciwCCF:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_xCopmihdcntaBbB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_xCopmihdcntaBbB
L$_16_blocks_overflow_xCopmihdcntaBbB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_xCopmihdcntaBbB:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_hGdwrxqbqclqCll
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_hGdwrxqbqclqCll
L$_16_blocks_overflow_hGdwrxqbqclqCll:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_hGdwrxqbqclqCll:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_tazGzuomBntDGcg

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_tazGzuomBntDGcg
	jb	L$_last_num_blocks_is_7_1_tazGzuomBntDGcg


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_tazGzuomBntDGcg
	jb	L$_last_num_blocks_is_11_9_tazGzuomBntDGcg


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_tazGzuomBntDGcg
	ja	L$_last_num_blocks_is_16_tazGzuomBntDGcg
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_tazGzuomBntDGcg
	jmp	L$_last_num_blocks_is_13_tazGzuomBntDGcg

L$_last_num_blocks_is_11_9_tazGzuomBntDGcg:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_tazGzuomBntDGcg
	ja	L$_last_num_blocks_is_11_tazGzuomBntDGcg
	jmp	L$_last_num_blocks_is_9_tazGzuomBntDGcg

L$_last_num_blocks_is_7_1_tazGzuomBntDGcg:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_tazGzuomBntDGcg
	jb	L$_last_num_blocks_is_3_1_tazGzuomBntDGcg

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_tazGzuomBntDGcg
	je	L$_last_num_blocks_is_6_tazGzuomBntDGcg
	jmp	L$_last_num_blocks_is_5_tazGzuomBntDGcg

L$_last_num_blocks_is_3_1_tazGzuomBntDGcg:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_tazGzuomBntDGcg
	je	L$_last_num_blocks_is_2_tazGzuomBntDGcg
L$_last_num_blocks_is_1_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_cDfieqjntvpmbmc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_cDfieqjntvpmbmc

L$_16_blocks_overflow_cDfieqjntvpmbmc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_cDfieqjntvpmbmc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rGdFkfjDGgcwmlg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rGdFkfjDGgcwmlg
L$_small_initial_partial_block_rGdFkfjDGgcwmlg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_rGdFkfjDGgcwmlg
L$_small_initial_compute_done_rGdFkfjDGgcwmlg:
L$_after_reduction_rGdFkfjDGgcwmlg:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_2_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_sgjeBeEufAjawpf
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_sgjeBeEufAjawpf

L$_16_blocks_overflow_sgjeBeEufAjawpf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_sgjeBeEufAjawpf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_csiEczunaAeCsBw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_csiEczunaAeCsBw
L$_small_initial_partial_block_csiEczunaAeCsBw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_csiEczunaAeCsBw:

	orq	%r8,%r8
	je	L$_after_reduction_csiEczunaAeCsBw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_csiEczunaAeCsBw:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_3_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_obutjqAivqzGCvi
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_obutjqAivqzGCvi

L$_16_blocks_overflow_obutjqAivqzGCvi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_obutjqAivqzGCvi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hmDbubkGywftjyb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hmDbubkGywftjyb
L$_small_initial_partial_block_hmDbubkGywftjyb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hmDbubkGywftjyb:

	orq	%r8,%r8
	je	L$_after_reduction_hmDbubkGywftjyb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hmDbubkGywftjyb:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_4_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_crvdvFswuikrgrq
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_crvdvFswuikrgrq

L$_16_blocks_overflow_crvdvFswuikrgrq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_crvdvFswuikrgrq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uecDgteGcirDaAn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uecDgteGcirDaAn
L$_small_initial_partial_block_uecDgteGcirDaAn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uecDgteGcirDaAn:

	orq	%r8,%r8
	je	L$_after_reduction_uecDgteGcirDaAn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uecDgteGcirDaAn:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_5_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_FCpBCmallxhCrFe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_FCpBCmallxhCrFe

L$_16_blocks_overflow_FCpBCmallxhCrFe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_FCpBCmallxhCrFe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_sllEanuojGceAvy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_sllEanuojGceAvy
L$_small_initial_partial_block_sllEanuojGceAvy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_sllEanuojGceAvy:

	orq	%r8,%r8
	je	L$_after_reduction_sllEanuojGceAvy
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_sllEanuojGceAvy:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_6_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_sDEouhnEsuDcjax
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_sDEouhnEsuDcjax

L$_16_blocks_overflow_sDEouhnEsuDcjax:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_sDEouhnEsuDcjax:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GgbbtpaECDEoBfy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GgbbtpaECDEoBfy
L$_small_initial_partial_block_GgbbtpaECDEoBfy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GgbbtpaECDEoBfy:

	orq	%r8,%r8
	je	L$_after_reduction_GgbbtpaECDEoBfy
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GgbbtpaECDEoBfy:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_7_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_zcpywyyshgzdcwj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_zcpywyyshgzdcwj

L$_16_blocks_overflow_zcpywyyshgzdcwj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_zcpywyyshgzdcwj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EAjBovDslFFglbn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EAjBovDslFFglbn
L$_small_initial_partial_block_EAjBovDslFFglbn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EAjBovDslFFglbn:

	orq	%r8,%r8
	je	L$_after_reduction_EAjBovDslFFglbn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EAjBovDslFFglbn:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_8_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_apsdgidDtoDdxdp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_apsdgidDtoDdxdp

L$_16_blocks_overflow_apsdgidDtoDdxdp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_apsdgidDtoDdxdp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_erCnhlimakDyGwj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_erCnhlimakDyGwj
L$_small_initial_partial_block_erCnhlimakDyGwj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_erCnhlimakDyGwj:

	orq	%r8,%r8
	je	L$_after_reduction_erCnhlimakDyGwj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_erCnhlimakDyGwj:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_9_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_BivszlCjBrcglEb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_BivszlCjBrcglEb

L$_16_blocks_overflow_BivszlCjBrcglEb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_BivszlCjBrcglEb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CfFcduhEjxzdmrx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CfFcduhEjxzdmrx
L$_small_initial_partial_block_CfFcduhEjxzdmrx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CfFcduhEjxzdmrx:

	orq	%r8,%r8
	je	L$_after_reduction_CfFcduhEjxzdmrx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CfFcduhEjxzdmrx:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_10_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_rEtFzFsexyGkyaw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_rEtFzFsexyGkyaw

L$_16_blocks_overflow_rEtFzFsexyGkyaw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_rEtFzFsexyGkyaw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fnbndjnetfohbtD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fnbndjnetfohbtD
L$_small_initial_partial_block_fnbndjnetfohbtD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fnbndjnetfohbtD:

	orq	%r8,%r8
	je	L$_after_reduction_fnbndjnetfohbtD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fnbndjnetfohbtD:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_11_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_gmaijydEyzzyDny
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_gmaijydEyzzyDny

L$_16_blocks_overflow_gmaijydEyzzyDny:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_gmaijydEyzzyDny:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nBFDalloDGEbmCb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nBFDalloDGEbmCb
L$_small_initial_partial_block_nBFDalloDGEbmCb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nBFDalloDGEbmCb:

	orq	%r8,%r8
	je	L$_after_reduction_nBFDalloDGEbmCb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nBFDalloDGEbmCb:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_12_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_CfvfoEGAaallfem
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_CfvfoEGAaallfem

L$_16_blocks_overflow_CfvfoEGAaallfem:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_CfvfoEGAaallfem:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ByzEpdnxeddBCkF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ByzEpdnxeddBCkF
L$_small_initial_partial_block_ByzEpdnxeddBCkF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ByzEpdnxeddBCkF:

	orq	%r8,%r8
	je	L$_after_reduction_ByzEpdnxeddBCkF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ByzEpdnxeddBCkF:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_13_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_yzjbClnhpvenrmE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_yzjbClnhpvenrmE

L$_16_blocks_overflow_yzjbClnhpvenrmE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_yzjbClnhpvenrmE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pdaktzbitiakAwd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pdaktzbitiakAwd
L$_small_initial_partial_block_pdaktzbitiakAwd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pdaktzbitiakAwd:

	orq	%r8,%r8
	je	L$_after_reduction_pdaktzbitiakAwd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pdaktzbitiakAwd:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_14_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_jGAzqyoCyrzhEkm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_jGAzqyoCyrzhEkm

L$_16_blocks_overflow_jGAzqyoCyrzhEkm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_jGAzqyoCyrzhEkm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BpnsdmpbcxnrFAE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BpnsdmpbcxnrFAE
L$_small_initial_partial_block_BpnsdmpbcxnrFAE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BpnsdmpbcxnrFAE:

	orq	%r8,%r8
	je	L$_after_reduction_BpnsdmpbcxnrFAE
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BpnsdmpbcxnrFAE:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_15_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_bfFlugmsBmDcGAy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_bfFlugmsBmDcGAy

L$_16_blocks_overflow_bfFlugmsBmDcGAy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_bfFlugmsBmDcGAy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mocEyCpaAnzxAaA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mocEyCpaAnzxAaA
L$_small_initial_partial_block_mocEyCpaAnzxAaA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mocEyCpaAnzxAaA:

	orq	%r8,%r8
	je	L$_after_reduction_mocEyCpaAnzxAaA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mocEyCpaAnzxAaA:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_16_tazGzuomBntDGcg:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_wCyjvafBEbleFGh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_wCyjvafBEbleFGh

L$_16_blocks_overflow_wCyjvafBEbleFGh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_wCyjvafBEbleFGh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_DrnboDwuFknsznw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DrnboDwuFknsznw:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DrnboDwuFknsznw:
	jmp	L$_last_blocks_done_tazGzuomBntDGcg
L$_last_num_blocks_is_0_tazGzuomBntDGcg:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_tazGzuomBntDGcg:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yksdxkGCqciwCCF
L$_encrypt_16_blocks_yksdxkGCqciwCCF:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_ADoqcGbtzzdzqaf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ADoqcGbtzzdzqaf
L$_16_blocks_overflow_ADoqcGbtzzdzqaf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ADoqcGbtzzdzqaf:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_zCcEimmdsiouycj

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_zCcEimmdsiouycj
	jb	L$_last_num_blocks_is_7_1_zCcEimmdsiouycj


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_zCcEimmdsiouycj
	jb	L$_last_num_blocks_is_11_9_zCcEimmdsiouycj


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_zCcEimmdsiouycj
	ja	L$_last_num_blocks_is_16_zCcEimmdsiouycj
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_zCcEimmdsiouycj
	jmp	L$_last_num_blocks_is_13_zCcEimmdsiouycj

L$_last_num_blocks_is_11_9_zCcEimmdsiouycj:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_zCcEimmdsiouycj
	ja	L$_last_num_blocks_is_11_zCcEimmdsiouycj
	jmp	L$_last_num_blocks_is_9_zCcEimmdsiouycj

L$_last_num_blocks_is_7_1_zCcEimmdsiouycj:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_zCcEimmdsiouycj
	jb	L$_last_num_blocks_is_3_1_zCcEimmdsiouycj

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_zCcEimmdsiouycj
	je	L$_last_num_blocks_is_6_zCcEimmdsiouycj
	jmp	L$_last_num_blocks_is_5_zCcEimmdsiouycj

L$_last_num_blocks_is_3_1_zCcEimmdsiouycj:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_zCcEimmdsiouycj
	je	L$_last_num_blocks_is_2_zCcEimmdsiouycj
L$_last_num_blocks_is_1_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_gzxAphyyqEcDyqj
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_gzxAphyyqEcDyqj

L$_16_blocks_overflow_gzxAphyyqEcDyqj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_gzxAphyyqEcDyqj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rABDkmvjkpmmbDb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rABDkmvjkpmmbDb
L$_small_initial_partial_block_rABDkmvjkpmmbDb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_rABDkmvjkpmmbDb
L$_small_initial_compute_done_rABDkmvjkpmmbDb:
L$_after_reduction_rABDkmvjkpmmbDb:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_2_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_irnuAbrtBoczbFk
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_irnuAbrtBoczbFk

L$_16_blocks_overflow_irnuAbrtBoczbFk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_irnuAbrtBoczbFk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aBabCqwjmzoajEv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aBabCqwjmzoajEv
L$_small_initial_partial_block_aBabCqwjmzoajEv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aBabCqwjmzoajEv:

	orq	%r8,%r8
	je	L$_after_reduction_aBabCqwjmzoajEv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aBabCqwjmzoajEv:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_3_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_lctlatmgaiAcqGe
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_lctlatmgaiAcqGe

L$_16_blocks_overflow_lctlatmgaiAcqGe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_lctlatmgaiAcqGe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uznDxyGynnpqtiD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uznDxyGynnpqtiD
L$_small_initial_partial_block_uznDxyGynnpqtiD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uznDxyGynnpqtiD:

	orq	%r8,%r8
	je	L$_after_reduction_uznDxyGynnpqtiD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uznDxyGynnpqtiD:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_4_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_eckDqfebkxjqsBc
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_eckDqfebkxjqsBc

L$_16_blocks_overflow_eckDqfebkxjqsBc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_eckDqfebkxjqsBc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ulaiDAGkvuhexsr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ulaiDAGkvuhexsr
L$_small_initial_partial_block_ulaiDAGkvuhexsr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ulaiDAGkvuhexsr:

	orq	%r8,%r8
	je	L$_after_reduction_ulaiDAGkvuhexsr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ulaiDAGkvuhexsr:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_5_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_tDhqibspeqcjpyn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_tDhqibspeqcjpyn

L$_16_blocks_overflow_tDhqibspeqcjpyn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_tDhqibspeqcjpyn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_wrsshEderzsdkcG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_wrsshEderzsdkcG
L$_small_initial_partial_block_wrsshEderzsdkcG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_wrsshEderzsdkcG:

	orq	%r8,%r8
	je	L$_after_reduction_wrsshEderzsdkcG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_wrsshEderzsdkcG:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_6_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_tkEfdEcCEfxaCcr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_tkEfdEcCEfxaCcr

L$_16_blocks_overflow_tkEfdEcCEfxaCcr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_tkEfdEcCEfxaCcr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aenwCBhrCGBsguk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aenwCBhrCGBsguk
L$_small_initial_partial_block_aenwCBhrCGBsguk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aenwCBhrCGBsguk:

	orq	%r8,%r8
	je	L$_after_reduction_aenwCBhrCGBsguk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aenwCBhrCGBsguk:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_7_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_uegvijsdwhiuwpn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_uegvijsdwhiuwpn

L$_16_blocks_overflow_uegvijsdwhiuwpn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_uegvijsdwhiuwpn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_axiDGeDucizGjuq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_axiDGeDucizGjuq
L$_small_initial_partial_block_axiDGeDucizGjuq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_axiDGeDucizGjuq:

	orq	%r8,%r8
	je	L$_after_reduction_axiDGeDucizGjuq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_axiDGeDucizGjuq:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_8_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_mBvwkzzwykspeAr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_mBvwkzzwykspeAr

L$_16_blocks_overflow_mBvwkzzwykspeAr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_mBvwkzzwykspeAr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_delmqloxhqfiBvC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_delmqloxhqfiBvC
L$_small_initial_partial_block_delmqloxhqfiBvC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_delmqloxhqfiBvC:

	orq	%r8,%r8
	je	L$_after_reduction_delmqloxhqfiBvC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_delmqloxhqfiBvC:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_9_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_oDiljAkgvDEjFtt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_oDiljAkgvDEjFtt

L$_16_blocks_overflow_oDiljAkgvDEjFtt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_oDiljAkgvDEjFtt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ABGDjiGxeaajCjb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ABGDjiGxeaajCjb
L$_small_initial_partial_block_ABGDjiGxeaajCjb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ABGDjiGxeaajCjb:

	orq	%r8,%r8
	je	L$_after_reduction_ABGDjiGxeaajCjb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ABGDjiGxeaajCjb:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_10_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_xlbzspBqBCuztvC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_xlbzspBqBCuztvC

L$_16_blocks_overflow_xlbzspBqBCuztvC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_xlbzspBqBCuztvC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xdsixBxwfvoxiFm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xdsixBxwfvoxiFm
L$_small_initial_partial_block_xdsixBxwfvoxiFm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xdsixBxwfvoxiFm:

	orq	%r8,%r8
	je	L$_after_reduction_xdsixBxwfvoxiFm
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xdsixBxwfvoxiFm:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_11_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_phzGffipzzkharB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_phzGffipzzkharB

L$_16_blocks_overflow_phzGffipzzkharB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_phzGffipzzkharB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_erBidxadGxtoECz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_erBidxadGxtoECz
L$_small_initial_partial_block_erBidxadGxtoECz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_erBidxadGxtoECz:

	orq	%r8,%r8
	je	L$_after_reduction_erBidxadGxtoECz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_erBidxadGxtoECz:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_12_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_cxtrDDmxfkxcsCq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_cxtrDDmxfkxcsCq

L$_16_blocks_overflow_cxtrDDmxfkxcsCq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_cxtrDDmxfkxcsCq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GeDcDiACvvFexFA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GeDcDiACvvFexFA
L$_small_initial_partial_block_GeDcDiACvvFexFA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GeDcDiACvvFexFA:

	orq	%r8,%r8
	je	L$_after_reduction_GeDcDiACvvFexFA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GeDcDiACvvFexFA:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_13_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_GBrgGjrFgisEADp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_GBrgGjrFgisEADp

L$_16_blocks_overflow_GBrgGjrFgisEADp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_GBrgGjrFgisEADp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AvqlviFlcyhwGgs





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AvqlviFlcyhwGgs
L$_small_initial_partial_block_AvqlviFlcyhwGgs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AvqlviFlcyhwGgs:

	orq	%r8,%r8
	je	L$_after_reduction_AvqlviFlcyhwGgs
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AvqlviFlcyhwGgs:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_14_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_xFmplywwidnouEy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_xFmplywwidnouEy

L$_16_blocks_overflow_xFmplywwidnouEy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_xFmplywwidnouEy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mjpFGdbuBiqceqe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mjpFGdbuBiqceqe
L$_small_initial_partial_block_mjpFGdbuBiqceqe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mjpFGdbuBiqceqe:

	orq	%r8,%r8
	je	L$_after_reduction_mjpFGdbuBiqceqe
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mjpFGdbuBiqceqe:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_15_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_ihDoAqpDhramhxs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ihDoAqpDhramhxs

L$_16_blocks_overflow_ihDoAqpDhramhxs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ihDoAqpDhramhxs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rodFqvFrxyFdaht





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rodFqvFrxyFdaht
L$_small_initial_partial_block_rodFqvFrxyFdaht:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rodFqvFrxyFdaht:

	orq	%r8,%r8
	je	L$_after_reduction_rodFqvFrxyFdaht
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rodFqvFrxyFdaht:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_16_zCcEimmdsiouycj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_yqhqpcwkvxEFhms
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_yqhqpcwkvxEFhms

L$_16_blocks_overflow_yqhqpcwkvxEFhms:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_yqhqpcwkvxEFhms:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_CqigirEemauAlBq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CqigirEemauAlBq:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CqigirEemauAlBq:
	jmp	L$_last_blocks_done_zCcEimmdsiouycj
L$_last_num_blocks_is_0_zCcEimmdsiouycj:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_zCcEimmdsiouycj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yksdxkGCqciwCCF

L$_message_below_32_blocks_yksdxkGCqciwCCF:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_eAcscdeGxppconf
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
L$_skip_hkeys_precomputation_eAcscdeGxppconf:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_lntznrtmkbrDfjd

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_lntznrtmkbrDfjd
	jb	L$_last_num_blocks_is_7_1_lntznrtmkbrDfjd


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_lntznrtmkbrDfjd
	jb	L$_last_num_blocks_is_11_9_lntznrtmkbrDfjd


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_lntznrtmkbrDfjd
	ja	L$_last_num_blocks_is_16_lntznrtmkbrDfjd
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_lntznrtmkbrDfjd
	jmp	L$_last_num_blocks_is_13_lntznrtmkbrDfjd

L$_last_num_blocks_is_11_9_lntznrtmkbrDfjd:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_lntznrtmkbrDfjd
	ja	L$_last_num_blocks_is_11_lntznrtmkbrDfjd
	jmp	L$_last_num_blocks_is_9_lntznrtmkbrDfjd

L$_last_num_blocks_is_7_1_lntznrtmkbrDfjd:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_lntznrtmkbrDfjd
	jb	L$_last_num_blocks_is_3_1_lntznrtmkbrDfjd

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_lntznrtmkbrDfjd
	je	L$_last_num_blocks_is_6_lntznrtmkbrDfjd
	jmp	L$_last_num_blocks_is_5_lntznrtmkbrDfjd

L$_last_num_blocks_is_3_1_lntznrtmkbrDfjd:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_lntznrtmkbrDfjd
	je	L$_last_num_blocks_is_2_lntznrtmkbrDfjd
L$_last_num_blocks_is_1_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_ACadpxybqGyslfc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_ACadpxybqGyslfc

L$_16_blocks_overflow_ACadpxybqGyslfc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_ACadpxybqGyslfc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fhDdukmxzaoEFyo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fhDdukmxzaoEFyo
L$_small_initial_partial_block_fhDdukmxzaoEFyo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_fhDdukmxzaoEFyo
L$_small_initial_compute_done_fhDdukmxzaoEFyo:
L$_after_reduction_fhDdukmxzaoEFyo:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_2_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_CDDEtitDoucFlou
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_CDDEtitDoucFlou

L$_16_blocks_overflow_CDDEtitDoucFlou:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_CDDEtitDoucFlou:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ghoBDAgocswuyAf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ghoBDAgocswuyAf
L$_small_initial_partial_block_ghoBDAgocswuyAf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ghoBDAgocswuyAf:

	orq	%r8,%r8
	je	L$_after_reduction_ghoBDAgocswuyAf
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ghoBDAgocswuyAf:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_3_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_qkzbaitAcokvifm
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_qkzbaitAcokvifm

L$_16_blocks_overflow_qkzbaitAcokvifm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_qkzbaitAcokvifm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pvpxczqBrqCifuf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pvpxczqBrqCifuf
L$_small_initial_partial_block_pvpxczqBrqCifuf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pvpxczqBrqCifuf:

	orq	%r8,%r8
	je	L$_after_reduction_pvpxczqBrqCifuf
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pvpxczqBrqCifuf:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_4_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_dktwGpDwqrAzmjv
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_dktwGpDwqrAzmjv

L$_16_blocks_overflow_dktwGpDwqrAzmjv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_dktwGpDwqrAzmjv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mmfhrfomgefEwns





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mmfhrfomgefEwns
L$_small_initial_partial_block_mmfhrfomgefEwns:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mmfhrfomgefEwns:

	orq	%r8,%r8
	je	L$_after_reduction_mmfhrfomgefEwns
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mmfhrfomgefEwns:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_5_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_tesfyscohuoaEqC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_tesfyscohuoaEqC

L$_16_blocks_overflow_tesfyscohuoaEqC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_tesfyscohuoaEqC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EBcdCvrfkfyBcpA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EBcdCvrfkfyBcpA
L$_small_initial_partial_block_EBcdCvrfkfyBcpA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EBcdCvrfkfyBcpA:

	orq	%r8,%r8
	je	L$_after_reduction_EBcdCvrfkfyBcpA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EBcdCvrfkfyBcpA:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_6_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_bwnBjorizDolyeg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_bwnBjorizDolyeg

L$_16_blocks_overflow_bwnBjorizDolyeg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_bwnBjorizDolyeg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nABwpuvjeyDiows





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nABwpuvjeyDiows
L$_small_initial_partial_block_nABwpuvjeyDiows:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nABwpuvjeyDiows:

	orq	%r8,%r8
	je	L$_after_reduction_nABwpuvjeyDiows
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nABwpuvjeyDiows:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_7_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_wsBttkgxFnmAoAm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_wsBttkgxFnmAoAm

L$_16_blocks_overflow_wsBttkgxFnmAoAm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_wsBttkgxFnmAoAm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DAyiairtxAjprig





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DAyiairtxAjprig
L$_small_initial_partial_block_DAyiairtxAjprig:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DAyiairtxAjprig:

	orq	%r8,%r8
	je	L$_after_reduction_DAyiairtxAjprig
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DAyiairtxAjprig:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_8_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_wgjEfcFipyviAdf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_wgjEfcFipyviAdf

L$_16_blocks_overflow_wgjEfcFipyviAdf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_wgjEfcFipyviAdf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pDctrolbzomDpxC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pDctrolbzomDpxC
L$_small_initial_partial_block_pDctrolbzomDpxC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pDctrolbzomDpxC:

	orq	%r8,%r8
	je	L$_after_reduction_pDctrolbzomDpxC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pDctrolbzomDpxC:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_9_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_EyukdEAsvoaiErg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_EyukdEAsvoaiErg

L$_16_blocks_overflow_EyukdEAsvoaiErg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_EyukdEAsvoaiErg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qhAsfxCumFkhdtF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qhAsfxCumFkhdtF
L$_small_initial_partial_block_qhAsfxCumFkhdtF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qhAsfxCumFkhdtF:

	orq	%r8,%r8
	je	L$_after_reduction_qhAsfxCumFkhdtF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qhAsfxCumFkhdtF:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_10_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_BunFyrcFFCcwojC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_BunFyrcFFCcwojC

L$_16_blocks_overflow_BunFyrcFFCcwojC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_BunFyrcFFCcwojC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AuaEzDjBvGnqnuC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AuaEzDjBvGnqnuC
L$_small_initial_partial_block_AuaEzDjBvGnqnuC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AuaEzDjBvGnqnuC:

	orq	%r8,%r8
	je	L$_after_reduction_AuaEzDjBvGnqnuC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AuaEzDjBvGnqnuC:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_11_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_exmBchmFiGiooFB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_exmBchmFiGiooFB

L$_16_blocks_overflow_exmBchmFiGiooFB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_exmBchmFiGiooFB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uyiefFylevcjzxi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uyiefFylevcjzxi
L$_small_initial_partial_block_uyiefFylevcjzxi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uyiefFylevcjzxi:

	orq	%r8,%r8
	je	L$_after_reduction_uyiefFylevcjzxi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uyiefFylevcjzxi:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_12_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_ApobdsauxqhmGom
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_ApobdsauxqhmGom

L$_16_blocks_overflow_ApobdsauxqhmGom:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_ApobdsauxqhmGom:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pGjEdAibAmjCroD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pGjEdAibAmjCroD
L$_small_initial_partial_block_pGjEdAibAmjCroD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pGjEdAibAmjCroD:

	orq	%r8,%r8
	je	L$_after_reduction_pGjEdAibAmjCroD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pGjEdAibAmjCroD:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_13_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_oteobCkeetntAde
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_oteobCkeetntAde

L$_16_blocks_overflow_oteobCkeetntAde:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_oteobCkeetntAde:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iFfsCDjkckfxazk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iFfsCDjkckfxazk
L$_small_initial_partial_block_iFfsCDjkckfxazk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iFfsCDjkckfxazk:

	orq	%r8,%r8
	je	L$_after_reduction_iFfsCDjkckfxazk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_iFfsCDjkckfxazk:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_14_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_xlngzABFxcDfrep
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_xlngzABFxcDfrep

L$_16_blocks_overflow_xlngzABFxcDfrep:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_xlngzABFxcDfrep:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fcyugsBwzntFFGa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fcyugsBwzntFFGa
L$_small_initial_partial_block_fcyugsBwzntFFGa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fcyugsBwzntFFGa:

	orq	%r8,%r8
	je	L$_after_reduction_fcyugsBwzntFFGa
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fcyugsBwzntFFGa:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_15_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_EBawhmlkGxrixyf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_EBawhmlkGxrixyf

L$_16_blocks_overflow_EBawhmlkGxrixyf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_EBawhmlkGxrixyf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_btpthFwEAupCgbo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_btpthFwEAupCgbo
L$_small_initial_partial_block_btpthFwEAupCgbo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_btpthFwEAupCgbo:

	orq	%r8,%r8
	je	L$_after_reduction_btpthFwEAupCgbo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_btpthFwEAupCgbo:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_16_lntznrtmkbrDfjd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_vlyqFjspltBztDb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_vlyqFjspltBztDb

L$_16_blocks_overflow_vlyqFjspltBztDb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_vlyqFjspltBztDb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_noykbADGyhAnkrc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_noykbADGyhAnkrc:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_noykbADGyhAnkrc:
	jmp	L$_last_blocks_done_lntznrtmkbrDfjd
L$_last_num_blocks_is_0_lntznrtmkbrDfjd:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_lntznrtmkbrDfjd:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_yksdxkGCqciwCCF

L$_message_below_equal_16_blocks_yksdxkGCqciwCCF:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	L$_small_initial_num_blocks_is_8_embccezClDsCcjl
	jl	L$_small_initial_num_blocks_is_7_1_embccezClDsCcjl


	cmpq	$12,%r12
	je	L$_small_initial_num_blocks_is_12_embccezClDsCcjl
	jl	L$_small_initial_num_blocks_is_11_9_embccezClDsCcjl


	cmpq	$16,%r12
	je	L$_small_initial_num_blocks_is_16_embccezClDsCcjl
	cmpq	$15,%r12
	je	L$_small_initial_num_blocks_is_15_embccezClDsCcjl
	cmpq	$14,%r12
	je	L$_small_initial_num_blocks_is_14_embccezClDsCcjl
	jmp	L$_small_initial_num_blocks_is_13_embccezClDsCcjl

L$_small_initial_num_blocks_is_11_9_embccezClDsCcjl:

	cmpq	$11,%r12
	je	L$_small_initial_num_blocks_is_11_embccezClDsCcjl
	cmpq	$10,%r12
	je	L$_small_initial_num_blocks_is_10_embccezClDsCcjl
	jmp	L$_small_initial_num_blocks_is_9_embccezClDsCcjl

L$_small_initial_num_blocks_is_7_1_embccezClDsCcjl:
	cmpq	$4,%r12
	je	L$_small_initial_num_blocks_is_4_embccezClDsCcjl
	jl	L$_small_initial_num_blocks_is_3_1_embccezClDsCcjl

	cmpq	$7,%r12
	je	L$_small_initial_num_blocks_is_7_embccezClDsCcjl
	cmpq	$6,%r12
	je	L$_small_initial_num_blocks_is_6_embccezClDsCcjl
	jmp	L$_small_initial_num_blocks_is_5_embccezClDsCcjl

L$_small_initial_num_blocks_is_3_1_embccezClDsCcjl:

	cmpq	$3,%r12
	je	L$_small_initial_num_blocks_is_3_embccezClDsCcjl
	cmpq	$2,%r12
	je	L$_small_initial_num_blocks_is_2_embccezClDsCcjl





L$_small_initial_num_blocks_is_1_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xAzjlbxpfmlrrrg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xAzjlbxpfmlrrrg
L$_small_initial_partial_block_xAzjlbxpfmlrrrg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	L$_after_reduction_xAzjlbxpfmlrrrg
L$_small_initial_compute_done_xAzjlbxpfmlrrrg:
L$_after_reduction_xAzjlbxpfmlrrrg:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_2_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vFCxaypeFkxviqu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vFCxaypeFkxviqu
L$_small_initial_partial_block_vFCxaypeFkxviqu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vFCxaypeFkxviqu:

	orq	%r8,%r8
	je	L$_after_reduction_vFCxaypeFkxviqu
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_vFCxaypeFkxviqu:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_3_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dwbCqnFzDalqCui





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dwbCqnFzDalqCui
L$_small_initial_partial_block_dwbCqnFzDalqCui:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dwbCqnFzDalqCui:

	orq	%r8,%r8
	je	L$_after_reduction_dwbCqnFzDalqCui
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_dwbCqnFzDalqCui:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_4_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AbjGgtqGFcaEjlt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AbjGgtqGFcaEjlt
L$_small_initial_partial_block_AbjGgtqGFcaEjlt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AbjGgtqGFcaEjlt:

	orq	%r8,%r8
	je	L$_after_reduction_AbjGgtqGFcaEjlt
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_AbjGgtqGFcaEjlt:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_5_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fbndaAefnGbagFh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fbndaAefnGbagFh
L$_small_initial_partial_block_fbndaAefnGbagFh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fbndaAefnGbagFh:

	orq	%r8,%r8
	je	L$_after_reduction_fbndaAefnGbagFh
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_fbndaAefnGbagFh:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_6_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_akqwjDBxucDexDh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_akqwjDBxucDexDh
L$_small_initial_partial_block_akqwjDBxucDexDh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_akqwjDBxucDexDh:

	orq	%r8,%r8
	je	L$_after_reduction_akqwjDBxucDexDh
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_akqwjDBxucDexDh:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_7_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lpxqonsnztgkjtb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lpxqonsnztgkjtb
L$_small_initial_partial_block_lpxqonsnztgkjtb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lpxqonsnztgkjtb:

	orq	%r8,%r8
	je	L$_after_reduction_lpxqonsnztgkjtb
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_lpxqonsnztgkjtb:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_8_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_eCqyuFFuhDbCACu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_eCqyuFFuhDbCACu
L$_small_initial_partial_block_eCqyuFFuhDbCACu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_eCqyuFFuhDbCACu:

	orq	%r8,%r8
	je	L$_after_reduction_eCqyuFFuhDbCACu
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_eCqyuFFuhDbCACu:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_9_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_plyzBktACFaoaAo





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_plyzBktACFaoaAo
L$_small_initial_partial_block_plyzBktACFaoaAo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_plyzBktACFaoaAo:

	orq	%r8,%r8
	je	L$_after_reduction_plyzBktACFaoaAo
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_plyzBktACFaoaAo:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_10_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EsietxuuqEzcnqe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EsietxuuqEzcnqe
L$_small_initial_partial_block_EsietxuuqEzcnqe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EsietxuuqEzcnqe:

	orq	%r8,%r8
	je	L$_after_reduction_EsietxuuqEzcnqe
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EsietxuuqEzcnqe:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_11_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EDijmgibtriCzig





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EDijmgibtriCzig
L$_small_initial_partial_block_EDijmgibtriCzig:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EDijmgibtriCzig:

	orq	%r8,%r8
	je	L$_after_reduction_EDijmgibtriCzig
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EDijmgibtriCzig:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_12_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_apBduukegiiqonq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_apBduukegiiqonq
L$_small_initial_partial_block_apBduukegiiqonq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_apBduukegiiqonq:

	orq	%r8,%r8
	je	L$_after_reduction_apBduukegiiqonq
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_apBduukegiiqonq:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_13_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nzGwbxvdFgeFyef





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nzGwbxvdFgeFyef
L$_small_initial_partial_block_nzGwbxvdFgeFyef:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nzGwbxvdFgeFyef:

	orq	%r8,%r8
	je	L$_after_reduction_nzGwbxvdFgeFyef
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_nzGwbxvdFgeFyef:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_14_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jGvhsCEbDDyhrjF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jGvhsCEbDDyhrjF
L$_small_initial_partial_block_jGvhsCEbDDyhrjF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jGvhsCEbDDyhrjF:

	orq	%r8,%r8
	je	L$_after_reduction_jGvhsCEbDDyhrjF
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_jGvhsCEbDDyhrjF:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_15_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EbgDzsFiklwpqfF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EbgDzsFiklwpqfF
L$_small_initial_partial_block_EbgDzsFiklwpqfF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EbgDzsFiklwpqfF:

	orq	%r8,%r8
	je	L$_after_reduction_EbgDzsFiklwpqfF
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EbgDzsFiklwpqfF:
	jmp	L$_small_initial_blocks_encrypted_embccezClDsCcjl
L$_small_initial_num_blocks_is_16_embccezClDsCcjl:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_ouCwcrqpbyGsAwe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ouCwcrqpbyGsAwe:
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ouCwcrqpbyGsAwe:
L$_small_initial_blocks_encrypted_embccezClDsCcjl:
L$_ghash_done_yksdxkGCqciwCCF:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
L$_enc_dec_done_yksdxkGCqciwCCF:
	jmp	L$exit_gcm_encrypt
L$exit_gcm_encrypt:
	cmpq	$256,%r8
	jbe	L$skip_hkeys_cleanup_DafsDjeqdGgzsrs
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
L$skip_hkeys_cleanup_DafsDjeqdGgzsrs:
	vzeroupper
	leaq	(%rbp),%rsp

	popq	%r15

	popq	%r14

	popq	%r13

	popq	%r12

	popq	%rbp

	popq	%rbx

	.byte	0xf3,0xc3
L$encrypt_seh_end:


.globl	_ossl_aes_gcm_decrypt_avx512

.p2align	5
_ossl_aes_gcm_decrypt_avx512:

L$decrypt_seh_begin:
.byte	243,15,30,250
	pushq	%rbx

L$decrypt_seh_push_rbx:
	pushq	%rbp

L$decrypt_seh_push_rbp:
	pushq	%r12

L$decrypt_seh_push_r12:
	pushq	%r13

L$decrypt_seh_push_r13:
	pushq	%r14

L$decrypt_seh_push_r14:
	pushq	%r15

L$decrypt_seh_push_r15:










	leaq	0(%rsp),%rbp

L$decrypt_seh_setfp:

L$decrypt_seh_prolog_end:
	subq	$1588,%rsp
	andq	$(-64),%rsp


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	L$aes_gcm_decrypt_128_avx512
	cmpl	$11,%eax
	je	L$aes_gcm_decrypt_192_avx512
	cmpl	$13,%eax
	je	L$aes_gcm_decrypt_256_avx512
	xorl	%eax,%eax
	jmp	L$exit_gcm_decrypt
.p2align	5
L$aes_gcm_decrypt_128_avx512:
	orq	%r8,%r8
	je	L$_enc_dec_done_DuuzltgzkmbbyjG
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	L$_partial_block_done_kDtiqxFcmhreEFb
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	L$_no_extra_mask_kDtiqxFcmhreEFb
	subq	%r13,%r12
L$_no_extra_mask_kDtiqxFcmhreEFb:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	L$_partial_incomplete_kDtiqxFcmhreEFb

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	L$_enc_dec_done_kDtiqxFcmhreEFb

L$_partial_incomplete_kDtiqxFcmhreEFb:
	addq	%r8,(%rdx)
	movq	%r8,%r11

L$_enc_dec_done_kDtiqxFcmhreEFb:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
L$_partial_block_done_kDtiqxFcmhreEFb:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	L$_enc_dec_done_DuuzltgzkmbbyjG
	cmpq	$256,%r8
	jbe	L$_message_below_equal_16_blocks_DuuzltgzkmbbyjG

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_mchxjspxpxwEibg
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_mchxjspxpxwEibg
L$_next_16_overflow_mchxjspxpxwEibg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_mchxjspxpxwEibg:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_vjprkimuwFoysyl

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
L$_skip_hkeys_precomputation_vjprkimuwFoysyl:
	cmpq	$512,%r8
	jb	L$_message_below_32_blocks_DuuzltgzkmbbyjG



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_vFkcjCnsnnhuuAF
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_vFkcjCnsnnhuuAF
L$_next_16_overflow_vFkcjCnsnnhuuAF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_vFkcjCnsnnhuuAF:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_nxmsArdhibjxuks
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
L$_skip_hkeys_precomputation_nxmsArdhibjxuks:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	L$_no_more_big_nblocks_DuuzltgzkmbbyjG
L$_encrypt_big_nblocks_DuuzltgzkmbbyjG:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_kieFrtGtdAgAEdq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_kieFrtGtdAgAEdq
L$_16_blocks_overflow_kieFrtGtdAgAEdq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_kieFrtGtdAgAEdq:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_qznwcjluwCjGxpD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_qznwcjluwCjGxpD
L$_16_blocks_overflow_qznwcjluwCjGxpD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_qznwcjluwCjGxpD:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_BgprqyGtvvtnEbD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_BgprqyGtvvtnEbD
L$_16_blocks_overflow_BgprqyGtvvtnEbD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_BgprqyGtvvtnEbD:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	L$_encrypt_big_nblocks_DuuzltgzkmbbyjG

L$_no_more_big_nblocks_DuuzltgzkmbbyjG:

	cmpq	$512,%r8
	jae	L$_encrypt_32_blocks_DuuzltgzkmbbyjG

	cmpq	$256,%r8
	jae	L$_encrypt_16_blocks_DuuzltgzkmbbyjG
L$_encrypt_0_blocks_ghash_32_DuuzltgzkmbbyjG:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_xtdcakDEjFmxctv

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_xtdcakDEjFmxctv
	jb	L$_last_num_blocks_is_7_1_xtdcakDEjFmxctv


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_xtdcakDEjFmxctv
	jb	L$_last_num_blocks_is_11_9_xtdcakDEjFmxctv


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_xtdcakDEjFmxctv
	ja	L$_last_num_blocks_is_16_xtdcakDEjFmxctv
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_xtdcakDEjFmxctv
	jmp	L$_last_num_blocks_is_13_xtdcakDEjFmxctv

L$_last_num_blocks_is_11_9_xtdcakDEjFmxctv:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_xtdcakDEjFmxctv
	ja	L$_last_num_blocks_is_11_xtdcakDEjFmxctv
	jmp	L$_last_num_blocks_is_9_xtdcakDEjFmxctv

L$_last_num_blocks_is_7_1_xtdcakDEjFmxctv:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_xtdcakDEjFmxctv
	jb	L$_last_num_blocks_is_3_1_xtdcakDEjFmxctv

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_xtdcakDEjFmxctv
	je	L$_last_num_blocks_is_6_xtdcakDEjFmxctv
	jmp	L$_last_num_blocks_is_5_xtdcakDEjFmxctv

L$_last_num_blocks_is_3_1_xtdcakDEjFmxctv:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_xtdcakDEjFmxctv
	je	L$_last_num_blocks_is_2_xtdcakDEjFmxctv
L$_last_num_blocks_is_1_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_bApAwzGdieyojre
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_bApAwzGdieyojre

L$_16_blocks_overflow_bApAwzGdieyojre:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_bApAwzGdieyojre:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qbriEBqzegCekxB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qbriEBqzegCekxB
L$_small_initial_partial_block_qbriEBqzegCekxB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_qbriEBqzegCekxB
L$_small_initial_compute_done_qbriEBqzegCekxB:
L$_after_reduction_qbriEBqzegCekxB:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_2_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_hsFdusyjFuqzplG
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_hsFdusyjFuqzplG

L$_16_blocks_overflow_hsFdusyjFuqzplG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_hsFdusyjFuqzplG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EfrEztvlzaaddAE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EfrEztvlzaaddAE
L$_small_initial_partial_block_EfrEztvlzaaddAE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EfrEztvlzaaddAE:

	orq	%r8,%r8
	je	L$_after_reduction_EfrEztvlzaaddAE
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EfrEztvlzaaddAE:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_3_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_qyeGEBpwzuaEsif
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_qyeGEBpwzuaEsif

L$_16_blocks_overflow_qyeGEBpwzuaEsif:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_qyeGEBpwzuaEsif:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ddhsmByCkzhoEAC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ddhsmByCkzhoEAC
L$_small_initial_partial_block_ddhsmByCkzhoEAC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ddhsmByCkzhoEAC:

	orq	%r8,%r8
	je	L$_after_reduction_ddhsmByCkzhoEAC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ddhsmByCkzhoEAC:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_4_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_cqtDDpohmfsnGDF
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_cqtDDpohmfsnGDF

L$_16_blocks_overflow_cqtDDpohmfsnGDF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_cqtDDpohmfsnGDF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mjjgonwhloxnvch





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mjjgonwhloxnvch
L$_small_initial_partial_block_mjjgonwhloxnvch:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mjjgonwhloxnvch:

	orq	%r8,%r8
	je	L$_after_reduction_mjjgonwhloxnvch
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mjjgonwhloxnvch:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_5_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_kntmnCvnnEjGpsw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_kntmnCvnnEjGpsw

L$_16_blocks_overflow_kntmnCvnnEjGpsw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_kntmnCvnnEjGpsw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ocbnvplaFlylouf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ocbnvplaFlylouf
L$_small_initial_partial_block_ocbnvplaFlylouf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ocbnvplaFlylouf:

	orq	%r8,%r8
	je	L$_after_reduction_ocbnvplaFlylouf
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ocbnvplaFlylouf:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_6_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_koampcltlAsfDtG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_koampcltlAsfDtG

L$_16_blocks_overflow_koampcltlAsfDtG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_koampcltlAsfDtG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gxEmCixufizAskF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gxEmCixufizAskF
L$_small_initial_partial_block_gxEmCixufizAskF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gxEmCixufizAskF:

	orq	%r8,%r8
	je	L$_after_reduction_gxEmCixufizAskF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gxEmCixufizAskF:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_7_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_twzEBryBosilzjo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_twzEBryBosilzjo

L$_16_blocks_overflow_twzEBryBosilzjo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_twzEBryBosilzjo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CtufpzoyeqGgawn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CtufpzoyeqGgawn
L$_small_initial_partial_block_CtufpzoyeqGgawn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CtufpzoyeqGgawn:

	orq	%r8,%r8
	je	L$_after_reduction_CtufpzoyeqGgawn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CtufpzoyeqGgawn:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_8_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_yxCfdbwuxoicGrv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_yxCfdbwuxoicGrv

L$_16_blocks_overflow_yxCfdbwuxoicGrv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_yxCfdbwuxoicGrv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cClfGgCAmAgduts





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cClfGgCAmAgduts
L$_small_initial_partial_block_cClfGgCAmAgduts:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cClfGgCAmAgduts:

	orq	%r8,%r8
	je	L$_after_reduction_cClfGgCAmAgduts
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_cClfGgCAmAgduts:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_9_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_utyqtldvcEDkkdq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_utyqtldvcEDkkdq

L$_16_blocks_overflow_utyqtldvcEDkkdq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_utyqtldvcEDkkdq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_wqnkbbkqdgppmAc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_wqnkbbkqdgppmAc
L$_small_initial_partial_block_wqnkbbkqdgppmAc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_wqnkbbkqdgppmAc:

	orq	%r8,%r8
	je	L$_after_reduction_wqnkbbkqdgppmAc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_wqnkbbkqdgppmAc:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_10_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_tjeqjFtoejdjzxx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_tjeqjFtoejdjzxx

L$_16_blocks_overflow_tjeqjFtoejdjzxx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_tjeqjFtoejdjzxx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hvlFayqlFyBnysE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hvlFayqlFyBnysE
L$_small_initial_partial_block_hvlFayqlFyBnysE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hvlFayqlFyBnysE:

	orq	%r8,%r8
	je	L$_after_reduction_hvlFayqlFyBnysE
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hvlFayqlFyBnysE:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_11_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_nnjofbwaGekpDyj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_nnjofbwaGekpDyj

L$_16_blocks_overflow_nnjofbwaGekpDyj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_nnjofbwaGekpDyj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ticCmyghiciCjGi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ticCmyghiciCjGi
L$_small_initial_partial_block_ticCmyghiciCjGi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ticCmyghiciCjGi:

	orq	%r8,%r8
	je	L$_after_reduction_ticCmyghiciCjGi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ticCmyghiciCjGi:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_12_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_ChrsthwywGCylCf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_ChrsthwywGCylCf

L$_16_blocks_overflow_ChrsthwywGCylCf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_ChrsthwywGCylCf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hFfGBhufDoCldie





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hFfGBhufDoCldie
L$_small_initial_partial_block_hFfGBhufDoCldie:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hFfGBhufDoCldie:

	orq	%r8,%r8
	je	L$_after_reduction_hFfGBhufDoCldie
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hFfGBhufDoCldie:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_13_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_kAtsgcwEzBgeoob
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_kAtsgcwEzBgeoob

L$_16_blocks_overflow_kAtsgcwEzBgeoob:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_kAtsgcwEzBgeoob:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qjkxkDkveDvmGxq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qjkxkDkveDvmGxq
L$_small_initial_partial_block_qjkxkDkveDvmGxq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qjkxkDkveDvmGxq:

	orq	%r8,%r8
	je	L$_after_reduction_qjkxkDkveDvmGxq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qjkxkDkveDvmGxq:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_14_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_bzjDuvieabtfxDg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_bzjDuvieabtfxDg

L$_16_blocks_overflow_bzjDuvieabtfxDg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_bzjDuvieabtfxDg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CnAnaxClbuEeAvl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CnAnaxClbuEeAvl
L$_small_initial_partial_block_CnAnaxClbuEeAvl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CnAnaxClbuEeAvl:

	orq	%r8,%r8
	je	L$_after_reduction_CnAnaxClbuEeAvl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CnAnaxClbuEeAvl:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_15_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_gvFydfCikmCixcb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_gvFydfCikmCixcb

L$_16_blocks_overflow_gvFydfCikmCixcb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_gvFydfCikmCixcb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_asuqzgmnjtFDorx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_asuqzgmnjtFDorx
L$_small_initial_partial_block_asuqzgmnjtFDorx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_asuqzgmnjtFDorx:

	orq	%r8,%r8
	je	L$_after_reduction_asuqzgmnjtFDorx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_asuqzgmnjtFDorx:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_16_xtdcakDEjFmxctv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_bpuwokFwnxmEodq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_bpuwokFwnxmEodq

L$_16_blocks_overflow_bpuwokFwnxmEodq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_bpuwokFwnxmEodq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_CeGqkFFxiCwuGBq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CeGqkFFxiCwuGBq:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CeGqkFFxiCwuGBq:
	jmp	L$_last_blocks_done_xtdcakDEjFmxctv
L$_last_num_blocks_is_0_xtdcakDEjFmxctv:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_xtdcakDEjFmxctv:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_DuuzltgzkmbbyjG
L$_encrypt_32_blocks_DuuzltgzkmbbyjG:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_ajhqFplicFDzdsp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_ajhqFplicFDzdsp
L$_16_blocks_overflow_ajhqFplicFDzdsp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_ajhqFplicFDzdsp:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_sGqzmAmGouCkaBq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_sGqzmAmGouCkaBq
L$_16_blocks_overflow_sGqzmAmGouCkaBq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_sGqzmAmGouCkaBq:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_wBtkiAgdEGruuew

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_wBtkiAgdEGruuew
	jb	L$_last_num_blocks_is_7_1_wBtkiAgdEGruuew


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_wBtkiAgdEGruuew
	jb	L$_last_num_blocks_is_11_9_wBtkiAgdEGruuew


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_wBtkiAgdEGruuew
	ja	L$_last_num_blocks_is_16_wBtkiAgdEGruuew
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_wBtkiAgdEGruuew
	jmp	L$_last_num_blocks_is_13_wBtkiAgdEGruuew

L$_last_num_blocks_is_11_9_wBtkiAgdEGruuew:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_wBtkiAgdEGruuew
	ja	L$_last_num_blocks_is_11_wBtkiAgdEGruuew
	jmp	L$_last_num_blocks_is_9_wBtkiAgdEGruuew

L$_last_num_blocks_is_7_1_wBtkiAgdEGruuew:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_wBtkiAgdEGruuew
	jb	L$_last_num_blocks_is_3_1_wBtkiAgdEGruuew

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_wBtkiAgdEGruuew
	je	L$_last_num_blocks_is_6_wBtkiAgdEGruuew
	jmp	L$_last_num_blocks_is_5_wBtkiAgdEGruuew

L$_last_num_blocks_is_3_1_wBtkiAgdEGruuew:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_wBtkiAgdEGruuew
	je	L$_last_num_blocks_is_2_wBtkiAgdEGruuew
L$_last_num_blocks_is_1_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_uipqAzvdmlzxvDp
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_uipqAzvdmlzxvDp

L$_16_blocks_overflow_uipqAzvdmlzxvDp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_uipqAzvdmlzxvDp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qqpjigBnaClzmFA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qqpjigBnaClzmFA
L$_small_initial_partial_block_qqpjigBnaClzmFA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_qqpjigBnaClzmFA
L$_small_initial_compute_done_qqpjigBnaClzmFA:
L$_after_reduction_qqpjigBnaClzmFA:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_2_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_ggkuzCCDvxkwGDy
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_ggkuzCCDvxkwGDy

L$_16_blocks_overflow_ggkuzCCDvxkwGDy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_ggkuzCCDvxkwGDy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_updqzptwsrugmbv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_updqzptwsrugmbv
L$_small_initial_partial_block_updqzptwsrugmbv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_updqzptwsrugmbv:

	orq	%r8,%r8
	je	L$_after_reduction_updqzptwsrugmbv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_updqzptwsrugmbv:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_3_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_eswbzxvuaFCdisi
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_eswbzxvuaFCdisi

L$_16_blocks_overflow_eswbzxvuaFCdisi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_eswbzxvuaFCdisi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gzwowpjcntDxmwA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gzwowpjcntDxmwA
L$_small_initial_partial_block_gzwowpjcntDxmwA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gzwowpjcntDxmwA:

	orq	%r8,%r8
	je	L$_after_reduction_gzwowpjcntDxmwA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gzwowpjcntDxmwA:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_4_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_ivblpkpfjnBkglC
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_ivblpkpfjnBkglC

L$_16_blocks_overflow_ivblpkpfjnBkglC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_ivblpkpfjnBkglC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rFojviAhhAoGciu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rFojviAhhAoGciu
L$_small_initial_partial_block_rFojviAhhAoGciu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rFojviAhhAoGciu:

	orq	%r8,%r8
	je	L$_after_reduction_rFojviAhhAoGciu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rFojviAhhAoGciu:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_5_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_bGvEewisleAtFGn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_bGvEewisleAtFGn

L$_16_blocks_overflow_bGvEewisleAtFGn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_bGvEewisleAtFGn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_argfDwlBAbhyxGz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_argfDwlBAbhyxGz
L$_small_initial_partial_block_argfDwlBAbhyxGz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_argfDwlBAbhyxGz:

	orq	%r8,%r8
	je	L$_after_reduction_argfDwlBAbhyxGz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_argfDwlBAbhyxGz:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_6_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_alucgzwtprlhtfB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_alucgzwtprlhtfB

L$_16_blocks_overflow_alucgzwtprlhtfB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_alucgzwtprlhtfB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ihbiwxqouCqwsug





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ihbiwxqouCqwsug
L$_small_initial_partial_block_ihbiwxqouCqwsug:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ihbiwxqouCqwsug:

	orq	%r8,%r8
	je	L$_after_reduction_ihbiwxqouCqwsug
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ihbiwxqouCqwsug:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_7_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_fywdpjdqeiAEtxa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_fywdpjdqeiAEtxa

L$_16_blocks_overflow_fywdpjdqeiAEtxa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_fywdpjdqeiAEtxa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vgcbpGGslcnGGnp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vgcbpGGslcnGGnp
L$_small_initial_partial_block_vgcbpGGslcnGGnp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vgcbpGGslcnGGnp:

	orq	%r8,%r8
	je	L$_after_reduction_vgcbpGGslcnGGnp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vgcbpGGslcnGGnp:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_8_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_ceyfnpuceDBojvd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_ceyfnpuceDBojvd

L$_16_blocks_overflow_ceyfnpuceDBojvd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_ceyfnpuceDBojvd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ximqGpmaAwjoegm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ximqGpmaAwjoegm
L$_small_initial_partial_block_ximqGpmaAwjoegm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ximqGpmaAwjoegm:

	orq	%r8,%r8
	je	L$_after_reduction_ximqGpmaAwjoegm
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ximqGpmaAwjoegm:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_9_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_CFklGEBgbfmtCeF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_CFklGEBgbfmtCeF

L$_16_blocks_overflow_CFklGEBgbfmtCeF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_CFklGEBgbfmtCeF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zbhBBhqdnqiqhse





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zbhBBhqdnqiqhse
L$_small_initial_partial_block_zbhBBhqdnqiqhse:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zbhBBhqdnqiqhse:

	orq	%r8,%r8
	je	L$_after_reduction_zbhBBhqdnqiqhse
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zbhBBhqdnqiqhse:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_10_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_misAruzAosApGzf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_misAruzAosApGzf

L$_16_blocks_overflow_misAruzAosApGzf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_misAruzAosApGzf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xyGrpcjckpinEsl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xyGrpcjckpinEsl
L$_small_initial_partial_block_xyGrpcjckpinEsl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xyGrpcjckpinEsl:

	orq	%r8,%r8
	je	L$_after_reduction_xyGrpcjckpinEsl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xyGrpcjckpinEsl:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_11_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_EtndjBpFkhorwcr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_EtndjBpFkhorwcr

L$_16_blocks_overflow_EtndjBpFkhorwcr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_EtndjBpFkhorwcr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vFAkzEefpzDgpka





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vFAkzEefpzDgpka
L$_small_initial_partial_block_vFAkzEefpzDgpka:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vFAkzEefpzDgpka:

	orq	%r8,%r8
	je	L$_after_reduction_vFAkzEefpzDgpka
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vFAkzEefpzDgpka:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_12_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_FBjketnqlvvkGhn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_FBjketnqlvvkGhn

L$_16_blocks_overflow_FBjketnqlvvkGhn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_FBjketnqlvvkGhn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EdijhahBqqwhCGs





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EdijhahBqqwhCGs
L$_small_initial_partial_block_EdijhahBqqwhCGs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EdijhahBqqwhCGs:

	orq	%r8,%r8
	je	L$_after_reduction_EdijhahBqqwhCGs
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EdijhahBqqwhCGs:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_13_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_qqFbEbawxvprBmx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_qqFbEbawxvprBmx

L$_16_blocks_overflow_qqFbEbawxvprBmx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_qqFbEbawxvprBmx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CbwFdDBxosGiyGB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CbwFdDBxosGiyGB
L$_small_initial_partial_block_CbwFdDBxosGiyGB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CbwFdDBxosGiyGB:

	orq	%r8,%r8
	je	L$_after_reduction_CbwFdDBxosGiyGB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CbwFdDBxosGiyGB:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_14_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_jnnCgxrDwCjEeFt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_jnnCgxrDwCjEeFt

L$_16_blocks_overflow_jnnCgxrDwCjEeFt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_jnnCgxrDwCjEeFt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rApdcbdmlzrGjou





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rApdcbdmlzrGjou
L$_small_initial_partial_block_rApdcbdmlzrGjou:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rApdcbdmlzrGjou:

	orq	%r8,%r8
	je	L$_after_reduction_rApdcbdmlzrGjou
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rApdcbdmlzrGjou:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_15_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_bEryGpkvmbdAeCu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_bEryGpkvmbdAeCu

L$_16_blocks_overflow_bEryGpkvmbdAeCu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_bEryGpkvmbdAeCu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ByAkxwwhjljDqmF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ByAkxwwhjljDqmF
L$_small_initial_partial_block_ByAkxwwhjljDqmF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ByAkxwwhjljDqmF:

	orq	%r8,%r8
	je	L$_after_reduction_ByAkxwwhjljDqmF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ByAkxwwhjljDqmF:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_16_wBtkiAgdEGruuew:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_dxhAGbmsexboxmq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_dxhAGbmsexboxmq

L$_16_blocks_overflow_dxhAGbmsexboxmq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_dxhAGbmsexboxmq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_Fhsdxcssheaewif:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_Fhsdxcssheaewif:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_Fhsdxcssheaewif:
	jmp	L$_last_blocks_done_wBtkiAgdEGruuew
L$_last_num_blocks_is_0_wBtkiAgdEGruuew:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_wBtkiAgdEGruuew:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_DuuzltgzkmbbyjG
L$_encrypt_16_blocks_DuuzltgzkmbbyjG:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_evdynnmzyGafAda
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_evdynnmzyGafAda
L$_16_blocks_overflow_evdynnmzyGafAda:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_evdynnmzyGafAda:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_hdjmDGFumdEyvrz

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_hdjmDGFumdEyvrz
	jb	L$_last_num_blocks_is_7_1_hdjmDGFumdEyvrz


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_hdjmDGFumdEyvrz
	jb	L$_last_num_blocks_is_11_9_hdjmDGFumdEyvrz


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_hdjmDGFumdEyvrz
	ja	L$_last_num_blocks_is_16_hdjmDGFumdEyvrz
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_hdjmDGFumdEyvrz
	jmp	L$_last_num_blocks_is_13_hdjmDGFumdEyvrz

L$_last_num_blocks_is_11_9_hdjmDGFumdEyvrz:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_hdjmDGFumdEyvrz
	ja	L$_last_num_blocks_is_11_hdjmDGFumdEyvrz
	jmp	L$_last_num_blocks_is_9_hdjmDGFumdEyvrz

L$_last_num_blocks_is_7_1_hdjmDGFumdEyvrz:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_hdjmDGFumdEyvrz
	jb	L$_last_num_blocks_is_3_1_hdjmDGFumdEyvrz

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_hdjmDGFumdEyvrz
	je	L$_last_num_blocks_is_6_hdjmDGFumdEyvrz
	jmp	L$_last_num_blocks_is_5_hdjmDGFumdEyvrz

L$_last_num_blocks_is_3_1_hdjmDGFumdEyvrz:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_hdjmDGFumdEyvrz
	je	L$_last_num_blocks_is_2_hdjmDGFumdEyvrz
L$_last_num_blocks_is_1_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_lrCzGsEhbnkqdhe
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_lrCzGsEhbnkqdhe

L$_16_blocks_overflow_lrCzGsEhbnkqdhe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_lrCzGsEhbnkqdhe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iknggBEspdjhxxE





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iknggBEspdjhxxE
L$_small_initial_partial_block_iknggBEspdjhxxE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_iknggBEspdjhxxE
L$_small_initial_compute_done_iknggBEspdjhxxE:
L$_after_reduction_iknggBEspdjhxxE:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_2_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_CFbGgxllmeyonFt
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_CFbGgxllmeyonFt

L$_16_blocks_overflow_CFbGgxllmeyonFt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_CFbGgxllmeyonFt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GfcFwgsDuofGngq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GfcFwgsDuofGngq
L$_small_initial_partial_block_GfcFwgsDuofGngq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GfcFwgsDuofGngq:

	orq	%r8,%r8
	je	L$_after_reduction_GfcFwgsDuofGngq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GfcFwgsDuofGngq:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_3_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_jGeiryEvlDxCeEa
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_jGeiryEvlDxCeEa

L$_16_blocks_overflow_jGeiryEvlDxCeEa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_jGeiryEvlDxCeEa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xCuhgpbqAFxiotg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xCuhgpbqAFxiotg
L$_small_initial_partial_block_xCuhgpbqAFxiotg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xCuhgpbqAFxiotg:

	orq	%r8,%r8
	je	L$_after_reduction_xCuhgpbqAFxiotg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xCuhgpbqAFxiotg:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_4_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_kfityjppEGEdFeb
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_kfityjppEGEdFeb

L$_16_blocks_overflow_kfityjppEGEdFeb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_kfityjppEGEdFeb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zcgsefFkwxFyhsx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zcgsefFkwxFyhsx
L$_small_initial_partial_block_zcgsefFkwxFyhsx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zcgsefFkwxFyhsx:

	orq	%r8,%r8
	je	L$_after_reduction_zcgsefFkwxFyhsx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zcgsefFkwxFyhsx:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_5_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_xBAtbmbvbEnCztB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_xBAtbmbvbEnCztB

L$_16_blocks_overflow_xBAtbmbvbEnCztB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_xBAtbmbvbEnCztB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lEcpBzGCoxkFgah





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lEcpBzGCoxkFgah
L$_small_initial_partial_block_lEcpBzGCoxkFgah:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lEcpBzGCoxkFgah:

	orq	%r8,%r8
	je	L$_after_reduction_lEcpBzGCoxkFgah
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_lEcpBzGCoxkFgah:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_6_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_stDmendisxECGGk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_stDmendisxECGGk

L$_16_blocks_overflow_stDmendisxECGGk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_stDmendisxECGGk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AjwwyuvbpsaADzp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AjwwyuvbpsaADzp
L$_small_initial_partial_block_AjwwyuvbpsaADzp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AjwwyuvbpsaADzp:

	orq	%r8,%r8
	je	L$_after_reduction_AjwwyuvbpsaADzp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AjwwyuvbpsaADzp:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_7_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_FgieCgAbtBDeoDF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_FgieCgAbtBDeoDF

L$_16_blocks_overflow_FgieCgAbtBDeoDF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_FgieCgAbtBDeoDF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dmxAfjjxulcpAbv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dmxAfjjxulcpAbv
L$_small_initial_partial_block_dmxAfjjxulcpAbv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dmxAfjjxulcpAbv:

	orq	%r8,%r8
	je	L$_after_reduction_dmxAfjjxulcpAbv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_dmxAfjjxulcpAbv:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_8_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_bpgvmwtjuhcCqvB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_bpgvmwtjuhcCqvB

L$_16_blocks_overflow_bpgvmwtjuhcCqvB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_bpgvmwtjuhcCqvB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vrDxpyGDeBzcrEA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vrDxpyGDeBzcrEA
L$_small_initial_partial_block_vrDxpyGDeBzcrEA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vrDxpyGDeBzcrEA:

	orq	%r8,%r8
	je	L$_after_reduction_vrDxpyGDeBzcrEA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vrDxpyGDeBzcrEA:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_9_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_auqGhoCyrqgvrwh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_auqGhoCyrqgvrwh

L$_16_blocks_overflow_auqGhoCyrqgvrwh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_auqGhoCyrqgvrwh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_icFermsivCDrtFv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_icFermsivCDrtFv
L$_small_initial_partial_block_icFermsivCDrtFv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_icFermsivCDrtFv:

	orq	%r8,%r8
	je	L$_after_reduction_icFermsivCDrtFv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_icFermsivCDrtFv:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_10_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_ECfngpthDznFdtu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_ECfngpthDznFdtu

L$_16_blocks_overflow_ECfngpthDznFdtu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_ECfngpthDznFdtu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_elemgArbxptFafr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_elemgArbxptFafr
L$_small_initial_partial_block_elemgArbxptFafr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_elemgArbxptFafr:

	orq	%r8,%r8
	je	L$_after_reduction_elemgArbxptFafr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_elemgArbxptFafr:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_11_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_CboGkptDrwupjlm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_CboGkptDrwupjlm

L$_16_blocks_overflow_CboGkptDrwupjlm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_CboGkptDrwupjlm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iaBsAdysvEmepBv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iaBsAdysvEmepBv
L$_small_initial_partial_block_iaBsAdysvEmepBv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iaBsAdysvEmepBv:

	orq	%r8,%r8
	je	L$_after_reduction_iaBsAdysvEmepBv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_iaBsAdysvEmepBv:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_12_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_EzxzDCCwzBhzqkg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_EzxzDCCwzBhzqkg

L$_16_blocks_overflow_EzxzDCCwzBhzqkg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_EzxzDCCwzBhzqkg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kmfckxCevatvfkC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kmfckxCevatvfkC
L$_small_initial_partial_block_kmfckxCevatvfkC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kmfckxCevatvfkC:

	orq	%r8,%r8
	je	L$_after_reduction_kmfckxCevatvfkC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kmfckxCevatvfkC:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_13_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_pErclpjBBnbDAae
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_pErclpjBBnbDAae

L$_16_blocks_overflow_pErclpjBBnbDAae:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_pErclpjBBnbDAae:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ADwcowrehFiwFdz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ADwcowrehFiwFdz
L$_small_initial_partial_block_ADwcowrehFiwFdz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ADwcowrehFiwFdz:

	orq	%r8,%r8
	je	L$_after_reduction_ADwcowrehFiwFdz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ADwcowrehFiwFdz:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_14_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_abFeqCFxAqwuurp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_abFeqCFxAqwuurp

L$_16_blocks_overflow_abFeqCFxAqwuurp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_abFeqCFxAqwuurp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_oFklEEEhDzvbgyc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_oFklEEEhDzvbgyc
L$_small_initial_partial_block_oFklEEEhDzvbgyc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_oFklEEEhDzvbgyc:

	orq	%r8,%r8
	je	L$_after_reduction_oFklEEEhDzvbgyc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_oFklEEEhDzvbgyc:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_15_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_BruwfwmjeBDmBFc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_BruwfwmjeBDmBFc

L$_16_blocks_overflow_BruwfwmjeBDmBFc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_BruwfwmjeBDmBFc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DECigjnGkEfheEh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DECigjnGkEfheEh
L$_small_initial_partial_block_DECigjnGkEfheEh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DECigjnGkEfheEh:

	orq	%r8,%r8
	je	L$_after_reduction_DECigjnGkEfheEh
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DECigjnGkEfheEh:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_16_hdjmDGFumdEyvrz:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_qfoDEwCwgxmBzGG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_qfoDEwCwgxmBzGG

L$_16_blocks_overflow_qfoDEwCwgxmBzGG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_qfoDEwCwgxmBzGG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_trjuxasnAredrdg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_trjuxasnAredrdg:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_trjuxasnAredrdg:
	jmp	L$_last_blocks_done_hdjmDGFumdEyvrz
L$_last_num_blocks_is_0_hdjmDGFumdEyvrz:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_hdjmDGFumdEyvrz:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_DuuzltgzkmbbyjG

L$_message_below_32_blocks_DuuzltgzkmbbyjG:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_xqgbAmvmxocdwCv
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
L$_skip_hkeys_precomputation_xqgbAmvmxocdwCv:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_ygAsqedfvzoeuet

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_ygAsqedfvzoeuet
	jb	L$_last_num_blocks_is_7_1_ygAsqedfvzoeuet


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_ygAsqedfvzoeuet
	jb	L$_last_num_blocks_is_11_9_ygAsqedfvzoeuet


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_ygAsqedfvzoeuet
	ja	L$_last_num_blocks_is_16_ygAsqedfvzoeuet
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_ygAsqedfvzoeuet
	jmp	L$_last_num_blocks_is_13_ygAsqedfvzoeuet

L$_last_num_blocks_is_11_9_ygAsqedfvzoeuet:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_ygAsqedfvzoeuet
	ja	L$_last_num_blocks_is_11_ygAsqedfvzoeuet
	jmp	L$_last_num_blocks_is_9_ygAsqedfvzoeuet

L$_last_num_blocks_is_7_1_ygAsqedfvzoeuet:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_ygAsqedfvzoeuet
	jb	L$_last_num_blocks_is_3_1_ygAsqedfvzoeuet

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_ygAsqedfvzoeuet
	je	L$_last_num_blocks_is_6_ygAsqedfvzoeuet
	jmp	L$_last_num_blocks_is_5_ygAsqedfvzoeuet

L$_last_num_blocks_is_3_1_ygAsqedfvzoeuet:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_ygAsqedfvzoeuet
	je	L$_last_num_blocks_is_2_ygAsqedfvzoeuet
L$_last_num_blocks_is_1_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_widttnhobmsqfCd
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_widttnhobmsqfCd

L$_16_blocks_overflow_widttnhobmsqfCd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_widttnhobmsqfCd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AGtEDgDCwCsBvbr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AGtEDgDCwCsBvbr
L$_small_initial_partial_block_AGtEDgDCwCsBvbr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_AGtEDgDCwCsBvbr
L$_small_initial_compute_done_AGtEDgDCwCsBvbr:
L$_after_reduction_AGtEDgDCwCsBvbr:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_2_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_GsxGFhEtaxbbByq
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_GsxGFhEtaxbbByq

L$_16_blocks_overflow_GsxGFhEtaxbbByq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_GsxGFhEtaxbbByq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mnkColmmknoxkrd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mnkColmmknoxkrd
L$_small_initial_partial_block_mnkColmmknoxkrd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mnkColmmknoxkrd:

	orq	%r8,%r8
	je	L$_after_reduction_mnkColmmknoxkrd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mnkColmmknoxkrd:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_3_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_fDrntpmmqbrjDpf
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_fDrntpmmqbrjDpf

L$_16_blocks_overflow_fDrntpmmqbrjDpf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_fDrntpmmqbrjDpf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DhmmdgecvnAnrli





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DhmmdgecvnAnrli
L$_small_initial_partial_block_DhmmdgecvnAnrli:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DhmmdgecvnAnrli:

	orq	%r8,%r8
	je	L$_after_reduction_DhmmdgecvnAnrli
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DhmmdgecvnAnrli:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_4_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_eglwxjrrmgCbomw
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_eglwxjrrmgCbomw

L$_16_blocks_overflow_eglwxjrrmgCbomw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_eglwxjrrmgCbomw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bDdCyquGleFgEgG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bDdCyquGleFgEgG
L$_small_initial_partial_block_bDdCyquGleFgEgG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bDdCyquGleFgEgG:

	orq	%r8,%r8
	je	L$_after_reduction_bDdCyquGleFgEgG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_bDdCyquGleFgEgG:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_5_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_GziocicxkaFvlfi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_GziocicxkaFvlfi

L$_16_blocks_overflow_GziocicxkaFvlfi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_GziocicxkaFvlfi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aBFbabdcAsbaCkf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aBFbabdcAsbaCkf
L$_small_initial_partial_block_aBFbabdcAsbaCkf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aBFbabdcAsbaCkf:

	orq	%r8,%r8
	je	L$_after_reduction_aBFbabdcAsbaCkf
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aBFbabdcAsbaCkf:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_6_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_zkcmiAxCkaFAunn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_zkcmiAxCkaFAunn

L$_16_blocks_overflow_zkcmiAxCkaFAunn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_zkcmiAxCkaFAunn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mnFrwjwCazvDwjD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mnFrwjwCazvDwjD
L$_small_initial_partial_block_mnFrwjwCazvDwjD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mnFrwjwCazvDwjD:

	orq	%r8,%r8
	je	L$_after_reduction_mnFrwjwCazvDwjD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mnFrwjwCazvDwjD:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_7_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_tAsheggecapDyhz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_tAsheggecapDyhz

L$_16_blocks_overflow_tAsheggecapDyhz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_tAsheggecapDyhz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EohmEuvapdEDpwk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EohmEuvapdEDpwk
L$_small_initial_partial_block_EohmEuvapdEDpwk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EohmEuvapdEDpwk:

	orq	%r8,%r8
	je	L$_after_reduction_EohmEuvapdEDpwk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EohmEuvapdEDpwk:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_8_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_jAcCsaywcqmyvDw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_jAcCsaywcqmyvDw

L$_16_blocks_overflow_jAcCsaywcqmyvDw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_jAcCsaywcqmyvDw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FjqFdpBbeshGtwG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FjqFdpBbeshGtwG
L$_small_initial_partial_block_FjqFdpBbeshGtwG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FjqFdpBbeshGtwG:

	orq	%r8,%r8
	je	L$_after_reduction_FjqFdpBbeshGtwG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FjqFdpBbeshGtwG:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_9_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_xfabeuleqCmszBy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_xfabeuleqCmszBy

L$_16_blocks_overflow_xfabeuleqCmszBy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_xfabeuleqCmszBy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ayjwwtvskkbAxto





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ayjwwtvskkbAxto
L$_small_initial_partial_block_ayjwwtvskkbAxto:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ayjwwtvskkbAxto:

	orq	%r8,%r8
	je	L$_after_reduction_ayjwwtvskkbAxto
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ayjwwtvskkbAxto:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_10_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_mmDbsvCakvdxjag
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_mmDbsvCakvdxjag

L$_16_blocks_overflow_mmDbsvCakvdxjag:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_mmDbsvCakvdxjag:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BslDhAqqauqwDqk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BslDhAqqauqwDqk
L$_small_initial_partial_block_BslDhAqqauqwDqk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BslDhAqqauqwDqk:

	orq	%r8,%r8
	je	L$_after_reduction_BslDhAqqauqwDqk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BslDhAqqauqwDqk:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_11_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_divavtuuGBzrBdF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_divavtuuGBzrBdF

L$_16_blocks_overflow_divavtuuGBzrBdF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_divavtuuGBzrBdF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DudDFCmgekiaApB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DudDFCmgekiaApB
L$_small_initial_partial_block_DudDFCmgekiaApB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DudDFCmgekiaApB:

	orq	%r8,%r8
	je	L$_after_reduction_DudDFCmgekiaApB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DudDFCmgekiaApB:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_12_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_bbrBtcCafmztrax
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_bbrBtcCafmztrax

L$_16_blocks_overflow_bbrBtcCafmztrax:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_bbrBtcCafmztrax:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ArfBxfrthBuwvze





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ArfBxfrthBuwvze
L$_small_initial_partial_block_ArfBxfrthBuwvze:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ArfBxfrthBuwvze:

	orq	%r8,%r8
	je	L$_after_reduction_ArfBxfrthBuwvze
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ArfBxfrthBuwvze:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_13_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_wGoykdCAhwxbvmc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_wGoykdCAhwxbvmc

L$_16_blocks_overflow_wGoykdCAhwxbvmc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_wGoykdCAhwxbvmc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ychdCqkykoDhomk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ychdCqkykoDhomk
L$_small_initial_partial_block_ychdCqkykoDhomk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ychdCqkykoDhomk:

	orq	%r8,%r8
	je	L$_after_reduction_ychdCqkykoDhomk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ychdCqkykoDhomk:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_14_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_scmpdqnawajssyp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_scmpdqnawajssyp

L$_16_blocks_overflow_scmpdqnawajssyp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_scmpdqnawajssyp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ihehdvwCBjegkqu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ihehdvwCBjegkqu
L$_small_initial_partial_block_ihehdvwCBjegkqu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ihehdvwCBjegkqu:

	orq	%r8,%r8
	je	L$_after_reduction_ihehdvwCBjegkqu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ihehdvwCBjegkqu:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_15_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_CrqoFFcmeFEAwaq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_CrqoFFcmeFEAwaq

L$_16_blocks_overflow_CrqoFFcmeFEAwaq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_CrqoFFcmeFEAwaq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mFyunaCAkvdggzb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mFyunaCAkvdggzb
L$_small_initial_partial_block_mFyunaCAkvdggzb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mFyunaCAkvdggzb:

	orq	%r8,%r8
	je	L$_after_reduction_mFyunaCAkvdggzb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mFyunaCAkvdggzb:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_16_ygAsqedfvzoeuet:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_oxulEAGGmozFbue
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_oxulEAGGmozFbue

L$_16_blocks_overflow_oxulEAGGmozFbue:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_oxulEAGGmozFbue:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_zrCtxovAcnDrooo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zrCtxovAcnDrooo:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zrCtxovAcnDrooo:
	jmp	L$_last_blocks_done_ygAsqedfvzoeuet
L$_last_num_blocks_is_0_ygAsqedfvzoeuet:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_ygAsqedfvzoeuet:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_DuuzltgzkmbbyjG

L$_message_below_equal_16_blocks_DuuzltgzkmbbyjG:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	L$_small_initial_num_blocks_is_8_plrbwbAurviGxtb
	jl	L$_small_initial_num_blocks_is_7_1_plrbwbAurviGxtb


	cmpq	$12,%r12
	je	L$_small_initial_num_blocks_is_12_plrbwbAurviGxtb
	jl	L$_small_initial_num_blocks_is_11_9_plrbwbAurviGxtb


	cmpq	$16,%r12
	je	L$_small_initial_num_blocks_is_16_plrbwbAurviGxtb
	cmpq	$15,%r12
	je	L$_small_initial_num_blocks_is_15_plrbwbAurviGxtb
	cmpq	$14,%r12
	je	L$_small_initial_num_blocks_is_14_plrbwbAurviGxtb
	jmp	L$_small_initial_num_blocks_is_13_plrbwbAurviGxtb

L$_small_initial_num_blocks_is_11_9_plrbwbAurviGxtb:

	cmpq	$11,%r12
	je	L$_small_initial_num_blocks_is_11_plrbwbAurviGxtb
	cmpq	$10,%r12
	je	L$_small_initial_num_blocks_is_10_plrbwbAurviGxtb
	jmp	L$_small_initial_num_blocks_is_9_plrbwbAurviGxtb

L$_small_initial_num_blocks_is_7_1_plrbwbAurviGxtb:
	cmpq	$4,%r12
	je	L$_small_initial_num_blocks_is_4_plrbwbAurviGxtb
	jl	L$_small_initial_num_blocks_is_3_1_plrbwbAurviGxtb

	cmpq	$7,%r12
	je	L$_small_initial_num_blocks_is_7_plrbwbAurviGxtb
	cmpq	$6,%r12
	je	L$_small_initial_num_blocks_is_6_plrbwbAurviGxtb
	jmp	L$_small_initial_num_blocks_is_5_plrbwbAurviGxtb

L$_small_initial_num_blocks_is_3_1_plrbwbAurviGxtb:

	cmpq	$3,%r12
	je	L$_small_initial_num_blocks_is_3_plrbwbAurviGxtb
	cmpq	$2,%r12
	je	L$_small_initial_num_blocks_is_2_plrbwbAurviGxtb





L$_small_initial_num_blocks_is_1_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DvhjumldznqglEe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DvhjumldznqglEe
L$_small_initial_partial_block_DvhjumldznqglEe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	L$_after_reduction_DvhjumldznqglEe
L$_small_initial_compute_done_DvhjumldznqglEe:
L$_after_reduction_DvhjumldznqglEe:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_2_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_smqBhDdBjEreFCk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_smqBhDdBjEreFCk
L$_small_initial_partial_block_smqBhDdBjEreFCk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_smqBhDdBjEreFCk:

	orq	%r8,%r8
	je	L$_after_reduction_smqBhDdBjEreFCk
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_smqBhDdBjEreFCk:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_3_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ljpvBguwBgwFgfF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ljpvBguwBgwFgfF
L$_small_initial_partial_block_ljpvBguwBgwFgfF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ljpvBguwBgwFgfF:

	orq	%r8,%r8
	je	L$_after_reduction_ljpvBguwBgwFgfF
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ljpvBguwBgwFgfF:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_4_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_myxuruAbwksDFtg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_myxuruAbwksDFtg
L$_small_initial_partial_block_myxuruAbwksDFtg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_myxuruAbwksDFtg:

	orq	%r8,%r8
	je	L$_after_reduction_myxuruAbwksDFtg
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_myxuruAbwksDFtg:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_5_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_psBoDbymtoCgDlj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_psBoDbymtoCgDlj
L$_small_initial_partial_block_psBoDbymtoCgDlj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_psBoDbymtoCgDlj:

	orq	%r8,%r8
	je	L$_after_reduction_psBoDbymtoCgDlj
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_psBoDbymtoCgDlj:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_6_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GscphAtFClmlhGB





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GscphAtFClmlhGB
L$_small_initial_partial_block_GscphAtFClmlhGB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GscphAtFClmlhGB:

	orq	%r8,%r8
	je	L$_after_reduction_GscphAtFClmlhGB
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_GscphAtFClmlhGB:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_7_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_Ahrbqdxjxqxpuuv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_Ahrbqdxjxqxpuuv
L$_small_initial_partial_block_Ahrbqdxjxqxpuuv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_Ahrbqdxjxqxpuuv:

	orq	%r8,%r8
	je	L$_after_reduction_Ahrbqdxjxqxpuuv
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_Ahrbqdxjxqxpuuv:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_8_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kzdiikdijdckhqA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kzdiikdijdckhqA
L$_small_initial_partial_block_kzdiikdijdckhqA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kzdiikdijdckhqA:

	orq	%r8,%r8
	je	L$_after_reduction_kzdiikdijdckhqA
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_kzdiikdijdckhqA:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_9_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CelzphhBdmjbBqf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CelzphhBdmjbBqf
L$_small_initial_partial_block_CelzphhBdmjbBqf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CelzphhBdmjbBqf:

	orq	%r8,%r8
	je	L$_after_reduction_CelzphhBdmjbBqf
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_CelzphhBdmjbBqf:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_10_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cnDbdtqgyfcfAaF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cnDbdtqgyfcfAaF
L$_small_initial_partial_block_cnDbdtqgyfcfAaF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cnDbdtqgyfcfAaF:

	orq	%r8,%r8
	je	L$_after_reduction_cnDbdtqgyfcfAaF
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_cnDbdtqgyfcfAaF:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_11_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FwEuqdywmrFeGBw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FwEuqdywmrFeGBw
L$_small_initial_partial_block_FwEuqdywmrFeGBw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FwEuqdywmrFeGBw:

	orq	%r8,%r8
	je	L$_after_reduction_FwEuqdywmrFeGBw
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_FwEuqdywmrFeGBw:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_12_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aClCEjrfAEdeaAD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aClCEjrfAEdeaAD
L$_small_initial_partial_block_aClCEjrfAEdeaAD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aClCEjrfAEdeaAD:

	orq	%r8,%r8
	je	L$_after_reduction_aClCEjrfAEdeaAD
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_aClCEjrfAEdeaAD:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_13_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_dzgopknlAtsndab





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_dzgopknlAtsndab
L$_small_initial_partial_block_dzgopknlAtsndab:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_dzgopknlAtsndab:

	orq	%r8,%r8
	je	L$_after_reduction_dzgopknlAtsndab
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_dzgopknlAtsndab:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_14_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CDBkvlyvCDbfzAq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CDBkvlyvCDbfzAq
L$_small_initial_partial_block_CDBkvlyvCDbfzAq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CDBkvlyvCDbfzAq:

	orq	%r8,%r8
	je	L$_after_reduction_CDBkvlyvCDbfzAq
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_CDBkvlyvCDbfzAq:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_15_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FkwjzkDhlcGqcnb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FkwjzkDhlcGqcnb
L$_small_initial_partial_block_FkwjzkDhlcGqcnb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FkwjzkDhlcGqcnb:

	orq	%r8,%r8
	je	L$_after_reduction_FkwjzkDhlcGqcnb
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_FkwjzkDhlcGqcnb:
	jmp	L$_small_initial_blocks_encrypted_plrbwbAurviGxtb
L$_small_initial_num_blocks_is_16_plrbwbAurviGxtb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_ibegspbuzunwzip:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ibegspbuzunwzip:
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ibegspbuzunwzip:
L$_small_initial_blocks_encrypted_plrbwbAurviGxtb:
L$_ghash_done_DuuzltgzkmbbyjG:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
L$_enc_dec_done_DuuzltgzkmbbyjG:
	jmp	L$exit_gcm_decrypt
.p2align	5
L$aes_gcm_decrypt_192_avx512:
	orq	%r8,%r8
	je	L$_enc_dec_done_klsaBjgiGmctyce
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	L$_partial_block_done_DDDakCzDwjqrExl
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	L$_no_extra_mask_DDDakCzDwjqrExl
	subq	%r13,%r12
L$_no_extra_mask_DDDakCzDwjqrExl:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	L$_partial_incomplete_DDDakCzDwjqrExl

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	L$_enc_dec_done_DDDakCzDwjqrExl

L$_partial_incomplete_DDDakCzDwjqrExl:
	addq	%r8,(%rdx)
	movq	%r8,%r11

L$_enc_dec_done_DDDakCzDwjqrExl:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
L$_partial_block_done_DDDakCzDwjqrExl:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	L$_enc_dec_done_klsaBjgiGmctyce
	cmpq	$256,%r8
	jbe	L$_message_below_equal_16_blocks_klsaBjgiGmctyce

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_bubgiqtyycunfDg
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_bubgiqtyycunfDg
L$_next_16_overflow_bubgiqtyycunfDg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_bubgiqtyycunfDg:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_joeqzqECgwpmmCj

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
L$_skip_hkeys_precomputation_joeqzqECgwpmmCj:
	cmpq	$512,%r8
	jb	L$_message_below_32_blocks_klsaBjgiGmctyce



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_DDkhleaAxCodnkv
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_DDkhleaAxCodnkv
L$_next_16_overflow_DDkhleaAxCodnkv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_DDkhleaAxCodnkv:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_qEmyChsisbkloxD
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
L$_skip_hkeys_precomputation_qEmyChsisbkloxD:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	L$_no_more_big_nblocks_klsaBjgiGmctyce
L$_encrypt_big_nblocks_klsaBjgiGmctyce:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_oFndwEmjvecFDcx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_oFndwEmjvecFDcx
L$_16_blocks_overflow_oFndwEmjvecFDcx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_oFndwEmjvecFDcx:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_vwmGqAuzvwxGgas
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_vwmGqAuzvwxGgas
L$_16_blocks_overflow_vwmGqAuzvwxGgas:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_vwmGqAuzvwxGgas:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_kzhldGycDcvDuyj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_kzhldGycDcvDuyj
L$_16_blocks_overflow_kzhldGycDcvDuyj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_kzhldGycDcvDuyj:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	L$_encrypt_big_nblocks_klsaBjgiGmctyce

L$_no_more_big_nblocks_klsaBjgiGmctyce:

	cmpq	$512,%r8
	jae	L$_encrypt_32_blocks_klsaBjgiGmctyce

	cmpq	$256,%r8
	jae	L$_encrypt_16_blocks_klsaBjgiGmctyce
L$_encrypt_0_blocks_ghash_32_klsaBjgiGmctyce:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_lEbccdcnmkgEpGA

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_lEbccdcnmkgEpGA
	jb	L$_last_num_blocks_is_7_1_lEbccdcnmkgEpGA


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_lEbccdcnmkgEpGA
	jb	L$_last_num_blocks_is_11_9_lEbccdcnmkgEpGA


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_lEbccdcnmkgEpGA
	ja	L$_last_num_blocks_is_16_lEbccdcnmkgEpGA
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_lEbccdcnmkgEpGA
	jmp	L$_last_num_blocks_is_13_lEbccdcnmkgEpGA

L$_last_num_blocks_is_11_9_lEbccdcnmkgEpGA:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_lEbccdcnmkgEpGA
	ja	L$_last_num_blocks_is_11_lEbccdcnmkgEpGA
	jmp	L$_last_num_blocks_is_9_lEbccdcnmkgEpGA

L$_last_num_blocks_is_7_1_lEbccdcnmkgEpGA:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_lEbccdcnmkgEpGA
	jb	L$_last_num_blocks_is_3_1_lEbccdcnmkgEpGA

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_lEbccdcnmkgEpGA
	je	L$_last_num_blocks_is_6_lEbccdcnmkgEpGA
	jmp	L$_last_num_blocks_is_5_lEbccdcnmkgEpGA

L$_last_num_blocks_is_3_1_lEbccdcnmkgEpGA:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_lEbccdcnmkgEpGA
	je	L$_last_num_blocks_is_2_lEbccdcnmkgEpGA
L$_last_num_blocks_is_1_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_bkxmriGwxpimrjD
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_bkxmriGwxpimrjD

L$_16_blocks_overflow_bkxmriGwxpimrjD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_bkxmriGwxpimrjD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lpsqklhiyluegyC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lpsqklhiyluegyC
L$_small_initial_partial_block_lpsqklhiyluegyC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_lpsqklhiyluegyC
L$_small_initial_compute_done_lpsqklhiyluegyC:
L$_after_reduction_lpsqklhiyluegyC:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_2_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_ujDxeCfzwcryofq
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_ujDxeCfzwcryofq

L$_16_blocks_overflow_ujDxeCfzwcryofq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_ujDxeCfzwcryofq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nmsCrCprytbDEqg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nmsCrCprytbDEqg
L$_small_initial_partial_block_nmsCrCprytbDEqg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nmsCrCprytbDEqg:

	orq	%r8,%r8
	je	L$_after_reduction_nmsCrCprytbDEqg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nmsCrCprytbDEqg:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_3_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_eavFdEzGuxCjgpb
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_eavFdEzGuxCjgpb

L$_16_blocks_overflow_eavFdEzGuxCjgpb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_eavFdEzGuxCjgpb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cCCjawmAsdsdvhb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cCCjawmAsdsdvhb
L$_small_initial_partial_block_cCCjawmAsdsdvhb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cCCjawmAsdsdvhb:

	orq	%r8,%r8
	je	L$_after_reduction_cCCjawmAsdsdvhb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_cCCjawmAsdsdvhb:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_4_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_vpvjehDrEtBiAeD
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_vpvjehDrEtBiAeD

L$_16_blocks_overflow_vpvjehDrEtBiAeD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_vpvjehDrEtBiAeD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aouuFctElwtnkmr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aouuFctElwtnkmr
L$_small_initial_partial_block_aouuFctElwtnkmr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aouuFctElwtnkmr:

	orq	%r8,%r8
	je	L$_after_reduction_aouuFctElwtnkmr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aouuFctElwtnkmr:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_5_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_alrxkzddxkanCdA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_alrxkzddxkanCdA

L$_16_blocks_overflow_alrxkzddxkanCdA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_alrxkzddxkanCdA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zEFbiCnvwtoiark





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zEFbiCnvwtoiark
L$_small_initial_partial_block_zEFbiCnvwtoiark:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zEFbiCnvwtoiark:

	orq	%r8,%r8
	je	L$_after_reduction_zEFbiCnvwtoiark
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zEFbiCnvwtoiark:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_6_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_ztmBjisDsAjwuEw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_ztmBjisDsAjwuEw

L$_16_blocks_overflow_ztmBjisDsAjwuEw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_ztmBjisDsAjwuEw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FxyvsrGglDwzCji





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FxyvsrGglDwzCji
L$_small_initial_partial_block_FxyvsrGglDwzCji:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FxyvsrGglDwzCji:

	orq	%r8,%r8
	je	L$_after_reduction_FxyvsrGglDwzCji
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FxyvsrGglDwzCji:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_7_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_yjmFsmzuuxGzztp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_yjmFsmzuuxGzztp

L$_16_blocks_overflow_yjmFsmzuuxGzztp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_yjmFsmzuuxGzztp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_llplmpncEfkkhGr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_llplmpncEfkkhGr
L$_small_initial_partial_block_llplmpncEfkkhGr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_llplmpncEfkkhGr:

	orq	%r8,%r8
	je	L$_after_reduction_llplmpncEfkkhGr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_llplmpncEfkkhGr:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_8_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_xbbGbdlBrAvBzFp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_xbbGbdlBrAvBzFp

L$_16_blocks_overflow_xbbGbdlBrAvBzFp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_xbbGbdlBrAvBzFp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bkwrqdbnmapvgCG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bkwrqdbnmapvgCG
L$_small_initial_partial_block_bkwrqdbnmapvgCG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bkwrqdbnmapvgCG:

	orq	%r8,%r8
	je	L$_after_reduction_bkwrqdbnmapvgCG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_bkwrqdbnmapvgCG:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_9_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_mGvwpbytdeEzAjx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_mGvwpbytdeEzAjx

L$_16_blocks_overflow_mGvwpbytdeEzAjx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_mGvwpbytdeEzAjx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lqpAalBigFxaheg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lqpAalBigFxaheg
L$_small_initial_partial_block_lqpAalBigFxaheg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lqpAalBigFxaheg:

	orq	%r8,%r8
	je	L$_after_reduction_lqpAalBigFxaheg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_lqpAalBigFxaheg:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_10_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_vGGrFcAyGjmfGuy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_vGGrFcAyGjmfGuy

L$_16_blocks_overflow_vGGrFcAyGjmfGuy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_vGGrFcAyGjmfGuy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fjGbcgzulwjqjBv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fjGbcgzulwjqjBv
L$_small_initial_partial_block_fjGbcgzulwjqjBv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fjGbcgzulwjqjBv:

	orq	%r8,%r8
	je	L$_after_reduction_fjGbcgzulwjqjBv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fjGbcgzulwjqjBv:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_11_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_fFekbbtxirxymGB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_fFekbbtxirxymGB

L$_16_blocks_overflow_fFekbbtxirxymGB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_fFekbbtxirxymGB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ifzvduredokhcCi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ifzvduredokhcCi
L$_small_initial_partial_block_ifzvduredokhcCi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ifzvduredokhcCi:

	orq	%r8,%r8
	je	L$_after_reduction_ifzvduredokhcCi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ifzvduredokhcCi:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_12_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_ysoohynsuljxotA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_ysoohynsuljxotA

L$_16_blocks_overflow_ysoohynsuljxotA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_ysoohynsuljxotA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_sonwGDpEpmtboid





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_sonwGDpEpmtboid
L$_small_initial_partial_block_sonwGDpEpmtboid:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_sonwGDpEpmtboid:

	orq	%r8,%r8
	je	L$_after_reduction_sonwGDpEpmtboid
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_sonwGDpEpmtboid:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_13_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_xvmgkfqpECDDqlq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_xvmgkfqpECDDqlq

L$_16_blocks_overflow_xvmgkfqpECDDqlq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_xvmgkfqpECDDqlq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gxysjCAuptkiygc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gxysjCAuptkiygc
L$_small_initial_partial_block_gxysjCAuptkiygc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gxysjCAuptkiygc:

	orq	%r8,%r8
	je	L$_after_reduction_gxysjCAuptkiygc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gxysjCAuptkiygc:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_14_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_cDDplgCetAAxoxt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_cDDplgCetAAxoxt

L$_16_blocks_overflow_cDDplgCetAAxoxt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_cDDplgCetAAxoxt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mqvwdcrtmdCwalB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mqvwdcrtmdCwalB
L$_small_initial_partial_block_mqvwdcrtmdCwalB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mqvwdcrtmdCwalB:

	orq	%r8,%r8
	je	L$_after_reduction_mqvwdcrtmdCwalB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mqvwdcrtmdCwalB:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_15_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_rEuostadeaBaeFr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_rEuostadeaBaeFr

L$_16_blocks_overflow_rEuostadeaBaeFr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_rEuostadeaBaeFr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_frxpuDCyFndtqbD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_frxpuDCyFndtqbD
L$_small_initial_partial_block_frxpuDCyFndtqbD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_frxpuDCyFndtqbD:

	orq	%r8,%r8
	je	L$_after_reduction_frxpuDCyFndtqbD
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_frxpuDCyFndtqbD:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_16_lEbccdcnmkgEpGA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_DCDcAFqnpEmgCgm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_DCDcAFqnpEmgCgm

L$_16_blocks_overflow_DCDcAFqnpEmgCgm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_DCDcAFqnpEmgCgm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_xszwrtvxnFnqceA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xszwrtvxnFnqceA:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xszwrtvxnFnqceA:
	jmp	L$_last_blocks_done_lEbccdcnmkgEpGA
L$_last_num_blocks_is_0_lEbccdcnmkgEpGA:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_lEbccdcnmkgEpGA:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_klsaBjgiGmctyce
L$_encrypt_32_blocks_klsaBjgiGmctyce:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_jyjnhngqBdoafFk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_jyjnhngqBdoafFk
L$_16_blocks_overflow_jyjnhngqBdoafFk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_jyjnhngqBdoafFk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_fDEmoeCyqbjiqnx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_fDEmoeCyqbjiqnx
L$_16_blocks_overflow_fDEmoeCyqbjiqnx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_fDEmoeCyqbjiqnx:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_yzyvnCtrlrlxder

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_yzyvnCtrlrlxder
	jb	L$_last_num_blocks_is_7_1_yzyvnCtrlrlxder


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_yzyvnCtrlrlxder
	jb	L$_last_num_blocks_is_11_9_yzyvnCtrlrlxder


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_yzyvnCtrlrlxder
	ja	L$_last_num_blocks_is_16_yzyvnCtrlrlxder
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_yzyvnCtrlrlxder
	jmp	L$_last_num_blocks_is_13_yzyvnCtrlrlxder

L$_last_num_blocks_is_11_9_yzyvnCtrlrlxder:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_yzyvnCtrlrlxder
	ja	L$_last_num_blocks_is_11_yzyvnCtrlrlxder
	jmp	L$_last_num_blocks_is_9_yzyvnCtrlrlxder

L$_last_num_blocks_is_7_1_yzyvnCtrlrlxder:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_yzyvnCtrlrlxder
	jb	L$_last_num_blocks_is_3_1_yzyvnCtrlrlxder

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_yzyvnCtrlrlxder
	je	L$_last_num_blocks_is_6_yzyvnCtrlrlxder
	jmp	L$_last_num_blocks_is_5_yzyvnCtrlrlxder

L$_last_num_blocks_is_3_1_yzyvnCtrlrlxder:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_yzyvnCtrlrlxder
	je	L$_last_num_blocks_is_2_yzyvnCtrlrlxder
L$_last_num_blocks_is_1_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_vDlrfpkcfcyjtwn
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_vDlrfpkcfcyjtwn

L$_16_blocks_overflow_vDlrfpkcfcyjtwn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_vDlrfpkcfcyjtwn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mDEmalGtDcrfcyq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mDEmalGtDcrfcyq
L$_small_initial_partial_block_mDEmalGtDcrfcyq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_mDEmalGtDcrfcyq
L$_small_initial_compute_done_mDEmalGtDcrfcyq:
L$_after_reduction_mDEmalGtDcrfcyq:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_2_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_BcClFdeDAiEAlEG
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_BcClFdeDAiEAlEG

L$_16_blocks_overflow_BcClFdeDAiEAlEG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_BcClFdeDAiEAlEG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_GwueveqhykneeCg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_GwueveqhykneeCg
L$_small_initial_partial_block_GwueveqhykneeCg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_GwueveqhykneeCg:

	orq	%r8,%r8
	je	L$_after_reduction_GwueveqhykneeCg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_GwueveqhykneeCg:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_3_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_xlFGotCiAxauAhy
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_xlFGotCiAxauAhy

L$_16_blocks_overflow_xlFGotCiAxauAhy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_xlFGotCiAxauAhy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CDqEeFtDqAyrnrj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CDqEeFtDqAyrnrj
L$_small_initial_partial_block_CDqEeFtDqAyrnrj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CDqEeFtDqAyrnrj:

	orq	%r8,%r8
	je	L$_after_reduction_CDqEeFtDqAyrnrj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CDqEeFtDqAyrnrj:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_4_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_nvimshcfADyjvFq
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_nvimshcfADyjvFq

L$_16_blocks_overflow_nvimshcfADyjvFq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_nvimshcfADyjvFq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mBnmarhFBhoDnef





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mBnmarhFBhoDnef
L$_small_initial_partial_block_mBnmarhFBhoDnef:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mBnmarhFBhoDnef:

	orq	%r8,%r8
	je	L$_after_reduction_mBnmarhFBhoDnef
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mBnmarhFBhoDnef:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_5_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_bjmBEuAldoreahh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_bjmBEuAldoreahh

L$_16_blocks_overflow_bjmBEuAldoreahh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_bjmBEuAldoreahh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xbhgxnmsbuFvkix





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xbhgxnmsbuFvkix
L$_small_initial_partial_block_xbhgxnmsbuFvkix:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xbhgxnmsbuFvkix:

	orq	%r8,%r8
	je	L$_after_reduction_xbhgxnmsbuFvkix
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xbhgxnmsbuFvkix:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_6_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_anokdFAwoxeobyD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_anokdFAwoxeobyD

L$_16_blocks_overflow_anokdFAwoxeobyD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_anokdFAwoxeobyD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BmzcDsiqDhkyozu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BmzcDsiqDhkyozu
L$_small_initial_partial_block_BmzcDsiqDhkyozu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BmzcDsiqDhkyozu:

	orq	%r8,%r8
	je	L$_after_reduction_BmzcDsiqDhkyozu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BmzcDsiqDhkyozu:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_7_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_EbDbpAFrxedhGiA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_EbDbpAFrxedhGiA

L$_16_blocks_overflow_EbDbpAFrxedhGiA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_EbDbpAFrxedhGiA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jxwermppvGuznaq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jxwermppvGuznaq
L$_small_initial_partial_block_jxwermppvGuznaq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jxwermppvGuznaq:

	orq	%r8,%r8
	je	L$_after_reduction_jxwermppvGuznaq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_jxwermppvGuznaq:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_8_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_bndkjfwedxfdlrk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_bndkjfwedxfdlrk

L$_16_blocks_overflow_bndkjfwedxfdlrk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_bndkjfwedxfdlrk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gauclDalvCgtkfu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gauclDalvCgtkfu
L$_small_initial_partial_block_gauclDalvCgtkfu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gauclDalvCgtkfu:

	orq	%r8,%r8
	je	L$_after_reduction_gauclDalvCgtkfu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gauclDalvCgtkfu:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_9_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_DkpyqBnwkcxrdsk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_DkpyqBnwkcxrdsk

L$_16_blocks_overflow_DkpyqBnwkcxrdsk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_DkpyqBnwkcxrdsk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uumeDokjAagjgtp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uumeDokjAagjgtp
L$_small_initial_partial_block_uumeDokjAagjgtp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uumeDokjAagjgtp:

	orq	%r8,%r8
	je	L$_after_reduction_uumeDokjAagjgtp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uumeDokjAagjgtp:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_10_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_xqyqbipdmBApqDi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_xqyqbipdmBApqDi

L$_16_blocks_overflow_xqyqbipdmBApqDi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_xqyqbipdmBApqDi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_glbzceoEfEvzACF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_glbzceoEfEvzACF
L$_small_initial_partial_block_glbzceoEfEvzACF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_glbzceoEfEvzACF:

	orq	%r8,%r8
	je	L$_after_reduction_glbzceoEfEvzACF
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_glbzceoEfEvzACF:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_11_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_eBwvhfGhuurrkxB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_eBwvhfGhuurrkxB

L$_16_blocks_overflow_eBwvhfGhuurrkxB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_eBwvhfGhuurrkxB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CeBeojAbvvzarpk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CeBeojAbvvzarpk
L$_small_initial_partial_block_CeBeojAbvvzarpk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CeBeojAbvvzarpk:

	orq	%r8,%r8
	je	L$_after_reduction_CeBeojAbvvzarpk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CeBeojAbvvzarpk:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_12_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_scufdtCgAABwsym
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_scufdtCgAABwsym

L$_16_blocks_overflow_scufdtCgAABwsym:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_scufdtCgAABwsym:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kfnFmvwxwnDebGj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kfnFmvwxwnDebGj
L$_small_initial_partial_block_kfnFmvwxwnDebGj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kfnFmvwxwnDebGj:

	orq	%r8,%r8
	je	L$_after_reduction_kfnFmvwxwnDebGj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kfnFmvwxwnDebGj:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_13_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_vdBGzhmChcdplEi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_vdBGzhmChcdplEi

L$_16_blocks_overflow_vdBGzhmChcdplEi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_vdBGzhmChcdplEi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lbrBocoFmGCGvkk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lbrBocoFmGCGvkk
L$_small_initial_partial_block_lbrBocoFmGCGvkk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lbrBocoFmGCGvkk:

	orq	%r8,%r8
	je	L$_after_reduction_lbrBocoFmGCGvkk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_lbrBocoFmGCGvkk:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_14_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_sdBtgvBgzmEynGB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_sdBtgvBgzmEynGB

L$_16_blocks_overflow_sdBtgvBgzmEynGB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_sdBtgvBgzmEynGB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aEhdDqphjsDEiek





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aEhdDqphjsDEiek
L$_small_initial_partial_block_aEhdDqphjsDEiek:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aEhdDqphjsDEiek:

	orq	%r8,%r8
	je	L$_after_reduction_aEhdDqphjsDEiek
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aEhdDqphjsDEiek:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_15_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_vuhieiEezrjefjm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_vuhieiEezrjefjm

L$_16_blocks_overflow_vuhieiEezrjefjm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_vuhieiEezrjefjm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mqheGnBAljywFqa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mqheGnBAljywFqa
L$_small_initial_partial_block_mqheGnBAljywFqa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mqheGnBAljywFqa:

	orq	%r8,%r8
	je	L$_after_reduction_mqheGnBAljywFqa
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mqheGnBAljywFqa:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_16_yzyvnCtrlrlxder:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_yqxkczxykEfxbyr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_yqxkczxykEfxbyr

L$_16_blocks_overflow_yqxkczxykEfxbyr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_yqxkczxykEfxbyr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_yhnrEqqwhrEggbm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yhnrEqqwhrEggbm:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yhnrEqqwhrEggbm:
	jmp	L$_last_blocks_done_yzyvnCtrlrlxder
L$_last_num_blocks_is_0_yzyvnCtrlrlxder:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_yzyvnCtrlrlxder:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_klsaBjgiGmctyce
L$_encrypt_16_blocks_klsaBjgiGmctyce:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_pwzEcCFkwjyuqDG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_pwzEcCFkwjyuqDG
L$_16_blocks_overflow_pwzEcCFkwjyuqDG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_pwzEcCFkwjyuqDG:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_wEgxfgCwprGhnFx

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_wEgxfgCwprGhnFx
	jb	L$_last_num_blocks_is_7_1_wEgxfgCwprGhnFx


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_wEgxfgCwprGhnFx
	jb	L$_last_num_blocks_is_11_9_wEgxfgCwprGhnFx


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_wEgxfgCwprGhnFx
	ja	L$_last_num_blocks_is_16_wEgxfgCwprGhnFx
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_wEgxfgCwprGhnFx
	jmp	L$_last_num_blocks_is_13_wEgxfgCwprGhnFx

L$_last_num_blocks_is_11_9_wEgxfgCwprGhnFx:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_wEgxfgCwprGhnFx
	ja	L$_last_num_blocks_is_11_wEgxfgCwprGhnFx
	jmp	L$_last_num_blocks_is_9_wEgxfgCwprGhnFx

L$_last_num_blocks_is_7_1_wEgxfgCwprGhnFx:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_wEgxfgCwprGhnFx
	jb	L$_last_num_blocks_is_3_1_wEgxfgCwprGhnFx

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_wEgxfgCwprGhnFx
	je	L$_last_num_blocks_is_6_wEgxfgCwprGhnFx
	jmp	L$_last_num_blocks_is_5_wEgxfgCwprGhnFx

L$_last_num_blocks_is_3_1_wEgxfgCwprGhnFx:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_wEgxfgCwprGhnFx
	je	L$_last_num_blocks_is_2_wEgxfgCwprGhnFx
L$_last_num_blocks_is_1_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_vBxDcwrDbfciFuG
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_vBxDcwrDbfciFuG

L$_16_blocks_overflow_vBxDcwrDbfciFuG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_vBxDcwrDbfciFuG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iroEsdfuAlyiwgG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iroEsdfuAlyiwgG
L$_small_initial_partial_block_iroEsdfuAlyiwgG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_iroEsdfuAlyiwgG
L$_small_initial_compute_done_iroEsdfuAlyiwgG:
L$_after_reduction_iroEsdfuAlyiwgG:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_2_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_FGepzwhjrcxhzjx
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_FGepzwhjrcxhzjx

L$_16_blocks_overflow_FGepzwhjrcxhzjx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_FGepzwhjrcxhzjx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jrjGGmptmFvnlsd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jrjGGmptmFvnlsd
L$_small_initial_partial_block_jrjGGmptmFvnlsd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jrjGGmptmFvnlsd:

	orq	%r8,%r8
	je	L$_after_reduction_jrjGGmptmFvnlsd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_jrjGGmptmFvnlsd:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_3_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_DaoAEpkrlafDEeA
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_DaoAEpkrlafDEeA

L$_16_blocks_overflow_DaoAEpkrlafDEeA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_DaoAEpkrlafDEeA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fquwzjncwcbizev





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fquwzjncwcbizev
L$_small_initial_partial_block_fquwzjncwcbizev:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fquwzjncwcbizev:

	orq	%r8,%r8
	je	L$_after_reduction_fquwzjncwcbizev
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fquwzjncwcbizev:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_4_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_bDBihbbDzbfbmyw
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_bDBihbbDzbfbmyw

L$_16_blocks_overflow_bDBihbbDzbfbmyw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_bDBihbbDzbfbmyw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ywpkdqwoGbyefaf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ywpkdqwoGbyefaf
L$_small_initial_partial_block_ywpkdqwoGbyefaf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ywpkdqwoGbyefaf:

	orq	%r8,%r8
	je	L$_after_reduction_ywpkdqwoGbyefaf
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ywpkdqwoGbyefaf:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_5_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_ycDdCfsezEwcAym
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_ycDdCfsezEwcAym

L$_16_blocks_overflow_ycDdCfsezEwcAym:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_ycDdCfsezEwcAym:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ceaFcCqjBddxhjE





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ceaFcCqjBddxhjE
L$_small_initial_partial_block_ceaFcCqjBddxhjE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ceaFcCqjBddxhjE:

	orq	%r8,%r8
	je	L$_after_reduction_ceaFcCqjBddxhjE
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ceaFcCqjBddxhjE:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_6_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_DxxFcDEykrGyqtE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_DxxFcDEykrGyqtE

L$_16_blocks_overflow_DxxFcDEykrGyqtE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_DxxFcDEykrGyqtE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FvGFEyhwGsmdzyg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FvGFEyhwGsmdzyg
L$_small_initial_partial_block_FvGFEyhwGsmdzyg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FvGFEyhwGsmdzyg:

	orq	%r8,%r8
	je	L$_after_reduction_FvGFEyhwGsmdzyg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FvGFEyhwGsmdzyg:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_7_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_wAxcpGzadyjFkBF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_wAxcpGzadyjFkBF

L$_16_blocks_overflow_wAxcpGzadyjFkBF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_wAxcpGzadyjFkBF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nxnGubywruordFc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nxnGubywruordFc
L$_small_initial_partial_block_nxnGubywruordFc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nxnGubywruordFc:

	orq	%r8,%r8
	je	L$_after_reduction_nxnGubywruordFc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nxnGubywruordFc:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_8_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_zGGpjFdsmtjmvcA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_zGGpjFdsmtjmvcA

L$_16_blocks_overflow_zGGpjFdsmtjmvcA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_zGGpjFdsmtjmvcA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vzEfGiesvwunxGr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vzEfGiesvwunxGr
L$_small_initial_partial_block_vzEfGiesvwunxGr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vzEfGiesvwunxGr:

	orq	%r8,%r8
	je	L$_after_reduction_vzEfGiesvwunxGr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vzEfGiesvwunxGr:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_9_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_nfzusrnBhgAqchy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_nfzusrnBhgAqchy

L$_16_blocks_overflow_nfzusrnBhgAqchy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_nfzusrnBhgAqchy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ewDllbiavqvhuqm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ewDllbiavqvhuqm
L$_small_initial_partial_block_ewDllbiavqvhuqm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ewDllbiavqvhuqm:

	orq	%r8,%r8
	je	L$_after_reduction_ewDllbiavqvhuqm
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ewDllbiavqvhuqm:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_10_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_EaDahlaFbGBqgji
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_EaDahlaFbGBqgji

L$_16_blocks_overflow_EaDahlaFbGBqgji:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_EaDahlaFbGBqgji:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_AbpohhvlBEqlhli





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_AbpohhvlBEqlhli
L$_small_initial_partial_block_AbpohhvlBEqlhli:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_AbpohhvlBEqlhli:

	orq	%r8,%r8
	je	L$_after_reduction_AbpohhvlBEqlhli
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_AbpohhvlBEqlhli:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_11_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_AhdgGDevbqlnodc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_AhdgGDevbqlnodc

L$_16_blocks_overflow_AhdgGDevbqlnodc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_AhdgGDevbqlnodc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ttlikclDEjDcomk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ttlikclDEjDcomk
L$_small_initial_partial_block_ttlikclDEjDcomk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ttlikclDEjDcomk:

	orq	%r8,%r8
	je	L$_after_reduction_ttlikclDEjDcomk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ttlikclDEjDcomk:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_12_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_hBclrDbxAyosfpg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_hBclrDbxAyosfpg

L$_16_blocks_overflow_hBclrDbxAyosfpg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_hBclrDbxAyosfpg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qzzfexvAzqzEwap





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qzzfexvAzqzEwap
L$_small_initial_partial_block_qzzfexvAzqzEwap:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qzzfexvAzqzEwap:

	orq	%r8,%r8
	je	L$_after_reduction_qzzfexvAzqzEwap
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qzzfexvAzqzEwap:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_13_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_gnrGGcgglzgivgd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_gnrGGcgglzgivgd

L$_16_blocks_overflow_gnrGGcgglzgivgd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_gnrGGcgglzgivgd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vjwlwafDDpBlCmd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vjwlwafDDpBlCmd
L$_small_initial_partial_block_vjwlwafDDpBlCmd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vjwlwafDDpBlCmd:

	orq	%r8,%r8
	je	L$_after_reduction_vjwlwafDDpBlCmd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vjwlwafDDpBlCmd:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_14_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_BmttgcAfmaFtmmm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_BmttgcAfmaFtmmm

L$_16_blocks_overflow_BmttgcAfmaFtmmm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_BmttgcAfmaFtmmm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ucmesFhjjhcEjBq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ucmesFhjjhcEjBq
L$_small_initial_partial_block_ucmesFhjjhcEjBq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ucmesFhjjhcEjBq:

	orq	%r8,%r8
	je	L$_after_reduction_ucmesFhjjhcEjBq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ucmesFhjjhcEjBq:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_15_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_igyBzyemAgdwygn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_igyBzyemAgdwygn

L$_16_blocks_overflow_igyBzyemAgdwygn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_igyBzyemAgdwygn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pvzApeooGxrasco





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pvzApeooGxrasco
L$_small_initial_partial_block_pvzApeooGxrasco:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pvzApeooGxrasco:

	orq	%r8,%r8
	je	L$_after_reduction_pvzApeooGxrasco
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pvzApeooGxrasco:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_16_wEgxfgCwprGhnFx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_bphDCpdFkciguge
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_bphDCpdFkciguge

L$_16_blocks_overflow_bphDCpdFkciguge:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_bphDCpdFkciguge:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_jiDflqzdGrCwkyj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jiDflqzdGrCwkyj:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_jiDflqzdGrCwkyj:
	jmp	L$_last_blocks_done_wEgxfgCwprGhnFx
L$_last_num_blocks_is_0_wEgxfgCwprGhnFx:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_wEgxfgCwprGhnFx:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_klsaBjgiGmctyce

L$_message_below_32_blocks_klsaBjgiGmctyce:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_mswGnstkGEvGlbv
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
L$_skip_hkeys_precomputation_mswGnstkGEvGlbv:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_skGlnAFisdoczuo

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_skGlnAFisdoczuo
	jb	L$_last_num_blocks_is_7_1_skGlnAFisdoczuo


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_skGlnAFisdoczuo
	jb	L$_last_num_blocks_is_11_9_skGlnAFisdoczuo


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_skGlnAFisdoczuo
	ja	L$_last_num_blocks_is_16_skGlnAFisdoczuo
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_skGlnAFisdoczuo
	jmp	L$_last_num_blocks_is_13_skGlnAFisdoczuo

L$_last_num_blocks_is_11_9_skGlnAFisdoczuo:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_skGlnAFisdoczuo
	ja	L$_last_num_blocks_is_11_skGlnAFisdoczuo
	jmp	L$_last_num_blocks_is_9_skGlnAFisdoczuo

L$_last_num_blocks_is_7_1_skGlnAFisdoczuo:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_skGlnAFisdoczuo
	jb	L$_last_num_blocks_is_3_1_skGlnAFisdoczuo

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_skGlnAFisdoczuo
	je	L$_last_num_blocks_is_6_skGlnAFisdoczuo
	jmp	L$_last_num_blocks_is_5_skGlnAFisdoczuo

L$_last_num_blocks_is_3_1_skGlnAFisdoczuo:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_skGlnAFisdoczuo
	je	L$_last_num_blocks_is_2_skGlnAFisdoczuo
L$_last_num_blocks_is_1_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_kBscuwdCAgmjqFo
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_kBscuwdCAgmjqFo

L$_16_blocks_overflow_kBscuwdCAgmjqFo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_kBscuwdCAgmjqFo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_typBqFwxzdhiokl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_typBqFwxzdhiokl
L$_small_initial_partial_block_typBqFwxzdhiokl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_typBqFwxzdhiokl
L$_small_initial_compute_done_typBqFwxzdhiokl:
L$_after_reduction_typBqFwxzdhiokl:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_2_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_xfwtxzDymhdrjty
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_xfwtxzDymhdrjty

L$_16_blocks_overflow_xfwtxzDymhdrjty:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_xfwtxzDymhdrjty:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ekrtpxlfrleukAv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ekrtpxlfrleukAv
L$_small_initial_partial_block_ekrtpxlfrleukAv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ekrtpxlfrleukAv:

	orq	%r8,%r8
	je	L$_after_reduction_ekrtpxlfrleukAv
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ekrtpxlfrleukAv:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_3_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_ynibBchbmjuBtgd
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_ynibBchbmjuBtgd

L$_16_blocks_overflow_ynibBchbmjuBtgd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_ynibBchbmjuBtgd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rtGzucddjbdeCaf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rtGzucddjbdeCaf
L$_small_initial_partial_block_rtGzucddjbdeCaf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rtGzucddjbdeCaf:

	orq	%r8,%r8
	je	L$_after_reduction_rtGzucddjbdeCaf
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rtGzucddjbdeCaf:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_4_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_aoaDlvphrczArlz
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_aoaDlvphrczArlz

L$_16_blocks_overflow_aoaDlvphrczArlz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_aoaDlvphrczArlz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zAjdEhDvBqfGaiq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zAjdEhDvBqfGaiq
L$_small_initial_partial_block_zAjdEhDvBqfGaiq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zAjdEhDvBqfGaiq:

	orq	%r8,%r8
	je	L$_after_reduction_zAjdEhDvBqfGaiq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_zAjdEhDvBqfGaiq:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_5_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_ECsjChBqswpDjtu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_ECsjChBqswpDjtu

L$_16_blocks_overflow_ECsjChBqswpDjtu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_ECsjChBqswpDjtu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xbwFDbCAjmzeing





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xbwFDbCAjmzeing
L$_small_initial_partial_block_xbwFDbCAjmzeing:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xbwFDbCAjmzeing:

	orq	%r8,%r8
	je	L$_after_reduction_xbwFDbCAjmzeing
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xbwFDbCAjmzeing:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_6_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_xGxBvbzkbqqqotw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_xGxBvbzkbqqqotw

L$_16_blocks_overflow_xGxBvbzkbqqqotw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_xGxBvbzkbqqqotw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ukGhxAyAlcfGhkq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ukGhxAyAlcfGhkq
L$_small_initial_partial_block_ukGhxAyAlcfGhkq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ukGhxAyAlcfGhkq:

	orq	%r8,%r8
	je	L$_after_reduction_ukGhxAyAlcfGhkq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ukGhxAyAlcfGhkq:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_7_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_AwAvosvEqCusFno
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_AwAvosvEqCusFno

L$_16_blocks_overflow_AwAvosvEqCusFno:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_AwAvosvEqCusFno:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mbfunzyBhrzthGc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mbfunzyBhrzthGc
L$_small_initial_partial_block_mbfunzyBhrzthGc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mbfunzyBhrzthGc:

	orq	%r8,%r8
	je	L$_after_reduction_mbfunzyBhrzthGc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mbfunzyBhrzthGc:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_8_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_zmypBpgblvsoEFr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_zmypBpgblvsoEFr

L$_16_blocks_overflow_zmypBpgblvsoEFr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_zmypBpgblvsoEFr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rqvzEhgzuvknCck





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rqvzEhgzuvknCck
L$_small_initial_partial_block_rqvzEhgzuvknCck:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rqvzEhgzuvknCck:

	orq	%r8,%r8
	je	L$_after_reduction_rqvzEhgzuvknCck
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_rqvzEhgzuvknCck:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_9_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_wFEnsCgnorDgyzy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_wFEnsCgnorDgyzy

L$_16_blocks_overflow_wFEnsCgnorDgyzy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_wFEnsCgnorDgyzy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BFahFpElqcfhsbj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BFahFpElqcfhsbj
L$_small_initial_partial_block_BFahFpElqcfhsbj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BFahFpElqcfhsbj:

	orq	%r8,%r8
	je	L$_after_reduction_BFahFpElqcfhsbj
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BFahFpElqcfhsbj:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_10_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_fzGijxGesrmyjni
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_fzGijxGesrmyjni

L$_16_blocks_overflow_fzGijxGesrmyjni:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_fzGijxGesrmyjni:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_yzkgBBeFCFqEoux





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_yzkgBBeFCFqEoux
L$_small_initial_partial_block_yzkgBBeFCFqEoux:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_yzkgBBeFCFqEoux:

	orq	%r8,%r8
	je	L$_after_reduction_yzkgBBeFCFqEoux
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_yzkgBBeFCFqEoux:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_11_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_DFFpuFwBlwxzlsr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_DFFpuFwBlwxzlsr

L$_16_blocks_overflow_DFFpuFwBlwxzlsr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_DFFpuFwBlwxzlsr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pGEEBclGgemtprh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pGEEBclGgemtprh
L$_small_initial_partial_block_pGEEBclGgemtprh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pGEEBclGgemtprh:

	orq	%r8,%r8
	je	L$_after_reduction_pGEEBclGgemtprh
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pGEEBclGgemtprh:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_12_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_CbfFfowgrxslhsu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_CbfFfowgrxslhsu

L$_16_blocks_overflow_CbfFfowgrxslhsu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_CbfFfowgrxslhsu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ejmokAazpnleqAx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ejmokAazpnleqAx
L$_small_initial_partial_block_ejmokAazpnleqAx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ejmokAazpnleqAx:

	orq	%r8,%r8
	je	L$_after_reduction_ejmokAazpnleqAx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ejmokAazpnleqAx:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_13_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_rxkbnicmxBeacpE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_rxkbnicmxBeacpE

L$_16_blocks_overflow_rxkbnicmxBeacpE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_rxkbnicmxBeacpE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iAmjaCGAEDklAgi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iAmjaCGAEDklAgi
L$_small_initial_partial_block_iAmjaCGAEDklAgi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iAmjaCGAEDklAgi:

	orq	%r8,%r8
	je	L$_after_reduction_iAmjaCGAEDklAgi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_iAmjaCGAEDklAgi:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_14_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_dAjdsbwpEpltcrm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_dAjdsbwpEpltcrm

L$_16_blocks_overflow_dAjdsbwpEpltcrm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_dAjdsbwpEpltcrm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DhlAFkkbDlxkllB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DhlAFkkbDlxkllB
L$_small_initial_partial_block_DhlAFkkbDlxkllB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DhlAFkkbDlxkllB:

	orq	%r8,%r8
	je	L$_after_reduction_DhlAFkkbDlxkllB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_DhlAFkkbDlxkllB:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_15_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_AnoyDEfapxhhdie
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_AnoyDEfapxhhdie

L$_16_blocks_overflow_AnoyDEfapxhhdie:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_AnoyDEfapxhhdie:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EjxenmAwcGpfvwm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EjxenmAwcGpfvwm
L$_small_initial_partial_block_EjxenmAwcGpfvwm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EjxenmAwcGpfvwm:

	orq	%r8,%r8
	je	L$_after_reduction_EjxenmAwcGpfvwm
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EjxenmAwcGpfvwm:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_16_skGlnAFisdoczuo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_lrvjgbdbpofcuxe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_lrvjgbdbpofcuxe

L$_16_blocks_overflow_lrvjgbdbpofcuxe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_lrvjgbdbpofcuxe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_pFvjdgCjDsxumGy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pFvjdgCjDsxumGy:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pFvjdgCjDsxumGy:
	jmp	L$_last_blocks_done_skGlnAFisdoczuo
L$_last_num_blocks_is_0_skGlnAFisdoczuo:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_skGlnAFisdoczuo:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_klsaBjgiGmctyce

L$_message_below_equal_16_blocks_klsaBjgiGmctyce:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	L$_small_initial_num_blocks_is_8_AyavxxBGffeljFm
	jl	L$_small_initial_num_blocks_is_7_1_AyavxxBGffeljFm


	cmpq	$12,%r12
	je	L$_small_initial_num_blocks_is_12_AyavxxBGffeljFm
	jl	L$_small_initial_num_blocks_is_11_9_AyavxxBGffeljFm


	cmpq	$16,%r12
	je	L$_small_initial_num_blocks_is_16_AyavxxBGffeljFm
	cmpq	$15,%r12
	je	L$_small_initial_num_blocks_is_15_AyavxxBGffeljFm
	cmpq	$14,%r12
	je	L$_small_initial_num_blocks_is_14_AyavxxBGffeljFm
	jmp	L$_small_initial_num_blocks_is_13_AyavxxBGffeljFm

L$_small_initial_num_blocks_is_11_9_AyavxxBGffeljFm:

	cmpq	$11,%r12
	je	L$_small_initial_num_blocks_is_11_AyavxxBGffeljFm
	cmpq	$10,%r12
	je	L$_small_initial_num_blocks_is_10_AyavxxBGffeljFm
	jmp	L$_small_initial_num_blocks_is_9_AyavxxBGffeljFm

L$_small_initial_num_blocks_is_7_1_AyavxxBGffeljFm:
	cmpq	$4,%r12
	je	L$_small_initial_num_blocks_is_4_AyavxxBGffeljFm
	jl	L$_small_initial_num_blocks_is_3_1_AyavxxBGffeljFm

	cmpq	$7,%r12
	je	L$_small_initial_num_blocks_is_7_AyavxxBGffeljFm
	cmpq	$6,%r12
	je	L$_small_initial_num_blocks_is_6_AyavxxBGffeljFm
	jmp	L$_small_initial_num_blocks_is_5_AyavxxBGffeljFm

L$_small_initial_num_blocks_is_3_1_AyavxxBGffeljFm:

	cmpq	$3,%r12
	je	L$_small_initial_num_blocks_is_3_AyavxxBGffeljFm
	cmpq	$2,%r12
	je	L$_small_initial_num_blocks_is_2_AyavxxBGffeljFm





L$_small_initial_num_blocks_is_1_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tBrBceAetetvBni





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tBrBceAetetvBni
L$_small_initial_partial_block_tBrBceAetetvBni:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	L$_after_reduction_tBrBceAetetvBni
L$_small_initial_compute_done_tBrBceAetetvBni:
L$_after_reduction_tBrBceAetetvBni:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_2_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BmlkhrohcGlknkg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BmlkhrohcGlknkg
L$_small_initial_partial_block_BmlkhrohcGlknkg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BmlkhrohcGlknkg:

	orq	%r8,%r8
	je	L$_after_reduction_BmlkhrohcGlknkg
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_BmlkhrohcGlknkg:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_3_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bxzoEkBAAndpmvz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bxzoEkBAAndpmvz
L$_small_initial_partial_block_bxzoEkBAAndpmvz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bxzoEkBAAndpmvz:

	orq	%r8,%r8
	je	L$_after_reduction_bxzoEkBAAndpmvz
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_bxzoEkBAAndpmvz:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_4_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uovCAaCABgowhAy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uovCAaCABgowhAy
L$_small_initial_partial_block_uovCAaCABgowhAy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uovCAaCABgowhAy:

	orq	%r8,%r8
	je	L$_after_reduction_uovCAaCABgowhAy
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_uovCAaCABgowhAy:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_5_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_wteyxwseqwolFBe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_wteyxwseqwolFBe
L$_small_initial_partial_block_wteyxwseqwolFBe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_wteyxwseqwolFBe:

	orq	%r8,%r8
	je	L$_after_reduction_wteyxwseqwolFBe
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_wteyxwseqwolFBe:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_6_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kwAqmFsEAcnhxoh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kwAqmFsEAcnhxoh
L$_small_initial_partial_block_kwAqmFsEAcnhxoh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kwAqmFsEAcnhxoh:

	orq	%r8,%r8
	je	L$_after_reduction_kwAqmFsEAcnhxoh
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_kwAqmFsEAcnhxoh:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_7_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aqyssBmicrhaDnk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aqyssBmicrhaDnk
L$_small_initial_partial_block_aqyssBmicrhaDnk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aqyssBmicrhaDnk:

	orq	%r8,%r8
	je	L$_after_reduction_aqyssBmicrhaDnk
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_aqyssBmicrhaDnk:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_8_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gubDEllehzgDvyt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gubDEllehzgDvyt
L$_small_initial_partial_block_gubDEllehzgDvyt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gubDEllehzgDvyt:

	orq	%r8,%r8
	je	L$_after_reduction_gubDEllehzgDvyt
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_gubDEllehzgDvyt:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_9_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vnxnkmzhFmyoGuc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vnxnkmzhFmyoGuc
L$_small_initial_partial_block_vnxnkmzhFmyoGuc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vnxnkmzhFmyoGuc:

	orq	%r8,%r8
	je	L$_after_reduction_vnxnkmzhFmyoGuc
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_vnxnkmzhFmyoGuc:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_10_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EDbyfyqqlEglDfE





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EDbyfyqqlEglDfE
L$_small_initial_partial_block_EDbyfyqqlEglDfE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EDbyfyqqlEglDfE:

	orq	%r8,%r8
	je	L$_after_reduction_EDbyfyqqlEglDfE
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_EDbyfyqqlEglDfE:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_11_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lbcskcGADsbbksx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lbcskcGADsbbksx
L$_small_initial_partial_block_lbcskcGADsbbksx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lbcskcGADsbbksx:

	orq	%r8,%r8
	je	L$_after_reduction_lbcskcGADsbbksx
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_lbcskcGADsbbksx:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_12_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ABCkDCzoxhCdlcj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ABCkDCzoxhCdlcj
L$_small_initial_partial_block_ABCkDCzoxhCdlcj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ABCkDCzoxhCdlcj:

	orq	%r8,%r8
	je	L$_after_reduction_ABCkDCzoxhCdlcj
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ABCkDCzoxhCdlcj:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_13_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fyGijCpFzebEvyr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fyGijCpFzebEvyr
L$_small_initial_partial_block_fyGijCpFzebEvyr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fyGijCpFzebEvyr:

	orq	%r8,%r8
	je	L$_after_reduction_fyGijCpFzebEvyr
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_fyGijCpFzebEvyr:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_14_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_tyCvjmxFizdazzb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_tyCvjmxFizdazzb
L$_small_initial_partial_block_tyCvjmxFizdazzb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_tyCvjmxFizdazzb:

	orq	%r8,%r8
	je	L$_after_reduction_tyCvjmxFizdazzb
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_tyCvjmxFizdazzb:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_15_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qcCalckpCAzDrzC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qcCalckpCAzDrzC
L$_small_initial_partial_block_qcCalckpCAzDrzC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qcCalckpCAzDrzC:

	orq	%r8,%r8
	je	L$_after_reduction_qcCalckpCAzDrzC
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_qcCalckpCAzDrzC:
	jmp	L$_small_initial_blocks_encrypted_AyavxxBGffeljFm
L$_small_initial_num_blocks_is_16_AyavxxBGffeljFm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_rxluirwgsCnstoy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rxluirwgsCnstoy:
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_rxluirwgsCnstoy:
L$_small_initial_blocks_encrypted_AyavxxBGffeljFm:
L$_ghash_done_klsaBjgiGmctyce:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
L$_enc_dec_done_klsaBjgiGmctyce:
	jmp	L$exit_gcm_decrypt
.p2align	5
L$aes_gcm_decrypt_256_avx512:
	orq	%r8,%r8
	je	L$_enc_dec_done_jfayxkkFBEajEfk
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	L$_partial_block_done_darCEGAsFCahGdc
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	L$_no_extra_mask_darCEGAsFCahGdc
	subq	%r13,%r12
L$_no_extra_mask_darCEGAsFCahGdc:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	L$_partial_incomplete_darCEGAsFCahGdc

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	L$_enc_dec_done_darCEGAsFCahGdc

L$_partial_incomplete_darCEGAsFCahGdc:
	addq	%r8,(%rdx)
	movq	%r8,%r11

L$_enc_dec_done_darCEGAsFCahGdc:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
L$_partial_block_done_darCEGAsFCahGdc:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	L$_enc_dec_done_jfayxkkFBEajEfk
	cmpq	$256,%r8
	jbe	L$_message_below_equal_16_blocks_jfayxkkFBEajEfk

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_BvoutxkFztFnrfE
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_BvoutxkFztFnrfE
L$_next_16_overflow_BvoutxkFztFnrfE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_BvoutxkFztFnrfE:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_oFbxcCAvkppgfwh

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
L$_skip_hkeys_precomputation_oFbxcCAvkppgfwh:
	cmpq	$512,%r8
	jb	L$_message_below_32_blocks_jfayxkkFBEajEfk



	cmpb	$240,%r15b
	jae	L$_next_16_overflow_mGfyCCphBocFvBl
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	L$_next_16_ok_mGfyCCphBocFvBl
L$_next_16_overflow_mGfyCCphBocFvBl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
L$_next_16_ok_mGfyCCphBocFvBl:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_saluemdfBboGqgf
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
L$_skip_hkeys_precomputation_saluemdfBboGqgf:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	L$_no_more_big_nblocks_jfayxkkFBEajEfk
L$_encrypt_big_nblocks_jfayxkkFBEajEfk:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_xwbBtahrfcuuiGr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_xwbBtahrfcuuiGr
L$_16_blocks_overflow_xwbBtahrfcuuiGr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_xwbBtahrfcuuiGr:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_wkGiBmnsezvBzeb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_wkGiBmnsezvBzeb
L$_16_blocks_overflow_wkGiBmnsezvBzeb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_wkGiBmnsezvBzeb:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_alwzpsmfrvhcAdA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_alwzpsmfrvhcAdA
L$_16_blocks_overflow_alwzpsmfrvhcAdA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_alwzpsmfrvhcAdA:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	L$_encrypt_big_nblocks_jfayxkkFBEajEfk

L$_no_more_big_nblocks_jfayxkkFBEajEfk:

	cmpq	$512,%r8
	jae	L$_encrypt_32_blocks_jfayxkkFBEajEfk

	cmpq	$256,%r8
	jae	L$_encrypt_16_blocks_jfayxkkFBEajEfk
L$_encrypt_0_blocks_ghash_32_jfayxkkFBEajEfk:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_yhyAkqbCGAvmlvd

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_yhyAkqbCGAvmlvd
	jb	L$_last_num_blocks_is_7_1_yhyAkqbCGAvmlvd


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_yhyAkqbCGAvmlvd
	jb	L$_last_num_blocks_is_11_9_yhyAkqbCGAvmlvd


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_yhyAkqbCGAvmlvd
	ja	L$_last_num_blocks_is_16_yhyAkqbCGAvmlvd
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_yhyAkqbCGAvmlvd
	jmp	L$_last_num_blocks_is_13_yhyAkqbCGAvmlvd

L$_last_num_blocks_is_11_9_yhyAkqbCGAvmlvd:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_yhyAkqbCGAvmlvd
	ja	L$_last_num_blocks_is_11_yhyAkqbCGAvmlvd
	jmp	L$_last_num_blocks_is_9_yhyAkqbCGAvmlvd

L$_last_num_blocks_is_7_1_yhyAkqbCGAvmlvd:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_yhyAkqbCGAvmlvd
	jb	L$_last_num_blocks_is_3_1_yhyAkqbCGAvmlvd

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_yhyAkqbCGAvmlvd
	je	L$_last_num_blocks_is_6_yhyAkqbCGAvmlvd
	jmp	L$_last_num_blocks_is_5_yhyAkqbCGAvmlvd

L$_last_num_blocks_is_3_1_yhyAkqbCGAvmlvd:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_yhyAkqbCGAvmlvd
	je	L$_last_num_blocks_is_2_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_1_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_kzdDrDkBtFDviGs
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_kzdDrDkBtFDviGs

L$_16_blocks_overflow_kzdDrDkBtFDviGs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_kzdDrDkBtFDviGs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qbhlztpwBEmhGdF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qbhlztpwBEmhGdF
L$_small_initial_partial_block_qbhlztpwBEmhGdF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_qbhlztpwBEmhGdF
L$_small_initial_compute_done_qbhlztpwBEmhGdF:
L$_after_reduction_qbhlztpwBEmhGdF:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_2_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_fmpoEhznngEBAAc
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_fmpoEhznngEBAAc

L$_16_blocks_overflow_fmpoEhznngEBAAc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_fmpoEhznngEBAAc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vACeuDAedidEwyr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vACeuDAedidEwyr
L$_small_initial_partial_block_vACeuDAedidEwyr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vACeuDAedidEwyr:

	orq	%r8,%r8
	je	L$_after_reduction_vACeuDAedidEwyr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vACeuDAedidEwyr:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_3_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_kdmgCoqBavEekfz
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_kdmgCoqBavEekfz

L$_16_blocks_overflow_kdmgCoqBavEekfz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_kdmgCoqBavEekfz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EyorDBhmnreqlBc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EyorDBhmnreqlBc
L$_small_initial_partial_block_EyorDBhmnreqlBc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EyorDBhmnreqlBc:

	orq	%r8,%r8
	je	L$_after_reduction_EyorDBhmnreqlBc
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EyorDBhmnreqlBc:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_4_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_auxqbnFDwuDmCaG
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_auxqbnFDwuDmCaG

L$_16_blocks_overflow_auxqbnFDwuDmCaG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_auxqbnFDwuDmCaG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ElGwvgpvzgyiegn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ElGwvgpvzgyiegn
L$_small_initial_partial_block_ElGwvgpvzgyiegn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ElGwvgpvzgyiegn:

	orq	%r8,%r8
	je	L$_after_reduction_ElGwvgpvzgyiegn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ElGwvgpvzgyiegn:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_5_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_ckaygrpfGznioaf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_ckaygrpfGznioaf

L$_16_blocks_overflow_ckaygrpfGznioaf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_ckaygrpfGznioaf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cCrrkauebCpApxo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cCrrkauebCpApxo
L$_small_initial_partial_block_cCrrkauebCpApxo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cCrrkauebCpApxo:

	orq	%r8,%r8
	je	L$_after_reduction_cCrrkauebCpApxo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_cCrrkauebCpApxo:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_6_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_emfydfjwketnnnD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_emfydfjwketnnnD

L$_16_blocks_overflow_emfydfjwketnnnD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_emfydfjwketnnnD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_bvCwpyxGtCFamjo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_bvCwpyxGtCFamjo
L$_small_initial_partial_block_bvCwpyxGtCFamjo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_bvCwpyxGtCFamjo:

	orq	%r8,%r8
	je	L$_after_reduction_bvCwpyxGtCFamjo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_bvCwpyxGtCFamjo:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_7_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_coFpBaeawiCGfrG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_coFpBaeawiCGfrG

L$_16_blocks_overflow_coFpBaeawiCGfrG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_coFpBaeawiCGfrG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hEDFhubFDagqGdw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hEDFhubFDagqGdw
L$_small_initial_partial_block_hEDFhubFDagqGdw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hEDFhubFDagqGdw:

	orq	%r8,%r8
	je	L$_after_reduction_hEDFhubFDagqGdw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hEDFhubFDagqGdw:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_8_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_qlikCnrqzemrjbi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_qlikCnrqzemrjbi

L$_16_blocks_overflow_qlikCnrqzemrjbi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_qlikCnrqzemrjbi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gsbmjgubyogsCaA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gsbmjgubyogsCaA
L$_small_initial_partial_block_gsbmjgubyogsCaA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gsbmjgubyogsCaA:

	orq	%r8,%r8
	je	L$_after_reduction_gsbmjgubyogsCaA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gsbmjgubyogsCaA:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_9_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_ocxGlcynAlByAyl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_ocxGlcynAlByAyl

L$_16_blocks_overflow_ocxGlcynAlByAyl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_ocxGlcynAlByAyl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_sunyynjowoFCkhz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_sunyynjowoFCkhz
L$_small_initial_partial_block_sunyynjowoFCkhz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_sunyynjowoFCkhz:

	orq	%r8,%r8
	je	L$_after_reduction_sunyynjowoFCkhz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_sunyynjowoFCkhz:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_10_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_llFdBsBbGDjjblt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_llFdBsBbGDjjblt

L$_16_blocks_overflow_llFdBsBbGDjjblt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_llFdBsBbGDjjblt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_agzowrvceglbdAo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_agzowrvceglbdAo
L$_small_initial_partial_block_agzowrvceglbdAo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_agzowrvceglbdAo:

	orq	%r8,%r8
	je	L$_after_reduction_agzowrvceglbdAo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_agzowrvceglbdAo:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_11_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_xwapnwizktAEBEw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_xwapnwizktAEBEw

L$_16_blocks_overflow_xwapnwizktAEBEw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_xwapnwizktAEBEw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mlkkvbldijmtmAh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mlkkvbldijmtmAh
L$_small_initial_partial_block_mlkkvbldijmtmAh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mlkkvbldijmtmAh:

	orq	%r8,%r8
	je	L$_after_reduction_mlkkvbldijmtmAh
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mlkkvbldijmtmAh:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_12_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_upgsDnFsEuhnhaB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_upgsDnFsEuhnhaB

L$_16_blocks_overflow_upgsDnFsEuhnhaB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_upgsDnFsEuhnhaB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_btCesCuFtjckDrg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_btCesCuFtjckDrg
L$_small_initial_partial_block_btCesCuFtjckDrg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_btCesCuFtjckDrg:

	orq	%r8,%r8
	je	L$_after_reduction_btCesCuFtjckDrg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_btCesCuFtjckDrg:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_13_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_dazoftzhraldFDw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_dazoftzhraldFDw

L$_16_blocks_overflow_dazoftzhraldFDw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_dazoftzhraldFDw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EyggctjgBonfszk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EyggctjgBonfszk
L$_small_initial_partial_block_EyggctjgBonfszk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EyggctjgBonfszk:

	orq	%r8,%r8
	je	L$_after_reduction_EyggctjgBonfszk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EyggctjgBonfszk:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_14_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_qGpgskfqisBjBEF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_qGpgskfqisBjBEF

L$_16_blocks_overflow_qGpgskfqisBjBEF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_qGpgskfqisBjBEF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_fhyejBdnErkgkeg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_fhyejBdnErkgkeg
L$_small_initial_partial_block_fhyejBdnErkgkeg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fhyejBdnErkgkeg:

	orq	%r8,%r8
	je	L$_after_reduction_fhyejBdnErkgkeg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fhyejBdnErkgkeg:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_15_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_AipxvllywGbazkm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_AipxvllywGbazkm

L$_16_blocks_overflow_AipxvllywGbazkm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_AipxvllywGbazkm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xdgfaxvbpFlADvz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xdgfaxvbpFlADvz
L$_small_initial_partial_block_xdgfaxvbpFlADvz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xdgfaxvbpFlADvz:

	orq	%r8,%r8
	je	L$_after_reduction_xdgfaxvbpFlADvz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xdgfaxvbpFlADvz:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_16_yhyAkqbCGAvmlvd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_CsGGpzelDhGnGCC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_CsGGpzelDhGnGCC

L$_16_blocks_overflow_CsGGpzelDhGnGCC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_CsGGpzelDhGnGCC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_qfkkttcwzkqhfaC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qfkkttcwzkqhfaC:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qfkkttcwzkqhfaC:
	jmp	L$_last_blocks_done_yhyAkqbCGAvmlvd
L$_last_num_blocks_is_0_yhyAkqbCGAvmlvd:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_yhyAkqbCGAvmlvd:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_jfayxkkFBEajEfk
L$_encrypt_32_blocks_jfayxkkFBEajEfk:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_pubvsAcveDvkhEf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_pubvsAcveDvkhEf
L$_16_blocks_overflow_pubvsAcveDvkhEf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_pubvsAcveDvkhEf:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_xCpqAkmeppauDij
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_xCpqAkmeppauDij
L$_16_blocks_overflow_xCpqAkmeppauDij:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_xCpqAkmeppauDij:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_mlsFaBbawyoyxjl

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_mlsFaBbawyoyxjl
	jb	L$_last_num_blocks_is_7_1_mlsFaBbawyoyxjl


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_mlsFaBbawyoyxjl
	jb	L$_last_num_blocks_is_11_9_mlsFaBbawyoyxjl


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_mlsFaBbawyoyxjl
	ja	L$_last_num_blocks_is_16_mlsFaBbawyoyxjl
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_mlsFaBbawyoyxjl
	jmp	L$_last_num_blocks_is_13_mlsFaBbawyoyxjl

L$_last_num_blocks_is_11_9_mlsFaBbawyoyxjl:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_mlsFaBbawyoyxjl
	ja	L$_last_num_blocks_is_11_mlsFaBbawyoyxjl
	jmp	L$_last_num_blocks_is_9_mlsFaBbawyoyxjl

L$_last_num_blocks_is_7_1_mlsFaBbawyoyxjl:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_mlsFaBbawyoyxjl
	jb	L$_last_num_blocks_is_3_1_mlsFaBbawyoyxjl

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_mlsFaBbawyoyxjl
	je	L$_last_num_blocks_is_6_mlsFaBbawyoyxjl
	jmp	L$_last_num_blocks_is_5_mlsFaBbawyoyxjl

L$_last_num_blocks_is_3_1_mlsFaBbawyoyxjl:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_mlsFaBbawyoyxjl
	je	L$_last_num_blocks_is_2_mlsFaBbawyoyxjl
L$_last_num_blocks_is_1_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_pFGoxdpppzuppnd
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_pFGoxdpppzuppnd

L$_16_blocks_overflow_pFGoxdpppzuppnd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_pFGoxdpppzuppnd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xbdxqlEwksCsdfa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xbdxqlEwksCsdfa
L$_small_initial_partial_block_xbdxqlEwksCsdfa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_xbdxqlEwksCsdfa
L$_small_initial_compute_done_xbdxqlEwksCsdfa:
L$_after_reduction_xbdxqlEwksCsdfa:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_2_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_sincyazqkglznma
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_sincyazqkglznma

L$_16_blocks_overflow_sincyazqkglznma:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_sincyazqkglznma:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vwFDfibibDkgbkl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vwFDfibibDkgbkl
L$_small_initial_partial_block_vwFDfibibDkgbkl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vwFDfibibDkgbkl:

	orq	%r8,%r8
	je	L$_after_reduction_vwFDfibibDkgbkl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vwFDfibibDkgbkl:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_3_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_diEbbohzAhBFeuk
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_diEbbohzAhBFeuk

L$_16_blocks_overflow_diEbbohzAhBFeuk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_diEbbohzAhBFeuk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mycudpjipyokCiy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mycudpjipyokCiy
L$_small_initial_partial_block_mycudpjipyokCiy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mycudpjipyokCiy:

	orq	%r8,%r8
	je	L$_after_reduction_mycudpjipyokCiy
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mycudpjipyokCiy:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_4_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_broupgvnyADfgdG
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_broupgvnyADfgdG

L$_16_blocks_overflow_broupgvnyADfgdG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_broupgvnyADfgdG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_geFemwCgiiCvzdh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_geFemwCgiiCvzdh
L$_small_initial_partial_block_geFemwCgiiCvzdh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_geFemwCgiiCvzdh:

	orq	%r8,%r8
	je	L$_after_reduction_geFemwCgiiCvzdh
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_geFemwCgiiCvzdh:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_5_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_caCwBvGAsmbdAau
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_caCwBvGAsmbdAau

L$_16_blocks_overflow_caCwBvGAsmbdAau:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_caCwBvGAsmbdAau:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mFDpyEauhlorbAx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mFDpyEauhlorbAx
L$_small_initial_partial_block_mFDpyEauhlorbAx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mFDpyEauhlorbAx:

	orq	%r8,%r8
	je	L$_after_reduction_mFDpyEauhlorbAx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mFDpyEauhlorbAx:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_6_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_yEggaubCnhvbqke
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_yEggaubCnhvbqke

L$_16_blocks_overflow_yEggaubCnhvbqke:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_yEggaubCnhvbqke:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ujEcaAuwukEzpFC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ujEcaAuwukEzpFC
L$_small_initial_partial_block_ujEcaAuwukEzpFC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ujEcaAuwukEzpFC:

	orq	%r8,%r8
	je	L$_after_reduction_ujEcaAuwukEzpFC
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ujEcaAuwukEzpFC:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_7_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_yiscyfzwspyeBaq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_yiscyfzwspyeBaq

L$_16_blocks_overflow_yiscyfzwspyeBaq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_yiscyfzwspyeBaq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_CqqikFedhFeavxg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_CqqikFedhFeavxg
L$_small_initial_partial_block_CqqikFedhFeavxg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_CqqikFedhFeavxg:

	orq	%r8,%r8
	je	L$_after_reduction_CqqikFedhFeavxg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_CqqikFedhFeavxg:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_8_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_ayxuCeFxkuBvznd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_ayxuCeFxkuBvznd

L$_16_blocks_overflow_ayxuCeFxkuBvznd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_ayxuCeFxkuBvznd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_Amzwfeatqlsipqk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_Amzwfeatqlsipqk
L$_small_initial_partial_block_Amzwfeatqlsipqk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_Amzwfeatqlsipqk:

	orq	%r8,%r8
	je	L$_after_reduction_Amzwfeatqlsipqk
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_Amzwfeatqlsipqk:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_9_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_szkvEisdCssltcn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_szkvEisdCssltcn

L$_16_blocks_overflow_szkvEisdCssltcn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_szkvEisdCssltcn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_orvhjyEztwAnDob





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_orvhjyEztwAnDob
L$_small_initial_partial_block_orvhjyEztwAnDob:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_orvhjyEztwAnDob:

	orq	%r8,%r8
	je	L$_after_reduction_orvhjyEztwAnDob
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_orvhjyEztwAnDob:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_10_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_DAAfarahlhGmtFb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_DAAfarahlhGmtFb

L$_16_blocks_overflow_DAAfarahlhGmtFb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_DAAfarahlhGmtFb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_iFEtkuEqelwgwwG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_iFEtkuEqelwgwwG
L$_small_initial_partial_block_iFEtkuEqelwgwwG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_iFEtkuEqelwgwwG:

	orq	%r8,%r8
	je	L$_after_reduction_iFEtkuEqelwgwwG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_iFEtkuEqelwgwwG:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_11_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_gBbBvwydbyBagto
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_gBbBvwydbyBagto

L$_16_blocks_overflow_gBbBvwydbyBagto:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_gBbBvwydbyBagto:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_oexyqCBshtaBumu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_oexyqCBshtaBumu
L$_small_initial_partial_block_oexyqCBshtaBumu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_oexyqCBshtaBumu:

	orq	%r8,%r8
	je	L$_after_reduction_oexyqCBshtaBumu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_oexyqCBshtaBumu:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_12_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_qqybiqDxDBrjuxu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_qqybiqDxDBrjuxu

L$_16_blocks_overflow_qqybiqDxDBrjuxu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_qqybiqDxDBrjuxu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EjwnbBtzkggdfBx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EjwnbBtzkggdfBx
L$_small_initial_partial_block_EjwnbBtzkggdfBx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EjwnbBtzkggdfBx:

	orq	%r8,%r8
	je	L$_after_reduction_EjwnbBtzkggdfBx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EjwnbBtzkggdfBx:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_13_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_bCGvsumrtGCtatt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_bCGvsumrtGCtatt

L$_16_blocks_overflow_bCGvsumrtGCtatt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_bCGvsumrtGCtatt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pEbagefcewuChCi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pEbagefcewuChCi
L$_small_initial_partial_block_pEbagefcewuChCi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_pEbagefcewuChCi:

	orq	%r8,%r8
	je	L$_after_reduction_pEbagefcewuChCi
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_pEbagefcewuChCi:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_14_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_uhlgtftgsuEwEoz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_uhlgtftgsuEwEoz

L$_16_blocks_overflow_uhlgtftgsuEwEoz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_uhlgtftgsuEwEoz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kjohAecgGgvfdAB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kjohAecgGgvfdAB
L$_small_initial_partial_block_kjohAecgGgvfdAB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kjohAecgGgvfdAB:

	orq	%r8,%r8
	je	L$_after_reduction_kjohAecgGgvfdAB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kjohAecgGgvfdAB:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_15_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_EwwdAlopgDinyGa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_EwwdAlopgDinyGa

L$_16_blocks_overflow_EwwdAlopgDinyGa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_EwwdAlopgDinyGa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aDwkbxngEGskctg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aDwkbxngEGskctg
L$_small_initial_partial_block_aDwkbxngEGskctg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aDwkbxngEGskctg:

	orq	%r8,%r8
	je	L$_after_reduction_aDwkbxngEGskctg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aDwkbxngEGskctg:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_16_mlsFaBbawyoyxjl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_gAdmkowCcdywFhs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_gAdmkowCcdywFhs

L$_16_blocks_overflow_gAdmkowCcdywFhs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_gAdmkowCcdywFhs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_ydGvvCzyustatqD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ydGvvCzyustatqD:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ydGvvCzyustatqD:
	jmp	L$_last_blocks_done_mlsFaBbawyoyxjl
L$_last_num_blocks_is_0_mlsFaBbawyoyxjl:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_mlsFaBbawyoyxjl:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_jfayxkkFBEajEfk
L$_encrypt_16_blocks_jfayxkkFBEajEfk:
	cmpb	$240,%r15b
	jae	L$_16_blocks_overflow_DebhtsakCsdqGnE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_DebhtsakCsdqGnE
L$_16_blocks_overflow_DebhtsakCsdqGnE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_DebhtsakCsdqGnE:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_zFrxmfffmvmphAn

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_zFrxmfffmvmphAn
	jb	L$_last_num_blocks_is_7_1_zFrxmfffmvmphAn


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_zFrxmfffmvmphAn
	jb	L$_last_num_blocks_is_11_9_zFrxmfffmvmphAn


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_zFrxmfffmvmphAn
	ja	L$_last_num_blocks_is_16_zFrxmfffmvmphAn
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_zFrxmfffmvmphAn
	jmp	L$_last_num_blocks_is_13_zFrxmfffmvmphAn

L$_last_num_blocks_is_11_9_zFrxmfffmvmphAn:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_zFrxmfffmvmphAn
	ja	L$_last_num_blocks_is_11_zFrxmfffmvmphAn
	jmp	L$_last_num_blocks_is_9_zFrxmfffmvmphAn

L$_last_num_blocks_is_7_1_zFrxmfffmvmphAn:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_zFrxmfffmvmphAn
	jb	L$_last_num_blocks_is_3_1_zFrxmfffmvmphAn

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_zFrxmfffmvmphAn
	je	L$_last_num_blocks_is_6_zFrxmfffmvmphAn
	jmp	L$_last_num_blocks_is_5_zFrxmfffmvmphAn

L$_last_num_blocks_is_3_1_zFrxmfffmvmphAn:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_zFrxmfffmvmphAn
	je	L$_last_num_blocks_is_2_zFrxmfffmvmphAn
L$_last_num_blocks_is_1_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_rgrseFrouBaAhAE
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_rgrseFrouBaAhAE

L$_16_blocks_overflow_rgrseFrouBaAhAE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_rgrseFrouBaAhAE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pcsrupvkEpwhCwG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pcsrupvkEpwhCwG
L$_small_initial_partial_block_pcsrupvkEpwhCwG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_pcsrupvkEpwhCwG
L$_small_initial_compute_done_pcsrupvkEpwhCwG:
L$_after_reduction_pcsrupvkEpwhCwG:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_2_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_mhaCCdbxygjmFbd
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_mhaCCdbxygjmFbd

L$_16_blocks_overflow_mhaCCdbxygjmFbd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_mhaCCdbxygjmFbd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_htkqjpuqvebnqvx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_htkqjpuqvebnqvx
L$_small_initial_partial_block_htkqjpuqvebnqvx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_htkqjpuqvebnqvx:

	orq	%r8,%r8
	je	L$_after_reduction_htkqjpuqvebnqvx
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_htkqjpuqvebnqvx:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_3_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_ldDGrBpCGbmypEA
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_ldDGrBpCGbmypEA

L$_16_blocks_overflow_ldDGrBpCGbmypEA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_ldDGrBpCGbmypEA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mBlfgfziAmAhtgo





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mBlfgfziAmAhtgo
L$_small_initial_partial_block_mBlfgfziAmAhtgo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mBlfgfziAmAhtgo:

	orq	%r8,%r8
	je	L$_after_reduction_mBlfgfziAmAhtgo
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mBlfgfziAmAhtgo:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_4_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_cAlwFobnddbpCvs
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_cAlwFobnddbpCvs

L$_16_blocks_overflow_cAlwFobnddbpCvs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_cAlwFobnddbpCvs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vDeFGggrDhgGtAA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vDeFGggrDhgGtAA
L$_small_initial_partial_block_vDeFGggrDhgGtAA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vDeFGggrDhgGtAA:

	orq	%r8,%r8
	je	L$_after_reduction_vDeFGggrDhgGtAA
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vDeFGggrDhgGtAA:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_5_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_hrGhbtiAysoqaAq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_hrGhbtiAysoqaAq

L$_16_blocks_overflow_hrGhbtiAysoqaAq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_hrGhbtiAysoqaAq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_flAvzkFdhcnonin





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_flAvzkFdhcnonin
L$_small_initial_partial_block_flAvzkFdhcnonin:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_flAvzkFdhcnonin:

	orq	%r8,%r8
	je	L$_after_reduction_flAvzkFdhcnonin
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_flAvzkFdhcnonin:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_6_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_walgGvopwwsqoGs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_walgGvopwwsqoGs

L$_16_blocks_overflow_walgGvopwwsqoGs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_walgGvopwwsqoGs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_uscnrpvkswkDmdd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_uscnrpvkswkDmdd
L$_small_initial_partial_block_uscnrpvkswkDmdd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_uscnrpvkswkDmdd:

	orq	%r8,%r8
	je	L$_after_reduction_uscnrpvkswkDmdd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_uscnrpvkswkDmdd:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_7_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_haBuwrgcrpvqqaw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_haBuwrgcrpvqqaw

L$_16_blocks_overflow_haBuwrgcrpvqqaw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_haBuwrgcrpvqqaw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BgxdClfaEbCDdFt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BgxdClfaEbCDdFt
L$_small_initial_partial_block_BgxdClfaEbCDdFt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BgxdClfaEbCDdFt:

	orq	%r8,%r8
	je	L$_after_reduction_BgxdClfaEbCDdFt
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BgxdClfaEbCDdFt:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_8_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_lltcgrcerEpnkjA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_lltcgrcerEpnkjA

L$_16_blocks_overflow_lltcgrcerEpnkjA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_lltcgrcerEpnkjA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hhiqBatxrFzBnCg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hhiqBatxrFzBnCg
L$_small_initial_partial_block_hhiqBatxrFzBnCg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hhiqBatxrFzBnCg:

	orq	%r8,%r8
	je	L$_after_reduction_hhiqBatxrFzBnCg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hhiqBatxrFzBnCg:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_9_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_Bvynglyubsdnvlv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_Bvynglyubsdnvlv

L$_16_blocks_overflow_Bvynglyubsdnvlv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_Bvynglyubsdnvlv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BndiCboFdFpaoxl





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BndiCboFdFpaoxl
L$_small_initial_partial_block_BndiCboFdFpaoxl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BndiCboFdFpaoxl:

	orq	%r8,%r8
	je	L$_after_reduction_BndiCboFdFpaoxl
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BndiCboFdFpaoxl:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_10_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_aeAdGxbGxBmwBjs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_aeAdGxbGxBmwBjs

L$_16_blocks_overflow_aeAdGxbGxBmwBjs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_aeAdGxbGxBmwBjs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_EnrugtDxztcFvmp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_EnrugtDxztcFvmp
L$_small_initial_partial_block_EnrugtDxztcFvmp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_EnrugtDxztcFvmp:

	orq	%r8,%r8
	je	L$_after_reduction_EnrugtDxztcFvmp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_EnrugtDxztcFvmp:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_11_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_jsjeocnEsnqiige
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_jsjeocnEsnqiige

L$_16_blocks_overflow_jsjeocnEsnqiige:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_jsjeocnEsnqiige:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cwCdcGnxswysiyn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cwCdcGnxswysiyn
L$_small_initial_partial_block_cwCdcGnxswysiyn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cwCdcGnxswysiyn:

	orq	%r8,%r8
	je	L$_after_reduction_cwCdcGnxswysiyn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_cwCdcGnxswysiyn:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_12_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_tekfvjuavoErnvd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_tekfvjuavoErnvd

L$_16_blocks_overflow_tekfvjuavoErnvd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_tekfvjuavoErnvd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FtkjiaioDyryiBq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FtkjiaioDyryiBq
L$_small_initial_partial_block_FtkjiaioDyryiBq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FtkjiaioDyryiBq:

	orq	%r8,%r8
	je	L$_after_reduction_FtkjiaioDyryiBq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_FtkjiaioDyryiBq:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_13_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_uErBbkArdGeEDjg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_uErBbkArdGeEDjg

L$_16_blocks_overflow_uErBbkArdGeEDjg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_uErBbkArdGeEDjg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_hyeGuFhAqsDbFkp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_hyeGuFhAqsDbFkp
L$_small_initial_partial_block_hyeGuFhAqsDbFkp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_hyeGuFhAqsDbFkp:

	orq	%r8,%r8
	je	L$_after_reduction_hyeGuFhAqsDbFkp
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_hyeGuFhAqsDbFkp:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_14_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_zhrrauGcajqgzeB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_zhrrauGcajqgzeB

L$_16_blocks_overflow_zhrrauGcajqgzeB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_zhrrauGcajqgzeB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_slxnufwhdlobwta





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_slxnufwhdlobwta
L$_small_initial_partial_block_slxnufwhdlobwta:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_slxnufwhdlobwta:

	orq	%r8,%r8
	je	L$_after_reduction_slxnufwhdlobwta
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_slxnufwhdlobwta:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_15_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_BkvmDiyDtBGBsvj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_BkvmDiyDtBGBsvj

L$_16_blocks_overflow_BkvmDiyDtBGBsvj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_BkvmDiyDtBGBsvj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_eDtfsjGlzDclirw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_eDtfsjGlzDclirw
L$_small_initial_partial_block_eDtfsjGlzDclirw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_eDtfsjGlzDclirw:

	orq	%r8,%r8
	je	L$_after_reduction_eDtfsjGlzDclirw
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_eDtfsjGlzDclirw:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_16_zFrxmfffmvmphAn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_bBzEwavlnbmvfbh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_bBzEwavlnbmvfbh

L$_16_blocks_overflow_bBzEwavlnbmvfbh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_bBzEwavlnbmvfbh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_qGjuvCAnAevopGd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qGjuvCAnAevopGd:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qGjuvCAnAevopGd:
	jmp	L$_last_blocks_done_zFrxmfffmvmphAn
L$_last_num_blocks_is_0_zFrxmfffmvmphAn:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_zFrxmfffmvmphAn:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_jfayxkkFBEajEfk

L$_message_below_32_blocks_jfayxkkFBEajEfk:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	L$_skip_hkeys_precomputation_kuuaetznwolpwfe
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
L$_skip_hkeys_precomputation_kuuaetznwolpwfe:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	L$_last_num_blocks_is_0_hAjcdCdcxrpczfn

	cmpl	$8,%r10d
	je	L$_last_num_blocks_is_8_hAjcdCdcxrpczfn
	jb	L$_last_num_blocks_is_7_1_hAjcdCdcxrpczfn


	cmpl	$12,%r10d
	je	L$_last_num_blocks_is_12_hAjcdCdcxrpczfn
	jb	L$_last_num_blocks_is_11_9_hAjcdCdcxrpczfn


	cmpl	$15,%r10d
	je	L$_last_num_blocks_is_15_hAjcdCdcxrpczfn
	ja	L$_last_num_blocks_is_16_hAjcdCdcxrpczfn
	cmpl	$14,%r10d
	je	L$_last_num_blocks_is_14_hAjcdCdcxrpczfn
	jmp	L$_last_num_blocks_is_13_hAjcdCdcxrpczfn

L$_last_num_blocks_is_11_9_hAjcdCdcxrpczfn:

	cmpl	$10,%r10d
	je	L$_last_num_blocks_is_10_hAjcdCdcxrpczfn
	ja	L$_last_num_blocks_is_11_hAjcdCdcxrpczfn
	jmp	L$_last_num_blocks_is_9_hAjcdCdcxrpczfn

L$_last_num_blocks_is_7_1_hAjcdCdcxrpczfn:
	cmpl	$4,%r10d
	je	L$_last_num_blocks_is_4_hAjcdCdcxrpczfn
	jb	L$_last_num_blocks_is_3_1_hAjcdCdcxrpczfn

	cmpl	$6,%r10d
	ja	L$_last_num_blocks_is_7_hAjcdCdcxrpczfn
	je	L$_last_num_blocks_is_6_hAjcdCdcxrpczfn
	jmp	L$_last_num_blocks_is_5_hAjcdCdcxrpczfn

L$_last_num_blocks_is_3_1_hAjcdCdcxrpczfn:

	cmpl	$2,%r10d
	ja	L$_last_num_blocks_is_3_hAjcdCdcxrpczfn
	je	L$_last_num_blocks_is_2_hAjcdCdcxrpczfn
L$_last_num_blocks_is_1_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	L$_16_blocks_overflow_lmxpvsnltuskvkg
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	L$_16_blocks_ok_lmxpvsnltuskvkg

L$_16_blocks_overflow_lmxpvsnltuskvkg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
L$_16_blocks_ok_lmxpvsnltuskvkg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_pahDDluuDvhcoFz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_pahDDluuDvhcoFz
L$_small_initial_partial_block_pahDDluuDvhcoFz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	L$_after_reduction_pahDDluuDvhcoFz
L$_small_initial_compute_done_pahDDluuDvhcoFz:
L$_after_reduction_pahDDluuDvhcoFz:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_2_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	L$_16_blocks_overflow_mEDvgEFdhenuwqf
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	L$_16_blocks_ok_mEDvgEFdhenuwqf

L$_16_blocks_overflow_mEDvgEFdhenuwqf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
L$_16_blocks_ok_mEDvgEFdhenuwqf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_vfogmqprvwluhiB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_vfogmqprvwluhiB
L$_small_initial_partial_block_vfogmqprvwluhiB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_vfogmqprvwluhiB:

	orq	%r8,%r8
	je	L$_after_reduction_vfogmqprvwluhiB
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_vfogmqprvwluhiB:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_3_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	L$_16_blocks_overflow_rmiCBdbuBciylbu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_rmiCBdbuBciylbu

L$_16_blocks_overflow_rmiCBdbuBciylbu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_rmiCBdbuBciylbu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ptpGjmeEEFxaqzq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ptpGjmeEEFxaqzq
L$_small_initial_partial_block_ptpGjmeEEFxaqzq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ptpGjmeEEFxaqzq:

	orq	%r8,%r8
	je	L$_after_reduction_ptpGjmeEEFxaqzq
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_ptpGjmeEEFxaqzq:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_4_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	L$_16_blocks_overflow_ixGvECkebqByChh
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	L$_16_blocks_ok_ixGvECkebqByChh

L$_16_blocks_overflow_ixGvECkebqByChh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
L$_16_blocks_ok_ixGvECkebqByChh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_mFmgcxjmkxvzvzu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_mFmgcxjmkxvzvzu
L$_small_initial_partial_block_mFmgcxjmkxvzvzu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_mFmgcxjmkxvzvzu:

	orq	%r8,%r8
	je	L$_after_reduction_mFmgcxjmkxvzvzu
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_mFmgcxjmkxvzvzu:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_5_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	L$_16_blocks_overflow_esjobwcjCrqfolj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	L$_16_blocks_ok_esjobwcjCrqfolj

L$_16_blocks_overflow_esjobwcjCrqfolj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
L$_16_blocks_ok_esjobwcjCrqfolj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aFwhGngbzvDnqyG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aFwhGngbzvDnqyG
L$_small_initial_partial_block_aFwhGngbzvDnqyG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aFwhGngbzvDnqyG:

	orq	%r8,%r8
	je	L$_after_reduction_aFwhGngbzvDnqyG
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aFwhGngbzvDnqyG:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_6_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	L$_16_blocks_overflow_qfbggCtkzifbisd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	L$_16_blocks_ok_qfbggCtkzifbisd

L$_16_blocks_overflow_qfbggCtkzifbisd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
L$_16_blocks_ok_qfbggCtkzifbisd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_aGvqGssvwfsmztg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_aGvqGssvwfsmztg
L$_small_initial_partial_block_aGvqGssvwfsmztg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_aGvqGssvwfsmztg:

	orq	%r8,%r8
	je	L$_after_reduction_aGvqGssvwfsmztg
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_aGvqGssvwfsmztg:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_7_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	L$_16_blocks_overflow_lbnjsEpvEvFqrjh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_lbnjsEpvEvFqrjh

L$_16_blocks_overflow_lbnjsEpvEvFqrjh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_lbnjsEpvEvFqrjh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_oqCfemEegjyCknz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_oqCfemEegjyCknz
L$_small_initial_partial_block_oqCfemEegjyCknz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_oqCfemEegjyCknz:

	orq	%r8,%r8
	je	L$_after_reduction_oqCfemEegjyCknz
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_oqCfemEegjyCknz:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_8_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	L$_16_blocks_overflow_oCrxilzdefwpFqq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	L$_16_blocks_ok_oCrxilzdefwpFqq

L$_16_blocks_overflow_oCrxilzdefwpFqq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
L$_16_blocks_ok_oCrxilzdefwpFqq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_gBEebGavmsikAyn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_gBEebGavmsikAyn
L$_small_initial_partial_block_gBEebGavmsikAyn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_gBEebGavmsikAyn:

	orq	%r8,%r8
	je	L$_after_reduction_gBEebGavmsikAyn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_gBEebGavmsikAyn:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_9_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	L$_16_blocks_overflow_xxuhDcjiFBEFlpC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	L$_16_blocks_ok_xxuhDcjiFBEFlpC

L$_16_blocks_overflow_xxuhDcjiFBEFlpC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
L$_16_blocks_ok_xxuhDcjiFBEFlpC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_jbxGmBrcBtaGBfb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_jbxGmBrcBtaGBfb
L$_small_initial_partial_block_jbxGmBrcBtaGBfb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_jbxGmBrcBtaGBfb:

	orq	%r8,%r8
	je	L$_after_reduction_jbxGmBrcBtaGBfb
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_jbxGmBrcBtaGBfb:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_10_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	L$_16_blocks_overflow_tapBCxjriBqwghn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	L$_16_blocks_ok_tapBCxjriBqwghn

L$_16_blocks_overflow_tapBCxjriBqwghn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
L$_16_blocks_ok_tapBCxjriBqwghn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_kzuiaxcFhbrzdey





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_kzuiaxcFhbrzdey
L$_small_initial_partial_block_kzuiaxcFhbrzdey:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_kzuiaxcFhbrzdey:

	orq	%r8,%r8
	je	L$_after_reduction_kzuiaxcFhbrzdey
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_kzuiaxcFhbrzdey:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_11_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	L$_16_blocks_overflow_GlDlgaAgdCkxwlq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_GlDlgaAgdCkxwlq

L$_16_blocks_overflow_GlDlgaAgdCkxwlq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_GlDlgaAgdCkxwlq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_nDqEygdAFqwAjBd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_nDqEygdAFqwAjBd
L$_small_initial_partial_block_nDqEygdAFqwAjBd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_nDqEygdAFqwAjBd:

	orq	%r8,%r8
	je	L$_after_reduction_nDqEygdAFqwAjBd
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_nDqEygdAFqwAjBd:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_12_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	L$_16_blocks_overflow_elfcwfaxmCyGouf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	L$_16_blocks_ok_elfcwfaxmCyGouf

L$_16_blocks_overflow_elfcwfaxmCyGouf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
L$_16_blocks_ok_elfcwfaxmCyGouf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_xpoakAtoCBAtfeE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_xpoakAtoCBAtfeE
L$_small_initial_partial_block_xpoakAtoCBAtfeE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_xpoakAtoCBAtfeE:

	orq	%r8,%r8
	je	L$_after_reduction_xpoakAtoCBAtfeE
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_xpoakAtoCBAtfeE:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_13_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	L$_16_blocks_overflow_xreDjhGyaoqnEgz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	L$_16_blocks_ok_xreDjhGyaoqnEgz

L$_16_blocks_overflow_xreDjhGyaoqnEgz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
L$_16_blocks_ok_xreDjhGyaoqnEgz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cldvfEhbwFkGAdn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cldvfEhbwFkGAdn
L$_small_initial_partial_block_cldvfEhbwFkGAdn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cldvfEhbwFkGAdn:

	orq	%r8,%r8
	je	L$_after_reduction_cldvfEhbwFkGAdn
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_cldvfEhbwFkGAdn:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_14_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	L$_16_blocks_overflow_fimuAxvofxdFtze
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	L$_16_blocks_ok_fimuAxvofxdFtze

L$_16_blocks_overflow_fimuAxvofxdFtze:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
L$_16_blocks_ok_fimuAxvofxdFtze:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qugrdqxjrjdjxza





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qugrdqxjrjdjxza
L$_small_initial_partial_block_qugrdqxjrjdjxza:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_qugrdqxjrjdjxza:

	orq	%r8,%r8
	je	L$_after_reduction_qugrdqxjrjdjxza
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_qugrdqxjrjdjxza:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_15_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	L$_16_blocks_overflow_xmCtiiwFwgyuubc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_xmCtiiwFwgyuubc

L$_16_blocks_overflow_xmCtiiwFwgyuubc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_xmCtiiwFwgyuubc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_BEmokAhnrtrFrmr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_BEmokAhnrtrFrmr
L$_small_initial_partial_block_BEmokAhnrtrFrmr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_BEmokAhnrtrFrmr:

	orq	%r8,%r8
	je	L$_after_reduction_BEmokAhnrtrFrmr
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_BEmokAhnrtrFrmr:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_16_hAjcdCdcxrpczfn:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	L$_16_blocks_overflow_edCgtmdAkiggzgo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	L$_16_blocks_ok_edCgtmdAkiggzgo

L$_16_blocks_overflow_edCgtmdAkiggzgo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
L$_16_blocks_ok_edCgtmdAkiggzgo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_fxxkokuBhCvzDzj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_fxxkokuBhCvzDzj:
	vpxorq	%xmm7,%xmm14,%xmm14
L$_after_reduction_fxxkokuBhCvzDzj:
	jmp	L$_last_blocks_done_hAjcdCdcxrpczfn
L$_last_num_blocks_is_0_hAjcdCdcxrpczfn:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

L$_last_blocks_done_hAjcdCdcxrpczfn:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	L$_ghash_done_jfayxkkFBEajEfk

L$_message_below_equal_16_blocks_jfayxkkFBEajEfk:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	L$_small_initial_num_blocks_is_8_zitzmAwGpFgywyg
	jl	L$_small_initial_num_blocks_is_7_1_zitzmAwGpFgywyg


	cmpq	$12,%r12
	je	L$_small_initial_num_blocks_is_12_zitzmAwGpFgywyg
	jl	L$_small_initial_num_blocks_is_11_9_zitzmAwGpFgywyg


	cmpq	$16,%r12
	je	L$_small_initial_num_blocks_is_16_zitzmAwGpFgywyg
	cmpq	$15,%r12
	je	L$_small_initial_num_blocks_is_15_zitzmAwGpFgywyg
	cmpq	$14,%r12
	je	L$_small_initial_num_blocks_is_14_zitzmAwGpFgywyg
	jmp	L$_small_initial_num_blocks_is_13_zitzmAwGpFgywyg

L$_small_initial_num_blocks_is_11_9_zitzmAwGpFgywyg:

	cmpq	$11,%r12
	je	L$_small_initial_num_blocks_is_11_zitzmAwGpFgywyg
	cmpq	$10,%r12
	je	L$_small_initial_num_blocks_is_10_zitzmAwGpFgywyg
	jmp	L$_small_initial_num_blocks_is_9_zitzmAwGpFgywyg

L$_small_initial_num_blocks_is_7_1_zitzmAwGpFgywyg:
	cmpq	$4,%r12
	je	L$_small_initial_num_blocks_is_4_zitzmAwGpFgywyg
	jl	L$_small_initial_num_blocks_is_3_1_zitzmAwGpFgywyg

	cmpq	$7,%r12
	je	L$_small_initial_num_blocks_is_7_zitzmAwGpFgywyg
	cmpq	$6,%r12
	je	L$_small_initial_num_blocks_is_6_zitzmAwGpFgywyg
	jmp	L$_small_initial_num_blocks_is_5_zitzmAwGpFgywyg

L$_small_initial_num_blocks_is_3_1_zitzmAwGpFgywyg:

	cmpq	$3,%r12
	je	L$_small_initial_num_blocks_is_3_zitzmAwGpFgywyg
	cmpq	$2,%r12
	je	L$_small_initial_num_blocks_is_2_zitzmAwGpFgywyg





L$_small_initial_num_blocks_is_1_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_qerdibwmmlstCzn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_qerdibwmmlstCzn
L$_small_initial_partial_block_qerdibwmmlstCzn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	L$_after_reduction_qerdibwmmlstCzn
L$_small_initial_compute_done_qerdibwmmlstCzn:
L$_after_reduction_qerdibwmmlstCzn:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_2_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_niEmwzbxGvyxwbg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_niEmwzbxGvyxwbg
L$_small_initial_partial_block_niEmwzbxGvyxwbg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_niEmwzbxGvyxwbg:

	orq	%r8,%r8
	je	L$_after_reduction_niEmwzbxGvyxwbg
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_niEmwzbxGvyxwbg:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_3_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_doxkBbBauFntfGu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_doxkBbBauFntfGu
L$_small_initial_partial_block_doxkBbBauFntfGu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_doxkBbBauFntfGu:

	orq	%r8,%r8
	je	L$_after_reduction_doxkBbBauFntfGu
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_doxkBbBauFntfGu:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_4_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_saAarExqmAqorhr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_saAarExqmAqorhr
L$_small_initial_partial_block_saAarExqmAqorhr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_saAarExqmAqorhr:

	orq	%r8,%r8
	je	L$_after_reduction_saAarExqmAqorhr
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_saAarExqmAqorhr:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_5_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ComFxcGytoBrain





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ComFxcGytoBrain
L$_small_initial_partial_block_ComFxcGytoBrain:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ComFxcGytoBrain:

	orq	%r8,%r8
	je	L$_after_reduction_ComFxcGytoBrain
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ComFxcGytoBrain:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_6_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_metqsxwhqFCDFmb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_metqsxwhqFCDFmb
L$_small_initial_partial_block_metqsxwhqFCDFmb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_metqsxwhqFCDFmb:

	orq	%r8,%r8
	je	L$_after_reduction_metqsxwhqFCDFmb
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_metqsxwhqFCDFmb:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_7_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_cqfhBgnAafzlzki





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_cqfhBgnAafzlzki
L$_small_initial_partial_block_cqfhBgnAafzlzki:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_cqfhBgnAafzlzki:

	orq	%r8,%r8
	je	L$_after_reduction_cqfhBgnAafzlzki
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_cqfhBgnAafzlzki:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_8_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_wpplkadiBlotkqn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_wpplkadiBlotkqn
L$_small_initial_partial_block_wpplkadiBlotkqn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_wpplkadiBlotkqn:

	orq	%r8,%r8
	je	L$_after_reduction_wpplkadiBlotkqn
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_wpplkadiBlotkqn:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_9_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ErEqiqdpAGbisxw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ErEqiqdpAGbisxw
L$_small_initial_partial_block_ErEqiqdpAGbisxw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ErEqiqdpAGbisxw:

	orq	%r8,%r8
	je	L$_after_reduction_ErEqiqdpAGbisxw
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ErEqiqdpAGbisxw:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_10_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_DvoaatzoBfcwBrm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_DvoaatzoBfcwBrm
L$_small_initial_partial_block_DvoaatzoBfcwBrm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_DvoaatzoBfcwBrm:

	orq	%r8,%r8
	je	L$_after_reduction_DvoaatzoBfcwBrm
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_DvoaatzoBfcwBrm:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_11_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_lGdrEFolweDjtci





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_lGdrEFolweDjtci
L$_small_initial_partial_block_lGdrEFolweDjtci:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_lGdrEFolweDjtci:

	orq	%r8,%r8
	je	L$_after_reduction_lGdrEFolweDjtci
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_lGdrEFolweDjtci:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_12_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_zvhfFGkvmyayAej





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_zvhfFGkvmyayAej
L$_small_initial_partial_block_zvhfFGkvmyayAej:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_zvhfFGkvmyayAej:

	orq	%r8,%r8
	je	L$_after_reduction_zvhfFGkvmyayAej
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_zvhfFGkvmyayAej:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_13_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_ipbrlyqrfBfyixF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_ipbrlyqrfBfyixF
L$_small_initial_partial_block_ipbrlyqrfBfyixF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_ipbrlyqrfBfyixF:

	orq	%r8,%r8
	je	L$_after_reduction_ipbrlyqrfBfyixF
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_ipbrlyqrfBfyixF:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_14_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_FqkjAxwCEnvEzaB





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_FqkjAxwCEnvEzaB
L$_small_initial_partial_block_FqkjAxwCEnvEzaB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_FqkjAxwCEnvEzaB:

	orq	%r8,%r8
	je	L$_after_reduction_FqkjAxwCEnvEzaB
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_FqkjAxwCEnvEzaB:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_15_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	L$_small_initial_partial_block_rpfyxiEfCgnyjuj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	L$_small_initial_compute_done_rpfyxiEfCgnyjuj
L$_small_initial_partial_block_rpfyxiEfCgnyjuj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_rpfyxiEfCgnyjuj:

	orq	%r8,%r8
	je	L$_after_reduction_rpfyxiEfCgnyjuj
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_rpfyxiEfCgnyjuj:
	jmp	L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg
L$_small_initial_num_blocks_is_16_zitzmAwGpFgywyg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
L$_small_initial_partial_block_evugFGnCtctqjeC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

L$_small_initial_compute_done_evugFGnCtctqjeC:
	vpxorq	%xmm13,%xmm14,%xmm14
L$_after_reduction_evugFGnCtctqjeC:
L$_small_initial_blocks_encrypted_zitzmAwGpFgywyg:
L$_ghash_done_jfayxkkFBEajEfk:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
L$_enc_dec_done_jfayxkkFBEajEfk:
	jmp	L$exit_gcm_decrypt
L$exit_gcm_decrypt:
	cmpq	$256,%r8
	jbe	L$skip_hkeys_cleanup_AaxEmrorFcBtccb
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
L$skip_hkeys_cleanup_AaxEmrorFcBtccb:
	vzeroupper
	leaq	(%rbp),%rsp

	popq	%r15

	popq	%r14

	popq	%r13

	popq	%r12

	popq	%rbp

	popq	%rbx

	.byte	0xf3,0xc3
L$decrypt_seh_end:


.globl	_ossl_aes_gcm_finalize_avx512

.p2align	5
_ossl_aes_gcm_finalize_avx512:

.byte	243,15,30,250
	vmovdqu	336(%rdi),%xmm2
	vmovdqu	32(%rdi),%xmm3
	vmovdqu	64(%rdi),%xmm4


	cmpq	$0,%rsi
	je	L$_partial_done_hpwginifsewADcg

	vpclmulqdq	$0x11,%xmm2,%xmm4,%xmm0
	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm16
	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm17
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm4,%xmm4

	vpsrldq	$8,%xmm4,%xmm17
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm0,%xmm0
	vpxorq	%xmm16,%xmm4,%xmm4



	vmovdqu64	POLY2(%rip),%xmm17

	vpclmulqdq	$0x01,%xmm4,%xmm17,%xmm16
	vpslldq	$8,%xmm16,%xmm16
	vpxorq	%xmm16,%xmm4,%xmm4



	vpclmulqdq	$0x00,%xmm4,%xmm17,%xmm16
	vpsrldq	$4,%xmm16,%xmm16
	vpclmulqdq	$0x10,%xmm4,%xmm17,%xmm4
	vpslldq	$4,%xmm4,%xmm4

	vpternlogq	$0x96,%xmm16,%xmm0,%xmm4

L$_partial_done_hpwginifsewADcg:
	vmovq	56(%rdi),%xmm5
	vpinsrq	$1,48(%rdi),%xmm5,%xmm5
	vpsllq	$3,%xmm5,%xmm5

	vpxor	%xmm5,%xmm4,%xmm4

	vpclmulqdq	$0x11,%xmm2,%xmm4,%xmm0
	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm16
	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm17
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm4,%xmm4

	vpsrldq	$8,%xmm4,%xmm17
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm0,%xmm0
	vpxorq	%xmm16,%xmm4,%xmm4



	vmovdqu64	POLY2(%rip),%xmm17

	vpclmulqdq	$0x01,%xmm4,%xmm17,%xmm16
	vpslldq	$8,%xmm16,%xmm16
	vpxorq	%xmm16,%xmm4,%xmm4



	vpclmulqdq	$0x00,%xmm4,%xmm17,%xmm16
	vpsrldq	$4,%xmm16,%xmm16
	vpclmulqdq	$0x10,%xmm4,%xmm17,%xmm4
	vpslldq	$4,%xmm4,%xmm4

	vpternlogq	$0x96,%xmm16,%xmm0,%xmm4

	vpshufb	SHUF_MASK(%rip),%xmm4,%xmm4
	vpxor	%xmm4,%xmm3,%xmm3

L$_return_T_hpwginifsewADcg:
	vmovdqu	%xmm3,64(%rdi)
L$abort_finalize:
	.byte	0xf3,0xc3


.globl	_ossl_gcm_gmult_avx512
.private_extern	_ossl_gcm_gmult_avx512

.p2align	5
_ossl_gcm_gmult_avx512:

.byte	243,15,30,250
	vmovdqu64	(%rdi),%xmm1
	vmovdqu64	336(%rsi),%xmm2

	vpclmulqdq	$0x11,%xmm2,%xmm1,%xmm3
	vpclmulqdq	$0x00,%xmm2,%xmm1,%xmm4
	vpclmulqdq	$0x01,%xmm2,%xmm1,%xmm5
	vpclmulqdq	$0x10,%xmm2,%xmm1,%xmm1
	vpxorq	%xmm5,%xmm1,%xmm1

	vpsrldq	$8,%xmm1,%xmm5
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm4,%xmm1,%xmm1



	vmovdqu64	POLY2(%rip),%xmm5

	vpclmulqdq	$0x01,%xmm1,%xmm5,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm1,%xmm1



	vpclmulqdq	$0x00,%xmm1,%xmm5,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm5,%xmm1
	vpslldq	$4,%xmm1,%xmm1

	vpternlogq	$0x96,%xmm4,%xmm3,%xmm1

	vmovdqu64	%xmm1,(%rdi)
	vzeroupper
L$abort_gmult:
	.byte	0xf3,0xc3


.data	
.p2align	4
POLY:.quad	0x0000000000000001, 0xC200000000000000

.p2align	6
POLY2:
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000

.p2align	4
TWOONE:.quad	0x0000000000000001, 0x0000000100000000



.p2align	6
SHUF_MASK:
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607

.p2align	4
SHIFT_MASK:
.quad	0x0706050403020100, 0x0f0e0d0c0b0a0908

ALL_F:
.quad	0xffffffffffffffff, 0xffffffffffffffff

ZERO:
.quad	0x0000000000000000, 0x0000000000000000

.p2align	4
ONE:
.quad	0x0000000000000001, 0x0000000000000000

.p2align	4
ONEf:
.quad	0x0000000000000000, 0x0100000000000000

.p2align	6
ddq_add_1234:
.quad	0x0000000000000001, 0x0000000000000000
.quad	0x0000000000000002, 0x0000000000000000
.quad	0x0000000000000003, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000

.p2align	6
ddq_add_5678:
.quad	0x0000000000000005, 0x0000000000000000
.quad	0x0000000000000006, 0x0000000000000000
.quad	0x0000000000000007, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000

.p2align	6
ddq_add_4444:
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000

.p2align	6
ddq_add_8888:
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000

.p2align	6
ddq_addbe_1234:
.quad	0x0000000000000000, 0x0100000000000000
.quad	0x0000000000000000, 0x0200000000000000
.quad	0x0000000000000000, 0x0300000000000000
.quad	0x0000000000000000, 0x0400000000000000

.p2align	6
ddq_addbe_4444:
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000

.p2align	6
byte_len_to_mask_table:
.value	0x0000, 0x0001, 0x0003, 0x0007
.value	0x000f, 0x001f, 0x003f, 0x007f
.value	0x00ff, 0x01ff, 0x03ff, 0x07ff
.value	0x0fff, 0x1fff, 0x3fff, 0x7fff
.value	0xffff

.p2align	6
byte64_len_to_mask_table:
.quad	0x0000000000000000, 0x0000000000000001
.quad	0x0000000000000003, 0x0000000000000007
.quad	0x000000000000000f, 0x000000000000001f
.quad	0x000000000000003f, 0x000000000000007f
.quad	0x00000000000000ff, 0x00000000000001ff
.quad	0x00000000000003ff, 0x00000000000007ff
.quad	0x0000000000000fff, 0x0000000000001fff
.quad	0x0000000000003fff, 0x0000000000007fff
.quad	0x000000000000ffff, 0x000000000001ffff
.quad	0x000000000003ffff, 0x000000000007ffff
.quad	0x00000000000fffff, 0x00000000001fffff
.quad	0x00000000003fffff, 0x00000000007fffff
.quad	0x0000000000ffffff, 0x0000000001ffffff
.quad	0x0000000003ffffff, 0x0000000007ffffff
.quad	0x000000000fffffff, 0x000000001fffffff
.quad	0x000000003fffffff, 0x000000007fffffff
.quad	0x00000000ffffffff, 0x00000001ffffffff
.quad	0x00000003ffffffff, 0x00000007ffffffff
.quad	0x0000000fffffffff, 0x0000001fffffffff
.quad	0x0000003fffffffff, 0x0000007fffffffff
.quad	0x000000ffffffffff, 0x000001ffffffffff
.quad	0x000003ffffffffff, 0x000007ffffffffff
.quad	0x00000fffffffffff, 0x00001fffffffffff
.quad	0x00003fffffffffff, 0x00007fffffffffff
.quad	0x0000ffffffffffff, 0x0001ffffffffffff
.quad	0x0003ffffffffffff, 0x0007ffffffffffff
.quad	0x000fffffffffffff, 0x001fffffffffffff
.quad	0x003fffffffffffff, 0x007fffffffffffff
.quad	0x00ffffffffffffff, 0x01ffffffffffffff
.quad	0x03ffffffffffffff, 0x07ffffffffffffff
.quad	0x0fffffffffffffff, 0x1fffffffffffffff
.quad	0x3fffffffffffffff, 0x7fffffffffffffff
.quad	0xffffffffffffffff
